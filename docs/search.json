[{"path":"https://mufflyt.github.io/tyler/mysteryshopper/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Tyler Muffly Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Analyzing Physician Demographics with tyler","text":"tyler package provides tools health services researchers analyze physician demographics, specialties, practice locations using public data sources. vignette demonstrates complete workflow : Finding validating physician NPI numbers Retrieving detailed physician information multiple data sources Analyzing practice settings locations Enriching data demographic information","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Analyzing Physician Demographics with tyler","text":"","code":"library(tyler) library(dplyr) library(stringr)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"example-dataset-acog-presidents","dir":"Articles","previous_headings":"","what":"Example Dataset: ACOG Presidents","title":"Analyzing Physician Demographics with tyler","text":"demonstration, ’ll analyze demographics practice characteristics ACOG (American College Obstetricians Gynecologists) presidents. package includes dataset:","code":"data(acog_presidents) head(acog_presidents)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"finding-npi-numbers","dir":"Articles","previous_headings":"","what":"1. Finding NPI Numbers","title":"Analyzing Physician Demographics with tyler","text":"first step finding NPI numbers physicians:","code":"# Prepare input data input_presidents <- acog_presidents %>%   filter(!is.na(first) & !is.na(last)) %>%   select(first, last)  # Search NPPES Registry npi_results <- phase0_search_batch_npi(   name_data = input_presidents,   max_results = 3 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"filtering-for-specialists","dir":"Articles","previous_headings":"1. Finding NPI Numbers","what":"Filtering for Specialists","title":"Analyzing Physician Demographics with tyler","text":"can filter results include OBGYNs using taxonomy codes:","code":"# Get OBGYN taxonomy codes obgyn_taxonomy_codes <- taxonomy %>%   filter(str_detect(Classification, regex(\"Obstetrics|Gynecology\", ignore_case = TRUE))) %>%   select(Code, Specialization)  # Filter NPI results  npi_results_obgyn <- npi_results %>%   filter(taxonomies_code %in% obgyn_taxonomy_codes$Code) %>%   distinct(npi, .keep_all = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"retrieving-detailed-provider-data","dir":"Articles","previous_headings":"","what":"2. Retrieving Detailed Provider Data","title":"Analyzing Physician Demographics with tyler","text":"Next, ’ll query National Downloadable File comprehensive provider information:","code":"NDF_data <- phase0_national_downloadable_file(   npi_numbers = npi_results_obgyn$npi,   batch_size = 10,   user_agent = \"Researcher Name <email@institution.edu>\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"enriching-with-demographics","dir":"Articles","previous_headings":"","what":"3. Enriching with Demographics","title":"Analyzing Physician Demographics with tyler","text":"package provides functions add demographic information:","code":"# Add predicted gender based on first names enriched_data <- phase0_genderize_physicians(   physician_data = NDF_data,   first_name_column = \"NDF_first\",   last_name_column = \"NDF_last\" )  # Classify practice settings enriched_data <- phase0_create_academic_column(   enriched_data,    address_columns = c(\"NDF_addresses_address_1\") )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"geocoding-practice-locations","dir":"Articles","previous_headings":"","what":"4. Geocoding Practice Locations","title":"Analyzing Physician Demographics with tyler","text":"spatial analysis, can geocode practice addresses:","code":"# Requires Google Maps API key geocoded_data <- phase0_geocode(   address_table = enriched_data,   api_key = Sys.getenv(\"GOOGLE_MAPS_API_KEY\"),   address_column_names = c(     \"addresses_address_1\", \"addresses_city\",      \"addresses_state\", \"addresses_postal_code\"   ) )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"output-data-structure","dir":"Articles","previous_headings":"","what":"Output Data Structure","title":"Analyzing Physician Demographics with tyler","text":"final dataset includes rich physician information: Demographics: Gender, estimated age range Credentials: MD/status, specialties, board certifications Practice: Academic vs private setting, location details Geographic: Coordinates, rurality indicators Medicare/Medicaid: Program participation, expansion state status","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"best-practices","dir":"Articles","previous_headings":"","what":"Best Practices","title":"Analyzing Physician Demographics with tyler","text":"Always provide descriptive user agent querying APIs Handle rate limits appropriately (built package functions) Validate NPI numbers detailed lookups Consider privacy implications working provider data Document data sources timestamps analysis","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"nppes-registry","dir":"Articles","previous_headings":"Detailed Data Source Information","what":"NPPES Registry","title":"Analyzing Physician Demographics with tyler","text":"National Plan Provider Enumeration System (NPPES) authoritative registry National Provider Identifier (NPI) numbers United States.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"nppes-data-structure","dir":"Articles","previous_headings":"Detailed Data Source Information > NPPES Registry","what":"NPPES Data Structure","title":"Analyzing Physician Demographics with tyler","text":"Type 1: Individual providers Type 2: Organizational providers Basic demographics (name, gender) Practice locations Taxonomy codes (specialties) Contact information","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"api-query-details","dir":"Articles","previous_headings":"Detailed Data Source Information > NPPES Registry","what":"API Query Details","title":"Analyzing Physician Demographics with tyler","text":"phase0_search_batch_npi() function constructs API queries like:","code":"# Example API endpoint structure https://npiregistry.cms.hhs.gov/api/?version=2.1   &first_name=JANE   &last_name=DOE   &limit=1   &skip=0"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"national-downloadable-file-ndf","dir":"Articles","previous_headings":"Detailed Data Source Information","what":"National Downloadable File (NDF)","title":"Analyzing Physician Demographics with tyler","text":"CMS National Downloadable File provides comprehensive Medicare provider data, updated quarterly.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"ndf-data-structure","dir":"Articles","previous_headings":"Detailed Data Source Information > National Downloadable File (NDF)","what":"NDF Data Structure","title":"Analyzing Physician Demographics with tyler","text":"file contains multiple related tables: NPI Name credentials Primary specialty Gender Medical school Multiple addresses per provider Practice type indicators Medicare enrollment status Organization relationships Group practice indicators Participation status Assignment indicators EHR program participation","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"query-process","dir":"Articles","previous_headings":"Detailed Data Source Information > National Downloadable File (NDF)","what":"Query Process","title":"Analyzing Physician Demographics with tyler","text":"phase0_national_downloadable_file() function: Authentication Setup Query Construction","code":"# Function requires a user agent for tracking user_agent <- httr::user_agent(user_agent_string)  # Configures API connection base_url <- \"https://data.cms.gov/provider-data/api/1/datastore/sql\" # Example query structure query <- sprintf(   \"SELECT * FROM provider_data WHERE npi IN (%s) AND type = 'individual'\",   paste(npi_numbers, collapse = \",\") )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"rate-limiting-and-quotas","dir":"Articles","previous_headings":"Detailed Data Source Information > National Downloadable File (NDF)","what":"Rate Limiting and Quotas","title":"Analyzing Physician Demographics with tyler","text":"20 requests per second daily limit IP-based rate limiting 5,000 requests per hour Token-based authentication Requires user agent identification package automatically handles limits exponential backoff request batching.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Analyzing_Physician_Demographics_with_tyler.html","id":"further-resources","dir":"Articles","previous_headings":"","what":"Further Resources","title":"Analyzing Physician Demographics with tyler","text":"NPPES NPI Registry CMS Provider Data Package Documentation","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Assign_Insurance_Assign_Scenarios.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Sampling Spine Surgeons and Assigning Cases","text":"vignette demonstrates structured workflow managing data related spine surgeons. key objectives : Data Loading Cleaning: Import dataset spine surgeons clean analysis. Dataset Expansion: Assign multiple insurance scenarios physician simulate real-world variability. Case Assignment: Distribute cases among lab assistants using priority-based splitting mechanism. Efficient Scaling: Highlight tools methods manage large-scale assignments ease. vignette guide step--step process achieve goals using tyler package tidyverse tools.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Assign_Insurance_Assign_Scenarios.html","id":"description","dir":"Articles","previous_headings":"Step 1: Load and Prepare the Data","what":"Description","title":"Sampling Spine Surgeons and Assigning Cases","text":"dataset spine surgeons loaded Excel file. following steps performed: Filtering Cleaning: Exclude physicians marked inclusion clean column names. Name Standardization: Ensure consistency capitalizing first last names. State Time Zone Mapping: Convert state abbreviations full names assign appropriate time zones. Data Enrichment: Combine multiple fields single descriptive column ease reference.","code":"# Load the data sample_specialists <- readxl::read_xlsx(\"~/Dropbox (Personal)/tyler/inst/extdata/Spine Physician List.xlsx\") %>%   dplyr::rename(     phone_number = `Correct Number`,     specialty_primary = taxonomies_desc,     NPI = npi   ) %>%   dplyr::filter(`Include (Y/N)` == \"Y\") %>%   dplyr::select(-`Include (Y/N)`, -`Reason for Exclusion`) %>%   dplyr::mutate(     last = stringr::str_to_title(last),     first = stringr::str_to_title(first),     full_name = paste(\"Dr.\", first, last),     addresses_state = dplyr::case_when(       `State Code` %in% state.abb ~ state.name[match(`State Code`, state.abb)],       TRUE ~ `State Code`     ),     timezone = dplyr::case_when(       addresses_state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\",                              \"Connecticut\", \"New York\", \"New Jersey\", \"Delaware\", \"Maryland\",                              \"Pennsylvania\", \"Virginia\", \"West Virginia\", \"North Carolina\",                              \"South Carolina\", \"Georgia\", \"Florida\", \"Ohio\", \"Indiana\", \"Michigan\") ~ \"Eastern Time\",       addresses_state %in% c(\"Wisconsin\", \"Illinois\", \"Missouri\", \"North Dakota\", \"South Dakota\",                              \"Nebraska\", \"Kansas\", \"Minnesota\", \"Iowa\", \"Kentucky\", \"Tennessee\",                              \"Alabama\", \"Mississippi\", \"Arkansas\", \"Louisiana\", \"Texas\", \"Oklahoma\") ~ \"Central Time\",       addresses_state %in% c(\"Montana\", \"Idaho\", \"Wyoming\", \"Utah\", \"Colorado\", \"New Mexico\", \"Arizona\") ~ \"Mountain Time\",       addresses_state %in% c(\"California\", \"Nevada\", \"Oregon\", \"Washington\") ~ \"Pacific Time\",       addresses_state == \"Hawaii\" ~ \"Hawaii-Aleutian Time\",       addresses_state == \"Alaska\" ~ \"Alaska Time\",       TRUE ~ \"Unknown Timezone\"     ),     NPI = paste(\"NPI:\", NPI),     combined_info = paste(full_name, \",\", phone_number, \",\", addresses_state, \",\", timezone, \",\", NPI)   ) %>%   dplyr::relocate(combined_info, .before = everything())  glimpse(sample_specialists) ## Rows: 343 ## Columns: 27 ## $ combined_info            <chr> \"Dr. Thomas Derian , 919-479-4160 , North Car… ## $ Division                 <chr> \"South Atlantic\", \"South Atlantic\", \"South At… ## $ NPI                      <chr> \"NPI: 1962463125\", \"NPI: 1821063439\", \"NPI: 1… ## $ first                    <chr> \"Thomas\", \"Joshua\", \"Stephen\", \"Craig\", \"Feli… ## $ last                     <chr> \"Derian\", \"Herzog\", \"Enguidanos\", \"Popp\", \"Fi… ## $ middle                   <chr> \"CRAIG\", \"PAUL\", \"T\", \"A\", NA, \"E.\", NA, NA, … ## $ basic_credential         <chr> \"MD\", NA, \"MD\", \"MD\", NA, \"MD\", NA, NA, NA, N… ## $ basic_sole_proprietor    <chr> \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO… ## $ gender                   <chr> \"M\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M\", \"M\", \"M\", … ## $ basic_enumeration_date   <chr> \"2006-03-29\", \"2006-02-17\", \"2006-06-26\", \"20… ## $ basic_last_updated       <chr> \"2009-09-25\", NA, \"2023-12-15\", \"2021-08-11\",… ## $ basic_certification_date <chr> NA, NA, NA, \"2021-08-11\", NA, NA, NA, NA, NA,… ## $ taxonomies_code          <chr> \"207XS0117X\", NA, \"207X00000X\", \"207X00000X\",… ## $ specialty_primary        <chr> \"Orthopaedic Surgery, Orthopaedic Surgery of … ## $ taxonomies_primary       <lgl> FALSE, NA, TRUE, FALSE, NA, TRUE, NA, NA, NA,… ## $ addresses_address_1      <chr> \"4206 N ROXBORO STREET\", \"1400 JOHNSTON WILLI… ## $ addresses_city           <chr> \"DURHAM\", \"NORTH CHESTERFIELD\", \"NICEVILLE\", … ## $ addresses_state          <chr> \"North Carolina\", \"Virginia\", \"Florida\", \"Flo… ## $ addresses_postal_code    <dbl> 27704, 23235, 32578, 32960, 26506, 30720, 201… ## $ phone_number             <chr> \"919-479-4160\", \"804-915-1910\", \"850-729-1444… ## $ Age                      <chr> \"68\", \"50\", \"59\", \"59\", NA, \"54\", \"40\", \"55\",… ## $ zip_code                 <dbl> NA, 23235, NA, NA, 26506, NA, 20191, 33607, 3… ## $ ID                       <chr> \"ortho_spine_batch_npi_search_cleaned_unique\"… ## $ `State Code`             <chr> \"NC\", \"VA\", \"FL\", \"FL\", \"WV\", \"GA\", \"VA\", \"FL… ## $ Region                   <chr> \"South\", \"South\", \"South\", \"South\", \"South\", … ## $ full_name                <chr> \"Dr. Thomas Derian\", \"Dr. Joshua Herzog\", \"Dr… ## $ timezone                 <chr> \"Eastern Time\", \"Eastern Time\", \"Eastern Time…"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Assign_Insurance_Assign_Scenarios.html","id":"description-1","dir":"Articles","previous_headings":"Step 2: Expand Data by Insurance Scenarios","what":"Description","title":"Sampling Spine Surgeons and Assigning Cases","text":"account variability physician behavior different insurance scenarios, physician assigned multiple insurance categories (e.g., Medicare, Medicaid, BCBS). step critical simulating real-world conditions. Key Points: * Function Used: phase2_sample_surgeons tyler package. * Input Parameters: * data: cleaned dataset physicians. * insurance_types: vector insurance categories. * seed: Ensures reproducibility randomized assignments. * Output: new dataset multiple rows per physician, representing unique insurance scenario.","code":"## INFO [2025-03-11 06:46:45] Starting phase2_sample_surgeons() with inputs: ## INFO [2025-03-11 06:46:45] Insurance types: Medicare ## INFO [2025-03-11 06:46:45] Insurance types: Medicaid ## INFO [2025-03-11 06:46:45] Insurance types: BCBS ## INFO [2025-03-11 06:46:45] Ensure unique phone numbers: TRUE ## INFO [2025-03-11 06:46:45] Output CSV path: data/phase_2 ## INFO [2025-03-11 06:46:45] Seed: 1978 ## INFO [2025-03-11 06:46:45] Specialty column: specialty_primary ## INFO [2025-03-11 06:46:45] NPI column: NPI ## INFO [2025-03-11 06:46:45] Seed set to 1978. ## INFO [2025-03-11 06:46:45] Ensuring unique phone numbers in the dataset. ## INFO [2025-03-11 06:46:45] Number of rows after ensuring unique phone numbers: 310. ## INFO [2025-03-11 06:46:45] Expanding physicians by insurance types... ## INFO [2025-03-11 06:46:45] Expanded data includes insurance types. Total rows: 930. ## INFO [2025-03-11 06:46:45] Saved expanded data to: data/phase_2/phase2_sample_surgeons_by_insurance_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Completed phase2_sample_surgeons(). Returning expanded data. ## Rows: 930 ## Columns: 28 ## $ combined_info            <chr> \"Dr. Thomas Derian , 919-479-4160 , North Car… ## $ Division                 <chr> \"South Atlantic\", \"South Atlantic\", \"South At… ## $ NPI                      <chr> \"NPI: 1962463125\", \"NPI: 1962463125\", \"NPI: 1… ## $ first                    <chr> \"Thomas\", \"Thomas\", \"Thomas\", \"Joshua\", \"Josh… ## $ last                     <chr> \"Derian\", \"Derian\", \"Derian\", \"Herzog\", \"Herz… ## $ middle                   <chr> \"CRAIG\", \"CRAIG\", \"CRAIG\", \"PAUL\", \"PAUL\", \"P… ## $ basic_credential         <chr> \"MD\", \"MD\", \"MD\", NA, NA, NA, \"MD\", \"MD\", \"MD… ## $ basic_sole_proprietor    <chr> \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO\", \"NO… ## $ gender                   <chr> \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", … ## $ basic_enumeration_date   <chr> \"2006-03-29\", \"2006-03-29\", \"2006-03-29\", \"20… ## $ basic_last_updated       <chr> \"2009-09-25\", \"2009-09-25\", \"2009-09-25\", NA,… ## $ basic_certification_date <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, \"2021-08-… ## $ taxonomies_code          <chr> \"207XS0117X\", \"207XS0117X\", \"207XS0117X\", NA,… ## $ specialty_primary        <chr> \"Orthopaedic Surgery, Orthopaedic Surgery of … ## $ taxonomies_primary       <lgl> FALSE, FALSE, FALSE, NA, NA, NA, TRUE, TRUE, … ## $ addresses_address_1      <chr> \"4206 N ROXBORO STREET\", \"4206 N ROXBORO STRE… ## $ addresses_city           <chr> \"DURHAM\", \"DURHAM\", \"DURHAM\", \"NORTH CHESTERF… ## $ addresses_state          <chr> \"North Carolina\", \"North Carolina\", \"North Ca… ## $ addresses_postal_code    <dbl> 27704, 27704, 27704, 23235, 23235, 23235, 325… ## $ phone_number             <chr> \"919-479-4160\", \"919-479-4160\", \"919-479-4160… ## $ Age                      <chr> \"68\", \"68\", \"68\", \"50\", \"50\", \"50\", \"59\", \"59… ## $ zip_code                 <dbl> NA, NA, NA, 23235, 23235, 23235, NA, NA, NA, … ## $ ID                       <chr> \"ortho_spine_batch_npi_search_cleaned_unique\"… ## $ `State Code`             <chr> \"NC\", \"NC\", \"NC\", \"VA\", \"VA\", \"VA\", \"FL\", \"FL… ## $ Region                   <chr> \"South\", \"South\", \"South\", \"South\", \"South\", … ## $ full_name                <chr> \"Dr. Thomas Derian\", \"Dr. Thomas Derian\", \"Dr… ## $ timezone                 <chr> \"Eastern Time\", \"Eastern Time\", \"Eastern Time… ## $ insurance                <chr> \"Medicare\", \"Medicaid\", \"BCBS\", \"Medicare\", \"…"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Assign_Insurance_Assign_Scenarios.html","id":"description-2","dir":"Articles","previous_headings":"Step 3: Assign Cases to Lab Assistants","what":"Description","title":"Sampling Spine Surgeons and Assigning Cases","text":"manage workloads effectively, cases assigned lab assistants based specific criteria. process ensures fair distribution work creates systematic record assignments. Key Points: * Function Used: phase0_split_calls_to_lab_assistants_and_save_by_priority * Input Parameters: * assistant_names: vector lab assistant names. * split_column: column (e.g., insurance type) used distribute cases. * output_folder: Directory assignments saved. * seed_value: Ensures reproducibility. Output: Files saved assistant, detailing assigned cases.","code":"# Define lab assistants assistant_names <- c(   \"Amy Du\", \"Bret Hatzinger\", \"Paul Botolin\", \"Jasmine Hartman Budnik\",   \"Miranda Manfre\", \"Jason Sidrak\", \"Daniel Stokes\", \"Ryan Tseng\",   \"Tristan Seawalt\", \"Christopher Hawryluk\", \"Lind\" )  # Assign cases and save tyler::phase0_split_calls_to_lab_assistants_and_save_by_priority(   input_data_or_path = phase2_sample_surgeons_output,   output_folder = \"inst/extdata/caller_assignments\",   assistant_names = assistant_names,   seed_value = 1978,   complete_file_prefix = \"complete_version_\",   split_file_prefix = \"\",   recursive_create = TRUE,   split_column = \"insurance\",   priority_values = NULL ) ## INFO [2025-03-11 06:46:45] Starting phase0_split_calls_to_lab_assistants_and_save_by_priority() with inputs: ## INFO [2025-03-11 06:46:45] Output folder: inst/extdata/caller_assignments ## INFO [2025-03-11 06:46:45] Assistant names: Amy Du, Bret Hatzinger, Paul Botolin, Jasmine Hartman Budnik, Miranda Manfre, Jason Sidrak, Daniel Stokes, Ryan Tseng, Tristan Seawalt, Christopher Hawryluk, Lind ## INFO [2025-03-11 06:46:45] Seed value: 1978 ## INFO [2025-03-11 06:46:45] Split column: insurance ## INFO [2025-03-11 06:46:45] Priority values: None ## INFO [2025-03-11 06:46:45] Loaded input data with 930 rows and 28 columns. ## INFO [2025-03-11 06:46:45] Validated presence of split column: insurance. ## INFO [2025-03-11 06:46:45] Assigned lab assistants to rows in the dataset. ## INFO [2025-03-11 06:46:45] Added row numbers and updated combined_info column. ## INFO [2025-03-11 06:46:45] Saved complete dataset to: inst/extdata/caller_assignments/complete_version_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Amy Du to: inst/extdata/caller_assignments/Amy Du_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Bret Hatzinger to: inst/extdata/caller_assignments/Bret Hatzinger_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Christopher Hawryluk to: inst/extdata/caller_assignments/Christopher Hawryluk_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Daniel Stokes to: inst/extdata/caller_assignments/Daniel Stokes_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Jasmine Hartman Budnik to: inst/extdata/caller_assignments/Jasmine Hartman Budnik_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Jason Sidrak to: inst/extdata/caller_assignments/Jason Sidrak_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Lind to: inst/extdata/caller_assignments/Lind_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Miranda Manfre to: inst/extdata/caller_assignments/Miranda Manfre_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Paul Botolin to: inst/extdata/caller_assignments/Paul Botolin_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Ryan Tseng to: inst/extdata/caller_assignments/Ryan Tseng_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Saved split data for Tristan Seawalt to: inst/extdata/caller_assignments/Tristan Seawalt_2025-03-11_06-46-45.csv. ## INFO [2025-03-11 06:46:45] Completed phase0_split_calls_to_lab_assistants_and_save_by_priority()."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Assign_Insurance_Assign_Scenarios.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Sampling Spine Surgeons and Assigning Cases","text":"vignette demonstrates streamlined workflow managing spine surgeon data, simulating real-world scenarios, distributing workloads efficiently. Key takeaways include: Data Loading Cleaning: Standardizing preparing datasets critical downstream analysis. Dataset Expansion: Assigning multiple insurance scenarios physicians enables robust testing simulations. Case Assignment: Distributing cases among lab assistants ensures fair systematic workload management. Using workflow, researchers can effectively scale manage large datasets, ensuring reproducibility efficiency projects.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Poisson_Regression_and_Effects.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Analyzing Wait Times for Appointments: Poisson Regression and Interaction Effects","text":"healthcare, understanding wait times medical appointments crucial factor improving patient care. vignette demonstrates analyze waiting times appointments using Poisson regression, appropriate count data, number business days patient’s appointment. Specifically, investigate insurance type medical scenario influence waiting times explore interaction effects two variables. Additionally, use Estimated Marginal Means (EMMs) interpret results, allowing clearer comparisons across groups.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Poisson_Regression_and_Effects.html","id":"data-preparation","dir":"Articles","previous_headings":"Introduction","what":"1. Data Preparation","title":"Analyzing Wait Times for Appointments: Poisson Regression and Interaction Effects","text":"begin analysis, need prepare load data. data ’ll use contains following columns: dataset contains following columns: Scenario: Different medical conditions (e.g., Tubo-Ovarian Abscess, Pregnancy Tubal Ligation, UTI, Vaginitis). Insurance: Patient insurance types (e.g., Medicaid, Blue Cross/Blue Shield). Business Days Appointment: Number days takes patients secure appointment. NPI: National Provider Identifier, representing healthcare providers (used random effect).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Poisson_Regression_and_Effects.html","id":"simulating-example-data","dir":"Articles","previous_headings":"Introduction > 1. Data Preparation","what":"Simulating Example Data","title":"Analyzing Wait Times for Appointments: Poisson Regression and Interaction Effects","text":"also simulate data demonstration purposes. , ’ve simulated data outcome variable, business_days_until_appointment, follows Poisson distribution, suitable model count data like . insurance column includes two types insurance, Medicaid Blue Cross/Blue Shield, scenario column includes different medical situations.","code":"# Load required libraries library(dplyr) ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union library(ggplot2) library(lme4) ## Loading required package: Matrix library(emmeans) ## Welcome to emmeans. ## Caution: You lose important information if you filter this package's results. ## See '? untidy' library(knitr) library(tidyr) ##  ## Attaching package: 'tidyr' ## The following objects are masked from 'package:Matrix': ##  ##     expand, pack, unpack # Example data preparation set.seed(123) df <- data.frame(   scenario = rep(c(\"TOA\", \"Pregnancy after Tubal Ligation\", \"UTI\", \"Vaginitis\"), each = 50),   insurance = rep(c(\"Medicaid\", \"Blue Cross/Blue Shield\"), times = 100),   business_days_until_appointment = rpois(200, lambda = 20),   NPI = sample(1:20, 200, replace = TRUE) )"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Poisson_Regression_and_Effects.html","id":"fit-the-interaction-model","dir":"Articles","previous_headings":"2. Fitting a Poisson Regression Model","what":"2.1 Fit the Interaction Model","title":"Analyzing Wait Times for Appointments: Poisson Regression and Interaction Effects","text":"analyze effect insurance scenario waiting times, fit Poisson regression model using glmer function lme4 package. model accounts interaction insurance type scenario, well random intercept NPI (since different healthcare providers might different waiting times). Poisson regression model assumes count data (waiting time days) follows Poisson distribution allows us estimate effects predictor variables (insurance scenario) outcome (business_days_until_appointment). glmer function includes random intercept NPI, accounting fact healthcare providers may systematically different wait times. Poisson regression model assumes count data (waiting time days) follows Poisson distribution allows us estimate effects predictor variables (insurance scenario) outcome (business_days_until_appointment). glmer function includes random intercept NPI, accounting fact healthcare providers may systematically different wait times.","code":"interaction_model <- glmer(   business_days_until_appointment ~ scenario * insurance + (1 | NPI),   data = df,   family = poisson(link = \"log\") )"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/Poisson_Regression_and_Effects.html","id":"extract-emms","dir":"Articles","previous_headings":"3. Estimated Marginal Means (EMMs)","what":"3.1 Extract EMMs","title":"Analyzing Wait Times for Appointments: Poisson Regression and Interaction Effects","text":"now use Estimated Marginal Means (EMMs) obtain adjusted estimates waiting times. EMMs allow us make comparisons levels predictor variables (e.g., insurance type), controlling factors model. emmeans function computes estimated marginal means combination scenario insurance type, providing us adjusted waiting times. adjusted means account variables model, helping us focus effect insurance type wait times.","code":"interaction_result <- emmeans::emmeans(interaction_model, ~ scenario * insurance, type = \"response\") interaction_data <- as.data.frame(interaction_result)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Gathering Drive Time Isochrones","text":"create_isochrones_for_dataframe function powerful tool allows calculate isochrones given location using hereR package. Isochrones represent areas can reached specific point within certain travel time distance. visual representations valuable various applications, location analysis, logistics, transportation planning. guide, walk use create_isochrones function.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"geodesic-versus-drive-time-for-patient-travel","dir":"Articles","previous_headings":"","what":"Geodesic versus Drive-Time for Patient Travel","title":"Gathering Drive Time Isochrones","text":"methods calculating patient travel distance hospitals can vary significantly. paper aims provide overview different methods characteristics. primary factor influencing travel distance calculations choice distance measure, specifically, whether ’s driving distance straight-line distance. distinction significant impact results.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"straight-line-distance","dir":"Articles","previous_headings":"Geodesic versus Drive-Time for Patient Travel","what":"Straight-Line Distance","title":"Gathering Drive Time Isochrones","text":"common practice AHRQ calculate shortest “straight-line” distance (geodetic great circle distance) patient location point care (e.g., hospital emergency department). method favored can easily computed using statistical software like SAS®. Agency Healthcare Research Quality (AHRQ): AHRQ employs equation convert straight-line distance drive time. equation includes various parameters like baseline distance, census division dummy variables, urban/rural location dummy variables, error terms. AHRQ utilizes ggmap package geocode addresses hospitals. AHRQ also considers alternative metric, driving distance driving times. can obtained various mapping software Google Maps, MapQuest, OpenStreetMaps, ArcGIS Network Analyst. AHRQ uses patient location geographic centroid patient’s zip code. (https://hcup-us.ahrq.gov/reports/methods/MS2021-02-Distance--Hospital.jsp) March Dimes Maternity Care Deserts: organization also uses drive time metric calculating travel distance. Reference ESRI Methodology: ESRI methodology creating drive-time areas, certain limitations travel times distances. Reference. Limitations: “must granted network analysis privilege use Create Drive-Time Areas.”, “Travel times exceed 9 hours (540 minutes) walking 5 hours (300 minutes) travel times.”, * “Travel distances exceed 27 miles (43.45 kilometers) walking 300 miles (482.8 kilometers) travel distances.” Veteran’s Administration: Veteran’s Administration utilizes drive time calculations. Reference Department Transportation: Department Transportation provides tools distance calculations. Reference","code":"`Di=αBi+Ciβ+ LiΥ + εi`   Where:  - i indexes patients - Di : driving distance - Bi : baseline distance - Ci : vector of census division dummy variables - Li : vector of urban/rural location dummy variables - α : coefficient for baseline distance - β : vector of coefficients for census division dummy variables - Υ : vector of coefficients for urban/rural location dummy variables - εi : mean-zero error term"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"potential-references-comparing-drive-time-vs--geodesic","dir":"Articles","previous_headings":"","what":"Potential References comparing Drive Time vs. Geodesic","title":"Gathering Drive Time Isochrones","text":"Lidsky , Sun Z, Nussbaum DP, Adam MA, Speicher PJ, Blazer DG. “Going extra mile: improved survival pancreatic cancer patients traveling high-volume centers.” Annals Surgery. 2017;266(2):333–8. Bliss RL, Katz JN, Wright EA, Losina E. “Estimating proximity care: straight line zipcode centroid distances acceptable measures?” Medical Care. 2012;50(1):99–106. isprs-archives-XLVIII-4-W7-2023-53-2023.pdf comprehensive overview highlights diversity methods used calculate patient travel distance hospitals potential impact healthcare outcomes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"prerequisites","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic","what":"Prerequisites","title":"Gathering Drive Time Isochrones","text":"start using create_isochrones function, make sure completed following steps: API Key: need API key. don’t one, can obtain Developer Portal. Environment Variable: Set API key environment variable named HERE_API_KEY. essential secure access services. Load tyler package: Ensure load tyler package, contains create_isochrones function.","code":"library(tyler) ## Loading required package: dplyr ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"usage","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic","what":"Usage","title":"Gathering Drive Time Isochrones","text":"Now prerequisites place, let’s explore use create_isochrones_for_dataframe function. use API calculate optimal routes directions various modes transportation, including driving, walking, cycling, public transit. provides detailed turn--turn instructions, estimated travel times, route alternatives. simpler using OSRM server running AWS cloud, cost minimal.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"input-parameters","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic > Usage","what":"Input Parameters","title":"Gathering Drive Time Isochrones","text":"create_isochrones function accepts following parameters: * location: sf object representing location isolines calculated. Need separate lat long columns. * range: numeric vector time ranges seconds. time ranges determine extent isolines. * posix_time: POSIXct object representing date time calculation. default set “2023-10-20 08:00:00”. chose date Influenza season people see physicians first appointment day 0800. may need split geometry column separate lat long columns using code:","code":"geocoded_data1 <- geocoded_data %>%         dplyr::mutate(lat = sf::st_coordinates(.)[, \"Y\"],                long = sf::st_coordinates(.)[, \"X\"])  readr::write_csv(geocoded_data1, \"/NPPES_November_filtered_data_for_geocoding_geocoded_addresses_not_sf.csv\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"thi-was-an-old-way-of-doing-things","dir":"Articles","previous_headings":"","what":"Thi was an OLD way of doing things:","title":"Gathering Drive Time Isochrones","text":"join postmastr file postmastr_clinician_data.csv geocoded results file geocoded_data_to_match_house_number. API allow pass master ID number API data washed geocoding. postmastr package allows parse addresses clinician_data can match addresses together based : state, house number, zip code. done exploratory.io read back Gathering data.R.","code":"inner_join(`geocoded_data_to_match_house_number`, by = join_by(   `postmastr.pm.state` == `here.state_code`,    `postmastr.pm.zip` == `here.postal_code`,    `postmastr.pm.house` == `here.house_number`)) inner_join_postmastr_clinician_data <- readr::read_csv(\"data/inner_join_postmastr_clinician_data.csv\")  %>%   st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%   dplyr::mutate(geometry = location)    create_isochrones_for_dataframe(inner_join_postmastr_clinician_data_sf, range = c(30*60, 60*60, 120*60, 180*60))"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"example","dir":"Articles","previous_headings":"Thi was an OLD way of doing things:","what":"Example","title":"Gathering Drive Time Isochrones","text":"added code needed read RDS, xlsx, xls, csv files. can also read sf files. needed column called ‘location’ another called geometry. ’s example use create_isochrones_for_dataframe function:","code":"input_file <- \"data/inner_join_postmastr_clinician_data.csv\" isochrones_data <- create_isochrones_for_dataframe(input_file, breaks = c(30*60, 60*60, 120*60, 180*60))  > isochrones_data [1] 1 splay setup instructions: To create isochrones for a specific point(s) use the following code: tryLocationMemo(location = location, range = c(1800, 3600, 7200, 10800)) Setting up the hereR access... Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 2.8 Kb Isoline successfully produced for range: 1800 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 3.1 Kb Isoline successfully produced for range: 3600 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 4.8 Kb Isoline successfully produced for range: 7200 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 6.9 Kb Isoline successfully produced for range: 10800 seconds"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"output","dir":"Articles","previous_headings":"Thi was an OLD way of doing things:","what":"Output","title":"Gathering Drive Time Isochrones","text":"function returns list isolines different time ranges. isoline represented sf object, making easy visualize analyze. create_isochrones function wrapped memoise nice job caching data. note, none columns feed function come side going API. Therefore, hoping 1:1 relationship rows isochrones. may need mark column different time feed API pseudo-identifier.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"conclusion","dir":"Articles","previous_headings":"Thi was an OLD way of doing things:","what":"Conclusion","title":"Gathering Drive Time Isochrones","text":"create_isochrones function simplifies process calculating isolines location-based analysis. Whether ’re exploring accessibility, optimizing routes, conducting spatial analysis, isochrones provide valuable insights travel times distances. tyler package create_isochrones function, can streamline location-based workflows make informed decisions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/create_isochrones.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Gathering Drive Time Isochrones","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Geocoding","text":"geocode function designed help geocode datasets containing addresses change lattitude longitude.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"installation","dir":"Articles","previous_headings":"Overview > Step 1","what":"Installation","title":"Geocoding","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":"library(tyler)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"understanding-geocoding","dir":"Articles","previous_headings":"Example Usage","what":"Understanding Geocoding","title":"Geocoding","text":"Certainly! Geocoding, process converting addresses place names geographic coordinates (latitude longitude), advantages disadvantages. ’s overview pluses minuses geocoding: Pluses Geocoding: Location Accuracy: Geocoding provides precise location information, allowing pinpoint addresses places map high accuracy. crucial various applications mapping, navigation, location-based services. Spatial Analysis: Geocoded data enables spatial analysis, allowing perform tasks like proximity analysis, spatial clustering, spatial interpolation. ’s invaluable geographic information systems (GIS) geographic research. Geographic Visualization: Geocoded data can visualized maps, making easier understand communicate spatial patterns trends. particularly useful data presentation decision-making. Routing Navigation: Geocoding essential navigation systems, delivery route optimization, location-based apps provide directions estimated travel times. Minuses Geocoding: Data Quality Issues: Geocoding accuracy heavily relies quality underlying address data. Inaccurate outdated address information can lead geocoding errors. Costs: Geocoding services software often come associated costs, particularly large-scale geocoding operations. costs can include data licensing fees usage charges. Complexity: Advanced geocoding tasks, reverse geocoding (converting coordinates addresses) batch geocoding, can technically complex may require expertise specialized tools. summary, geocoding offers numerous benefits terms location accuracy, spatial analysis, visualization, navigation. However, also comes challenges related data quality, costs, complexity. Careful consideration factors essential using geocoding various applications. geocode Zip codes several issues limitations associated geocoding solely based zip codes: Lack Precision: Zip codes designed cover group addresses area, specific points. Therefore, geocoding based solely zip code provides approximation location, often center centroid zip code area. lack precision can problematic applications require accurate coordinates. Zip Code Boundaries: Zip code boundaries can irregular may align natural administrative boundaries. means geocoding based zip codes can result coordinates reflect actual geography area, leading inaccuracies. Zip Code Changes: Zip code boundaries assignments can change time due population growth, urban development, administrative reasons. Geocoding based outdated zip code data can lead incorrect locations. Large Zip Codes: zip codes cover vast geographic areas, especially rural regions. Geocoding center large zip code areas can highly inaccurate specific locations within area. Overlapping Zip Codes: cases, zip codes may overlap one another. Geocoding based solely zip code may distinguish overlapping areas, leading ambiguity. Urban Density: densely populated urban areas, zip codes can small densely packed addresses. Geocoding solely zip code may still result lack precision trying identify particular location within zip code.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Prepare Your Data","title":"Geocoding","text":"can provide data either dataframe CSV file argument input_data. hood ggmap::geocode accessing Google API. ggmap::geocode program AHRQ uses. data CSV file, pass file path input_data parameter. Sometimes data needs separate lat long columns. code can : match geocoded_data orginal dataframe? can use postmastr package allows addresses dataframes parsed house number, street, state, etc. individual parts can join together. seems janky af. postmast’s functionality rests order operations must followed ensure correct parsing: prep postal code state city unit house number ranged house number fractional house number house suffix street directionals street suffix street name reconstruct","code":"output_data <- geocode_unique_addresses(     file_path =\"address_for_geocoding.csv\",     google_maps_api_key = \"123\",     output_file_path = \"data/geocoded_unique_addresses.csv\") geocoded_data1 <- geocoded_data %>%         dplyr::mutate(lat = sf::st_coordinates(.)[, \"Y\"],                long = sf::st_coordinates(.)[, \"X\"]) # install.packages(\"remotes\")       # remotes::install_github(\"slu-openGIS/postmastr\")       library(postmastr)       abc <- read_csv(csv_file)       abc %>% pm_identify(var = \"address\") -> sushi2       sushi2_min <- pm_prep(sushi2, var = \"address\", type = \"street\")       pm_postal_all(sushi2_min)       sushi2_min <- pm_postal_parse(sushi2_min)       moDict <- pm_dictionary(locale = \"us\", type = \"state\", case = c(\"title\", \"upper\"))       moDict       pm_state_all(sushi2_min, dictionary = moDict) #Checks to make sure that all states have matches       sushi2_min <- pm_state_parse(sushi2_min, dictionary = moDict)       pm_house_all(sushi2_min)       sushi2_min <- pm_house_parse(sushi2_min)       sushi2_min <- pm_streetDir_parse(sushi2_min)       sushi2_min <- pm_streetSuf_parse(sushi2_min)       sushi2_min <- pm_street_parse(sushi2_min, ordinal = TRUE, drop = TRUE)       sushi2_parsed <- pm_replace(sushi2_min, source = sushi2)       readr::write_csv(sushi2_parsed, \"~/Dropbox (Personal)/workforce/Master_References/NPPES/NPPES_November_filtered_data_address_parsed.csv\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Geocoding","text":"validate_and_remove_invalid_npi function handy tool cleaning validating datasets NPI numbers. following steps outlined vignette, can ensure data contains valid NPIs analysis processing.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/geocode.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Geocoding","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Data from the US Census Bureau for Isochrones","text":"vignette demonstrates usage get_census_data function, designed retrieve Census data states’ block groups. leverages censusapi package query U.S. Census Bureau’s API collect demographic information specified state FIPS codes. ’ll get population block group using censusapi library relies heavily vignette.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"centers-for-medicare-and-medicaid-services-doctors-and-clinicians-downloadable-file","dir":"Articles","previous_headings":"Introduction","what":"Centers for Medicare and Medicaid Services Doctors and Clinicians Downloadable file","title":"Getting Data from the US Census Bureau for Isochrones","text":"Downloadable File housed CMS Medicare Compare (aka Physician Compare site): CMS Medicare Compare. downloaded full data set file left join runs risk date give data update monthly. data dictionary file: CMS Medicare Compare Data Dictionar. Doctors Clinicians national downloadable file organized individual clinician level; line unique clinicianenrollment record-group-address (NPI-Ind_enrl_ID-Org_PAC_ID-adrs_id) level. Clinicians multiple Medicare enrollment records /single enrollments linking multiple practice locations listed multiple lines.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"state-federal-information-processing-standards-codes","dir":"Articles","previous_headings":"Introduction","what":"State Federal Information Processing Standards Codes","title":"Getting Data from the US Census Bureau for Isochrones","text":"function retrieves Census data using censusapi states’ block groups looping specified list state FIPS codes. brings back data females “B01001_01, 26, 33:49E”. FIPS codes, Federal Information Processing Standards codes, standardized set codes used uniquely identify geographic areas United States. codes assigned various administrative geographical entities, states, counties, cities, . used block groups analysis. GEOID block groups United States can constructed using following format: STATECOUNTYTRACTBLOCK_GROUP. Specifically: * STATE 2-digit code state. * COUNTY 3-digit code county. * TRACT 6-digit code census tract. * BLOCK_GROUP 1-digit code block group within tract.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"block-group-as-the-unit-of-measurement","dir":"Articles","previous_headings":"Introduction","what":"Block Group as the Unit of Measurement","title":"Getting Data from the US Census Bureau for Isochrones","text":"United States Census Bureau’s geographic hierarchy, “block group” smaller detailed geographic unit used collecting reporting demographic statistical data. Block groups subdivisions census tracts typically designed contain 600 3,000 people, although can vary depending population density area. Census block group borders defined based visible easily identifiable features roads, rivers, streets, natural boundaries like mountains parks. Census Bureau aims create block group boundaries follow features make easily distinguishable. Block groups used primary units collecting detailed demographic socioeconomic data decennial census American Community Survey (ACS). Census enumerators visit households within block group collect information population, housing, employment, income, education, . densely populated urban area, block group might represent city block small neighborhood within larger city. example, block group cover city blocks downtown Manhattan, New York City.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"data-from-the-us-census-bureau","dir":"Articles","previous_headings":"","what":"Data From the US Census Bureau","title":"Getting Data from the US Census Bureau for Isochrones","text":"variables part dataset obtained U.S. Census Bureau’s American Community Survey (ACS). U.S. Census Bureau’s American Community Survey (ACS) ongoing nationwide survey conducted United States Census Bureau. designed collect provide detailed demographic, social, economic, housing information American population. key features aspects ACS: Continuous Survey: ACS conducted continuously throughout year, providing updated current data. Unlike decennial census, occurs every ten years, ACS conducted annually, allowing frequent timely information. Sampling: ACS uses sample-based approach collect data representative subset U.S. population. sample includes households individuals 50 states, District Columbia, Puerto Rico. Questionnaire: Respondents asked complete detailed questionnaire covers wide range topics, including demographics (age, sex, race, etc.), housing characteristics, education, employment, income, health insurance, . Geographic Coverage: ACS provides data various geographic levels, including national, state, county, city, town, even census tract block group. allows detailed analysis communities regions. Data Release: ACS releases data various forms, including one-year estimates, three-year estimates, five-year estimates. One-year estimates available areas larger populations, three-year five-year estimates designed smaller areas subpopulations. five-year estimates provide reliable data small geographic areas specific demographic groups. Accessibility: ACS data publicly accessible can accessed Census Bureau’s website, data.census.gov, data dissemination platforms. Researchers, policymakers, businesses, general public use ACS data various purposes, including policy development, market research, community planning. Importance: ACS critical tool understanding changing demographics socio-economic characteristics U.S. population. used congressional apportionment, resource allocation, grant distribution, various research purposes. Privacy Confidentiality: Census Bureau takes privacy confidentiality seriously. Personal information collected ACS questionnaire protected law, responses aggregated ensure individual respondents identified. Census Long Form Replacement: ACS introduced replace long-form questionnaire part decennial census. long-form collected detailed demographic housing information, ACS continues provide valuable data ongoing basis. represent demographic information block groups within various states. ’s explanation variable: name: variable represents name label block group. total_females: represents total number females block group. female_21_yo: variable represents number females aged 21 years older block group. female_22_to_24_years: represents number females aged 22 24 years block group. female_25_to_29_years: variable represents number females aged 25 29 years block group. female_30_to_34_years: represents number females aged 30 34 years block group. etc. Eventually data matched onto Block Groups. block group shapefile 2021 ACS via National Historical Geographic Information System (NHGIS). calculate many people live within outside drive time isochrones, ’ll need identify percent Census block group lies within isochrones.","code":"name = NAME,     total_females = B01001_026E,     female_21_yo = B01001_033E,     female_22_to_24_years = B01001_034E,     female_25_to_29_years = B01001_035E,     female_30_to_34_years = B01001_036E,     female_35_to_39_years = B01001_037E,     female_40_to_44_years = B01001_038E,     female_45_to_49_years = B01001_039E,     female_50_to_54_years = B01001_040E,     female_55_to_59_years = B01001_041E,     female_60_to_61_years = B01001_042E,     female_62_to_64_years = B01001_043E,     female_65_to_66_years = B01001_044E,     female_67_to_69_years = B01001_045E,     female_70_to_74_years = B01001_046E,     female_75_to_79_years = B01001_047E,     female_80_to_84_years = B01001_048E,     female_85_years_and_older = B01001_049E,     fips_state = state"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"function-description","dir":"Articles","previous_headings":"","what":"Function Description","title":"Getting Data from the US Census Bureau for Isochrones","text":"get_census_data function retrieves Census data states’ block groups. ’s brief description parameters: us_fips: vector state FIPS (Federal Information Processing Standards) codes. code uniquely identifies U.S. state. example, Colorado represented FIPS code 08. resulting data combined single dataframe analysis.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"installation","dir":"Articles","previous_headings":"Function Description > Step 1","what":"Installation","title":"Getting Data from the US Census Bureau for Isochrones","text":"using tyler::get_census_data function, need install load required packages. can running following code: lists contain metadata general variables variables related race ethnicity.","code":"# Install and load the necessary packages install.packages(\"censusapi\") library(censusapi) library(dplyr) library(tyler)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"all-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"All Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B01001\") %>%        readr::write_csv(\"data/acs_vars.csv\")        # This code cleans it up a bit acs_vars <- acs_vars %>%   dplyr::select(-predicateType, -group, -limit, -predicateOnly) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"!!Male:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"Annotation of Margin of Error\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Annotation of Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"Margin of Error!!\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Annotation of Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(name, fixed(\"EA\")) & !str_detect(label, fixed(\"!!Male:\"))) %>%   dplyr::mutate(numbers = purrr::map_chr(str_extract_all(label, \"^[:digit:]+\"), ~ ifelse(length(.) == 0, NA_character_, paste(.x, collapse = \"\")))) %>%   dplyr::mutate(numbers = as.numeric(numbers)) %>%   dplyr::mutate(numbers = tidyr::replace_na(numbers, 0)) %>%   dplyr::mutate(numbers = as.numeric(numbers)) %>%   dplyr::arrange(numbers)    > acs_vars           name                     label    concept numbers 1  B01001_026E Estimate!!Total:!!Female: SEX BY AGE       0 2  B01001_027E             Under 5 years SEX BY AGE       0 3  B01001_001E          Estimate!!Total: SEX BY AGE       0 4  B01001_028E              5 to 9 years SEX BY AGE       5 5  B01001_029E            10 to 14 years SEX BY AGE      10"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"race-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"Race Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_race_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B02001\") %>%       readr::write_csv(\"data/acs_race_vars.csv\")  #output: > acs_race_vars # A tibble: 40 × 7    name         label                                                      concept predicateType group limit predicateOnly    <chr>        <chr>                                                      <chr>   <chr>         <chr> <dbl> <lgl>          1 B02001_010EA Annotation of Estimate!!Total:!!Two or more races:!!Two r… RACE    string        B020…     0 TRUE           2 B02001_010MA Annotation of Margin of Error!!Total:!!Two or more races:… RACE    string        B020…     0 TRUE           3 B02001_001EA Annotation of Estimate!!Total:                             RACE    string        B020…     0 TRUE           4 B02001_001MA Annotation of Margin of Error!!Total:                      RACE    string        B020…     0 TRUE           5 B02001_004EA Annotation of Estimate!!Total:!!American Indian and Alask… RACE    string        B020…     0 TRUE           6 B02001_004MA Annotation of Margin of Error!!Total:!!American Indian an… RACE    string        B020…     0 TRUE           7 B02001_005EA Annotation of Estimate!!Total:!!Asian alone                RACE    string        B020…     0 TRUE           8 B02001_005MA Annotation of Margin of Error!!Total:!!Asian alone         RACE    string        B020…     0 TRUE           9 B02001_002EA Annotation of Estimate!!Total:!!White alone                RACE    string        B020…     0 TRUE          10 B02001_002MA Annotation of Margin of Error!!Total:!!White alone         RACE    string        B020…     0 TRUE"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"race-and-ethnicity-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"Race and Ethnicity Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_raceeth_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B03002\") %>%       readr::write_csv(\"data/acs_raceeth_vars.csv\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Function Description","what":"Step 2: Prepare Your Data","title":"Getting Data from the US Census Bureau for Isochrones","text":"Define vector state FIPS codes. example, can use tigris package obtain FIPS codes U.S. states:","code":"us_fips_list <- tigris::fips_codes %>%     dplyr::select(state_code, state_name) %>%     dplyr::distinct(state_code, .keep_all = TRUE) %>%     filter(state_code < 56) %>%                         #state_codes over 56 are territories     dplyr::select(state_code) %>%     dplyr::pull()                                                # All US State FIPS Codes us_fips_list <- c(\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"53\", \"54\", \"55\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"step-3-gather-the-data-from-the-us-census-bureau-api","dir":"Articles","previous_headings":"Function Description","what":"Step 3: Gather the Data from the US Census Bureau API","title":"Getting Data from the US Census Bureau for Isochrones","text":"Call get_census_data function us_fips_list vector. example:","code":"all_census_data <- get_census_data(us_fips = us_fips_list)  ########################################################################## # Get Census data by block group in relevant states # Construct: for=block group:*&in=state:01&in=county:*&in=tract:* ###########################################################################  #output GOES HERE!!!!"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"step-4-the-function-will-retrieve-census-data-for-all-specified-states-and-combine-it-into-a-single-dataframe-which-you-can-use-for-further-analysis-","dir":"Articles","previous_headings":"Function Description","what":"Step 4: The function will retrieve Census data for all specified states and combine it into a single dataframe, which you can use for further analysis.","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"demographics_bg <- acs_block_group %>%   rename(     name = NAME,     total_females = B01001_026E,     female_21_yo = B01001_033E,     female_22_to_24_years = B01001_034E,     female_25_to_29_years = B01001_035E,     female_30_to_34_years = B01001_036E,     female_35_to_39_years = B01001_037E,     female_40_to_44_years = B01001_038E,     female_45_to_49_years = B01001_039E,     female_50_to_54_years = B01001_040E,     female_55_to_59_years = B01001_041E,     female_60_to_61_years = B01001_042E,     female_62_to_64_years = B01001_043E,     female_65_to_66_years = B01001_044E,     female_67_to_69_years = B01001_045E,     female_70_to_74_years = B01001_046E,     female_75_to_79_years = B01001_047E,     female_80_to_84_years = B01001_048E,     female_85_years_and_older = B01001_049E,     fips_state = state   ) %>%   mutate(     fips_county = paste0(fips_state, county),     fips_tract = paste0(fips_state, county, tract),     fips_block_group = paste0(       fips_state,       county,       str_pad(tract, width = 6, pad = \"0\"),       block_group     )   ) %>%   mutate(     population = female_21_yo + female_22_to_24_years + female_25_to_29_years +       female_30_to_34_years + female_35_to_39_years + female_40_to_44_years +       female_45_to_49_years +       female_50_to_54_years +       female_55_to_59_years +       female_60_to_61_years +       female_62_to_64_years +       female_65_to_66_years +       female_67_to_69_years +       female_70_to_74_years +       female_75_to_79_years +       female_80_to_84_years +       female_85_years_and_older   ) %>% #total of reproductive age women   arrange(fips_state) %>%   select(     fips_block_group,     fips_state,     fips_county,     fips_tract,     name,     population,     everything()   ) %>%   select(-starts_with(\"B\"),          -contains(\"universe\"),          -county,          -tract,          -block_group)  colnames(demographics_bg)  demographics_bg <- demographics_bg %>% arrange(fips_block_group) readr::write.csv(demographics_bg, \"data/acs-block-group-demographics.csv\", na = \"\", row.names = F) readr::write_rds(demographics_bg, \"data/acs-block-group-demographics.rds\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"step-5-join-the-data-to-the-block-groups","dir":"Articles","previous_headings":"Function Description","what":"Step 5: Join the Data to the Block Groups","title":"Getting Data from the US Census Bureau for Isochrones","text":"code starts loading block group shapefile using sf::st_read() function. shapefile path replaced actual file path. left join performed “demographics_bg” dataset “bg_shape” dataset using dplyr::left_join(). join based matching “fips_block_group” column “demographics_bg” “GEOID” column “bg_shape”.","code":"# Load the block group shapefile using sf::st_read() function # Replace \"/data/shp/block_group/\" with the actual file path to the shapefile bg_shape <- sf::st_read(/data/shp/block_group/\") %>%      # Remove leading zeros from the GEOID column using stringr::str_remove()   # This is a common step to ensure GEOIDs are consistent      dplyr::mutate(GEOID = stringr::str_remove(GEOID, regex(\"^0\", ignore_case = TRUE))) %>%      # Select only the GEOID and geometry columns from the shapefile   dplyr::select(GEOID, geometry)   # Write the block group shapefile with selected columns to a CSV file # This will create a CSV file with GEOID and geometry information bg_shape %>%   readr::write_csv(\"bg_shape_with_geometry.csv\")   # Convert the \"fips_block_group\" column in the \"demographics_bg\" dataset to character # This is done to ensure compatibility for joining with the GEOID column in the shapefile demographics_bg$fips_block_group <- as.character(demographics_bg$fips_block_group)   # Perform a left join between the demographics dataset and the block group shapefile # Join the datasets using the \"fips_block_group\" column from demographics_bg # and the \"GEOID\" column from bg_shape  geometry <- dplyr::left_join(x = demographics_bg,            y = bg_shape,            by = c(\"fips_block_group\" = \"GEOID\")) # Write the resulting dataset with geometry information to a CSV file # This will create a CSV file containing demographic data and geometry information readr::write_csv(geometry, \"block_groups_with_geometry.csv\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"usage-tips","dir":"Articles","previous_headings":"","what":"Usage Tips","title":"Getting Data from the US Census Bureau for Isochrones","text":"Ensure valid Census API key access data. Replace \"your_census_api_key_here\" actual API key function call. included one second pause function loop mindful rate limiting API usage policies making multiple requests Census Bureau’s API.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Getting Data from the US Census Bureau for Isochrones","text":"get_census_data function simplifies process obtaining Census data states’ block groups.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/get_census_data.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Getting Data from the US Census Bureau for Isochrones","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"tyler::taxonomy data tyler::search_by_taxonomy function R package offers convenient efficient way query NPI Database healthcare providers based taxonomy descriptions. vignette provides comprehensive guide effectively utilize function, explores various capabilities, offers illustrative use cases.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"installation","dir":"Articles","previous_headings":"Overview","what":"Installation","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"understanding-taxonomy-descriptions","dir":"Articles","previous_headings":"Example Usage","what":"Understanding Taxonomy Descriptions","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"Taxonomy descriptions, derived National Physician Taxonomy Codes NPPES (National Plan Provider Enumeration System) database, fundamental components United States healthcare system. play pivotal role identification categorization healthcare providers various purposes, including billing, insurance, regulatory compliance. particular, tyler::taxonomy data frame contains NUCC taxonomy codes utilized NPPES data files. Taxonomy Code comprises unique ten-character identifier aids identification healthcare provider types areas expertise. Notably, OBGYN taxonomy codes sourced Version 23.1 dated July 1, 2023. Taxonomy codes can obtained National Uniform Claim Committee (NUCC) website . can employ codes pinpoint specific taxonomy descriptions search. instance, interested finding taxonomy codes include string \"GYN\" can use code facilitate search search_by_taxonomy function.","code":"obgyn_taxonomy <- tyler::taxonomy %>%   dplyr::filter(str_detect(`Classification`, fixed(\"GYN\", ignore_case = TRUE))) %>%   dplyr::select(Code, Specialization) Code       Specialization                                       <chr>      <chr>                                              1 207V00000X NA                                                 2 207VC0300X Complex Family Planning                            3 207VC0200X Critical Care Medicine                             4 207VF0040X Female Pelvic Medicine and Reconstructive Surgery  5 207VX0201X Gynecologic Oncology                               6 207VG0400X Gynecology                                         7 207VH0002X Hospice and Palliative Medicine                    8 207VM0101X Maternal & Fetal Medicine                          9 207VB0002X Obesity Medicine                                  10 207VX0000X Obstetrics                                        11 207VE0102X Reproductive Endocrinology"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"search-by-taxonomy-description","dir":"Articles","previous_headings":"Example Usage","what":"Search by Taxonomy Description","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function excels searching NPI Database healthcare providers based taxonomy descriptions. functionality proves invaluable verifying external data regarding subspecialist provider counts filling gaps providers may board-certified actively practicing (board-eligible). data can seamlessly integrated databases, enhancing utility. internal use, can refer \"Exploratory/Workforce/subspecialists_only\". One significant advantage search results include National Provider Identifier (NPI).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"example-usage-1","dir":"Articles","previous_headings":"Example Usage","what":"Example Usage","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"illustrative example, employ search_by_taxonomy function identify healthcare providers specializing “Hospice Palliative Medicine” based taxonomy descriptions. resulting output dataframe containing information physicians either MD qualification, practicing United States individuals, self-identifying taxonomy “Hospice Palliative Medicine.” can easily view resulting data frame user-friendly format. often need merge rows search_taxonomy data get_clinicians data. steps needed make structure names similar get_clinicians data.","code":"# Search for providers based on taxonomy descriptions taxonomy_descriptions <- c(\"Hospice and Palliative Medicine\")  data <- search_by_taxonomy(taxonomy_to_search = taxonomy_descriptions) #> 1200 records requested #> Requesting records 0-200... #> Requesting records 200-400... #> Requesting records 400-600... #> Requesting records 600-800... #> Requesting records 800-1000... #> Requesting records 1000-1200... > data           npi basic_first_name basic_last_name basic_sole_proprietor basic_gender basic_enumeration_date 1  1437277092         MARIETTA   ABALOS-GALITO                   YES            F             2007-03-26 2  1629034905          ANTHONY        ABBRUZZI                    NO            M             2006-04-25 3  1093806697            AYMAN     ABDEL HALIM                    NO            M             2006-09-27 # View the resulting data frame head(data) all_taxonomy_search_data <- data %>%    distinct(npi, .keep_all = TRUE) %>%      # Keep only the OBGYN subspecialist taxonomy descriptions.     filter(taxonomies_desc %in% c(\"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\", \"Obstetrics & Gynecology, Gynecologic Oncology\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\", \"Obstetrics & Gynecology, Reproductive Endocrinology\")) %>%      # Extract the first five of the zip code.     mutate(addresses_postal_code = str_sub(addresses_postal_code,1 ,5)) %>%   mutate(basic_enumeration_date = ymd(basic_enumeration_date)) %>%      # Pull the year out of the enumeration full data.     mutate(basic_enumeration_date_year = year(basic_enumeration_date), .after = ifelse(\"basic_enumeration_date\" %in% names(.), \"basic_enumeration_date\", last_col())) %>%   mutate(basic_middle_name = str_sub(basic_middle_name,1 ,1)) %>%   mutate(across(c(basic_first_name, basic_last_name, basic_middle_name), .fns = ~str_remove_all(., \"[[\\\\p{P}][\\\\p{S}]]\"))) %>%      # Get data ready to add these taxonomy rows to the `search_npi`/GOBA data set.     rename(NPI = npi, first_name = basic_first_name, last_name = basic_last_name, middle_name = basic_middle_name, GenderPhysicianCompare = basic_gender, sub1 = taxonomies_desc, city = addresses_city, state = addresses_state, name.x = full_name, `Zip CodePhysicianCompare` = addresses_postal_code) %>%   mutate(GenderPhysicianCompare = recode(GenderPhysicianCompare, \"F\" = \"Female\", \"M\" = \"Male\", type_convert = TRUE)) %>%      # Show the subspecialty from goba.     mutate(sub1 = recode(sub1, \"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\" = \"FPM\", \"Obstetrics & Gynecology, Gynecologic Oncology\" = \"ONC\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\" = \"MFM\", \"Obstetrics & Gynecology, Reproductive Endocrinology\" = \"REI\", type_convert = TRUE))"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"parameters","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Parameters","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"taxonomy_to_search: character vector contain desired taxonomy description(s) used search criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"output","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Output","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"function returns data frame filtered include NPI data matching specified taxonomy description(s).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function stands wrapper exploring NPI Database taxonomy descriptions. empowers users identify healthcare providers precise specializations, rendering resource healthcare-related research -depth analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/my-vignette.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Search and Process NPI Numbers","text":"search_and_process_npi function tool working datasets containing National Provider Identifier (NPI) numbers. search_and_process_npi wrapper fantastic npi package. Thank authors maintainers npi package. NPI numbers provide standardized way identify track healthcare providers, including physicians, across United States. Government agencies, Centers Medicare & Medicaid Services (CMS), use NPI-based data plan allocate healthcare resources, including provider reimbursements, medical services, workforce distribution. search_and_process_npi allows search NPIs based first last names clinicians start many mystery caller projects getting names patient-facing directory physicians. Getting NPI number unlock multiple demographics physicians (gender, medical school type, address) function like `retrieve_clinician_data``.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"why-use-apis-for-healthcare-data","dir":"Articles","previous_headings":"Overview","what":"Why use APIs for healthcare data?","title":"Search and Process NPI Numbers","text":"Accessing APIs retrieve healthcare data can offer several advantages downloading joining data multiple sources: Real-Time Data: APIs often provide access real-time near-real-time data. Downloading static data files may result using outdated information, APIs can provide latest data becomes available. Data Integrity: APIs typically offer structured validated data. access data via API, can confident quality consistency. contrast, downloading joining data various sources may introduce data integrity issues, missing mismatched records. Efficiency: APIs allow request specific subsets data, reducing amount data transferred processed. can improve efficiency reduce processing time, especially dealing large datasets. Downloading joining entire datasets can time-consuming resource-intensive. Reduced Storage Requirements: Storing large datasets locally can costly terms storage space. Accessing data APIs means don’t need maintain local copy entire dataset, saving storage costs reducing risk data redundancy. Scalability: APIs designed handle high volume requests. Security Privacy: Healthcare data often contains sensitive information, APIs can provide better control data access authentication. Data Source Aggregation: APIs can provide centralized point access data multiple sources. Data Governance: APIs often come documentation usage policies, can help ensure compliance data governance privacy regulations. provides transparency data usage, making easier adhere legal ethical standards. Version Control: APIs versioned, allowing users specify version API want use. ensures backward compatibility provides level stability accessing data. downloading joining data files, version control can challenging. Reduced Maintenance: APIs maintained updated data providers. using APIs, rely provider manage data updates, ensuring always access latest information.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Search and Process NPI Numbers","text":"can use search_and_process_npi function, make sure tyler package installed. can install using following command:","code":"# install.packages(\"tyler\") library(tyler)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"example-usage","dir":"Articles","previous_headings":"","what":"Example Usage","title":"Search and Process NPI Numbers","text":"National Provider Identifier Search defaults find individual people (individuals enumeration_type = “ind”), physicians (“MD”, “”) United States listed NPPES.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"step-1-load-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 1: Load Your Data","title":"Search and Process NPI Numbers","text":"can provide data either dataframe specify path CSV, XLS, XLSX file containing data. dataframe must column named first another named last surname. acog_presidents example data set can use case.","code":"# Toy example using a dataframe data_df <- data.frame(   first = c(\"John\", \"Jane\", \"Bob\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") )  # Example using a CSV file input_file <- \"acog_presidents.csv\"  # Note the file must have a column named \"first\" and a column named \"last\"."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"step-2-call-the-search_and_process_npi-function","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Call the search_and_process_npi Function","title":"Search and Process NPI Numbers","text":"Now, let’s use search_and_process_npi function search NPI numbers based first last names data. cast WIDE net matches specialties.","code":"# Example using a CSV file output_result <- search_and_process_npi(input_file = input_file)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"step-3-customize-your-search","dir":"Articles","previous_headings":"Example Usage","what":"Step 3: Customize Your Search","title":"Search and Process NPI Numbers","text":"can customize NPI search specifying parameters enumeration_type, limit, country_code, filter_credentials. Magic numbers :) take long time run. Best run overnight 2,000 searches. acog_president dataframe take 10 minutes. ’s can : worried message API accessed. just means match NAMES.","code":"# Example with custom search parameters result_df <- search_and_process_npi(   input_data = input_file,   enumeration_type = \"ind\",               # Search for individual NPIs   limit = 10,                             # Set the search limit to 10 results per name pair   country_code = \"US\",                    # Filter for NPIs in the United States   filter_credentials = c(\"MD\", \"DO\")      # Filter for specific credentials ) ERROR : `df` must be an npi_results S3 object, not tbl_df."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"step-4-access-the-results","dir":"Articles","previous_headings":"Example Usage","what":"Step 4: Access the Results","title":"Search and Process NPI Numbers","text":"function return data frame containing processed NPI search results. can access data frame analysis.going lot duplicates need clean thoughtfully.","code":"# Access the result data frame result_df  > output_result               npi basic_first_name basic_last_name basic_middle_name basic_credential basic_sole_proprietor basic_gender     1: 1053601807             RYAN       SCHLUETER            JEWELL               DO                   YES            M     2: 1184186256            LAURA          MARTIN         ELIZABETH               DO                    NO            F     3: 1063703494           LAUREN          BISHOP            ALICIA             M.D.                    NO            F     4: 1740800705           LAUREN          BISHOP         ELISABETH             M.D.                    NO            F             basic_enumeration_date basic_last_updated basic_certification_date basic_status taxonomies_code     1:             2011-04-13         2021-09-30               2021-09-30            A      207VM0101X     2:             2019-04-05         2023-03-16               2023-03-16            A      207P00000X     3:             2011-04-19         2023-03-16               2023-03-16            A      207VE0102X     4:             2020-04-20         2023-07-03               2023-07-03            A      207Q00000X                           taxonomies_taxonomy_group                                     taxonomies_desc taxonomies_state     1: 193400000X - Single Specialty Group  Obstetrics & Gynecology, Maternal & Fetal Medicine               GA     2:                                                                      Emergency Medicine               MS     3:                                     Obstetrics & Gynecology, Reproductive Endocrinology               NY     4:                                                                         Family Medicine               TX                 taxonomies_license taxonomies_primary basic_name_prefix basic_name_suffix     1:              80379               TRUE              <NA>              <NA>     2:              29372               TRUE               Dr.              <NA>     3:          302927-01               TRUE               Dr.              <NA>     4:              U5076               TRUE              <NA>              <NA>"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"step-5-analyze-the-results","dir":"Articles","previous_headings":"Example Usage","what":"Step 5: Analyze the Results","title":"Search and Process NPI Numbers","text":"can now analyze NPI search results needed specific use case. result_df data frame contains information NPIs match search criteria. NPI numbers directly NPPES need run validate_and_remove_invalid_npi. One key step cleaning data filtering taxonomies. can changed different applications various subspecialties. Note people tricky list taxonomy specialty “Specialist” something else super vague. create code fix well, shown . Finally helpful join results called processed_result input_file called acog_presidents code can used .","code":"# Remove selected columns from the 'output_result' dataframe processed_result <- output_result %>%   dplyr::select(     -basic_middle_name,      -basic_certification_date,      -basic_name_prefix,      -basic_name_suffix,      -taxonomies_taxonomy_group,      -taxonomies_license,      -taxonomies_primary   ) %>%    mutate(across(c(basic_first_name, basic_last_name, basic_credential),        .fns = ~str_remove_all(., \"[[\\\\p{P}][\\\\p{S}]]\"))) %>%   mutate(basic_credential = str_to_upper(basic_credential)) %>%   filter(str_detect(basic_credential, \"MD|DO\")) %>%   mutate(basic_credential = str_sub(basic_credential,1 ,2)) %>%   filter(basic_credential %in% c(\"DO\", \"MD\")) %>%   filter(str_detect(taxonomies_desc, fixed(\"Gyn\", ignore_case=TRUE))) %>%   distinct(npi, .keep_all = TRUE)  > processed_result %>% head(5)           npi basic_first_name basic_last_name basic_credential basic_sole_proprietor basic_gender basic_enumeration_date 1: 1053601807             RYAN       SCHLUETER               DO                   YES            M             2011-04-13 2: 1063703494           LAUREN          BISHOP               MD                    NO            F             2011-04-19 3: 1376862383            JAMIE     SZCZEPANSKI               MD                    NO            F             2010-06-01 4: 1457676405          JESSICA         SHIELDS               DO                    NO            F             2010-04-01 5: 1366752107         CAROLINA          SUELDO               MD                    NO            F             2010-10-14    basic_last_updated basic_status taxonomies_code                                     taxonomies_desc taxonomies_state 1:         2021-09-30            A      207VM0101X  Obstetrics & Gynecology, Maternal & Fetal Medicine               GA 2:         2023-03-16            A      207VE0102X Obstetrics & Gynecology, Reproductive Endocrinology               NY 3:         2020-07-23            A      207V00000X                             Obstetrics & Gynecology               NY 4:         2019-07-18            A      207V00000X                             Obstetrics & Gynecology               MA 5:         2019-02-25            A      207V00000X                             Obstetrics & Gynecology               FL # Filter out rows where 'taxonomies_desc' contains the substring \"Gyn\" (case-insensitive).  This can be changed for different applications: \"Ortho\", \"Rheum\", \"Otolary\", \"Heme\", \"Anesthesi\".    processed_result <- processed_result %>%   dplyr::filter(stringr::str_detect(taxonomies_desc, fixed(\"gyn\", ignore_case = TRUE)) |   stringr::str_detect(taxonomies_desc, fixed(\"specialist\", ignore_case = TRUE))) combined_acog_presidents <-    acog_presidents %>%   dplyr::left_join(`processed_result`, by = c(\"first\" = \"basic_first_name\",                                                \"last\" = \"basic_last_name\",                                                \"honorrific\" = \"basic_credential\"),                                                ignore.case=TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Search and Process NPI Numbers","text":"search_and_process_npi function simplifies task searching processing NPI numbers healthcare datasets.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/search_and_process_npi.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Search and Process NPI Numbers","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"tyler::taxonomy data tyler::search_by_taxonomy function R package offers convenient efficient way query NPI Database healthcare providers based taxonomy descriptions. vignette provides comprehensive guide effectively utilize function, explores various capabilities, offers illustrative use cases.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"installation","dir":"Articles","previous_headings":"Overview","what":"Installation","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"understanding-taxonomy-descriptions","dir":"Articles","previous_headings":"Example Usage","what":"Understanding Taxonomy Descriptions","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"Taxonomy descriptions, derived National Physician Taxonomy Codes NPPES (National Plan Provider Enumeration System) database, fundamental components United States healthcare system. play pivotal role identification categorization healthcare providers various purposes, including billing, insurance, regulatory compliance. particular, tyler::taxonomy data frame contains NUCC taxonomy codes utilized NPPES data files. Taxonomy Code comprises unique ten-character identifier aids identification healthcare provider types areas expertise. Notably, OBGYN taxonomy codes sourced Version 23.1 dated July 1, 2023. Taxonomy codes can obtained National Uniform Claim Committee (NUCC) website . can employ codes pinpoint specific taxonomy descriptions search. instance, interested finding taxonomy codes include string \"GYN\" can use code facilitate search search_by_taxonomy function.","code":"obgyn_taxonomy <- tyler::taxonomy %>%   dplyr::filter(str_detect(`Classification`, fixed(\"GYN\", ignore_case = TRUE))) %>%   dplyr::select(Code, Specialization) Code       Specialization                                       <chr>      <chr>                                              1 207V00000X NA                                                 2 207VC0300X Complex Family Planning                            3 207VC0200X Critical Care Medicine                             4 207VF0040X Female Pelvic Medicine and Reconstructive Surgery  5 207VX0201X Gynecologic Oncology                               6 207VG0400X Gynecology                                         7 207VH0002X Hospice and Palliative Medicine                    8 207VM0101X Maternal & Fetal Medicine                          9 207VB0002X Obesity Medicine                                  10 207VX0000X Obstetrics                                        11 207VE0102X Reproductive Endocrinology"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"search-by-taxonomy-description","dir":"Articles","previous_headings":"Example Usage","what":"Search by Taxonomy Description","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function excels searching NPI Database healthcare providers based taxonomy descriptions. functionality proves invaluable verifying external data regarding subspecialist provider counts filling gaps providers may board-certified actively practicing (board-eligible). data can seamlessly integrated databases, enhancing utility. internal use, can refer \"Exploratory/Workforce/subspecialists_only\". One significant advantage search results include National Provider Identifier (NPI).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"example-usage-1","dir":"Articles","previous_headings":"Example Usage","what":"Example Usage","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"illustrative example, employ search_by_taxonomy function identify healthcare providers specializing “Hospice Palliative Medicine” based taxonomy descriptions. resulting output dataframe containing information physicians either MD qualification, practicing United States individuals, self-identifying taxonomy “Hospice Palliative Medicine.” can easily view resulting data frame user-friendly format. often need merge rows search_taxonomy data get_clinicians data. steps needed make structure names similar get_clinicians data.","code":"# Search for providers based on taxonomy descriptions taxonomy_descriptions <- c(\"Hospice and Palliative Medicine\")  data <- search_by_taxonomy(taxonomy_to_search = taxonomy_descriptions) #> 1200 records requested #> Requesting records 0-200... #> Requesting records 200-400... #> Requesting records 400-600... #> Requesting records 600-800... #> Requesting records 800-1000... #> Requesting records 1000-1200... #> Error in search for Hospice and Palliative Medicine: #> Must use existing variables. > data           npi basic_first_name basic_last_name basic_sole_proprietor basic_gender basic_enumeration_date 1  1437277092         MARIETTA   ABALOS-GALITO                   YES            F             2007-03-26 2  1629034905          ANTHONY        ABBRUZZI                    NO            M             2006-04-25 3  1093806697            AYMAN     ABDEL HALIM                    NO            M             2006-09-27 # View the resulting data frame head(data) all_taxonomy_search_data <- data %>%    distinct(npi, .keep_all = TRUE) %>%      # Keep only the OBGYN subspecialist taxonomy descriptions.     filter(taxonomies_desc %in% c(\"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\", \"Obstetrics & Gynecology, Gynecologic Oncology\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\", \"Obstetrics & Gynecology, Reproductive Endocrinology\")) %>%      # Extract the first five of the zip code.     mutate(addresses_postal_code = str_sub(addresses_postal_code,1 ,5)) %>%   mutate(basic_enumeration_date = ymd(basic_enumeration_date)) %>%      # Pull the year out of the enumeration full data.     mutate(basic_enumeration_date_year = year(basic_enumeration_date), .after = ifelse(\"basic_enumeration_date\" %in% names(.), \"basic_enumeration_date\", last_col())) %>%   mutate(basic_middle_name = str_sub(basic_middle_name,1 ,1)) %>%   mutate(across(c(basic_first_name, basic_last_name, basic_middle_name), .fns = ~str_remove_all(., \"[[\\\\p{P}][\\\\p{S}]]\"))) %>%      # Get data ready to add these taxonomy rows to the `search_npi`/GOBA data set.     rename(NPI = npi, first_name = basic_first_name, last_name = basic_last_name, middle_name = basic_middle_name, GenderPhysicianCompare = basic_gender, sub1 = taxonomies_desc, city = addresses_city, state = addresses_state, name.x = full_name, `Zip CodePhysicianCompare` = addresses_postal_code) %>%   mutate(GenderPhysicianCompare = recode(GenderPhysicianCompare, \"F\" = \"Female\", \"M\" = \"Male\", type_convert = TRUE)) %>%      # Show the subspecialty from goba.     mutate(sub1 = recode(sub1, \"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\" = \"FPM\", \"Obstetrics & Gynecology, Gynecologic Oncology\" = \"ONC\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\" = \"MFM\", \"Obstetrics & Gynecology, Reproductive Endocrinology\" = \"REI\", type_convert = TRUE))"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"parameters","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Parameters","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"taxonomy_to_search: character vector contain desired taxonomy description(s) used search criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"output","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Output","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"function returns data frame filtered include NPI data matching specified taxonomy description(s).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function stands wrapper exploring NPI Database taxonomy descriptions. empowers users identify healthcare providers precise specializations, rendering resource healthcare-related research -depth analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/taxonomy_search.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Gather Physician Data Starting With NPI Numbers","text":"validate_and_remove_invalid_npi function designed help process datasets containing National Provider Identifier (NPI) numbers search_by_taxonomy. validates format NPIs using npi package removes rows missing invalid NPIs. vignette guide usage.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"installation","dir":"Articles","previous_headings":"Overview > Step 1","what":"Installation","title":"Gather Physician Data Starting With NPI Numbers","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":"library(tyler)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"understanding-national-provider-identifier","dir":"Articles","previous_headings":"Example Usage","what":"Understanding National Provider Identifier","title":"Gather Physician Data Starting With NPI Numbers","text":"valid National Provider Identifier (NPI) number United States meet certain criteria considered legitimate. key characteristics make NPI number valid: Length: NPI number consists ten digits. shorter longer ten digits. Numeric Digits: characters NPI must numeric digits (0-9). letters, symbols, special characters allowed. Luhn Algorithm: Luhn algorithm commonly used validate credit card numbers, applied NPI numbers. NPIs supposedly checksummed using Luhn algorithm. summary, valid NPI number ten numeric digits additional characters. validate NPI numbers programmatically, can check length confirm contain numeric digits (0-9). However, ’s important note specific format validation rules NPI numbers defined National Plan Provider Enumeration System (NPPES).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Prepare Your Data","title":"Gather Physician Data Starting With NPI Numbers","text":"can provide data either dataframe CSV file argument input_data. data dataframe, simply pass input_data parameter. data CSV file, pass file path input_data parameter.","code":"# Example using a dataframe data_df <- data.frame(npi = c(\"1234567890\", \"9876543210\", \"invalid_npi\")) valid_df <- validate_and_remove_invalid_npi(input_data)  # Example using a CSV file input_data <- \"path/to/your/file.csv\" valid_df <- validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"step-3-data-validation","dir":"Articles","previous_headings":"Example Usage","what":"Step 3: Data Validation","title":"Gather Physician Data Starting With NPI Numbers","text":"function validate NPIs data. performs following checks: Removes rows missing NPIs. Removes rows empty NPIs. Ensures NPIs valid format (numeric 10 characters length). Invalid NPIs removed, new column named “npi_is_valid” added indicate NPI validity.","code":"# A tibble: 7,494 × 7    sub1  first_name last_name          npi state         city             npi_is_valid    <chr> <chr>      <chr>            <dbl> <chr>         <chr>            <lgl>         1 MFM   Ryan       Schlueter   1053601807 Georgia       Atlanta          TRUE          2 FPM   Laura      Martin      1528351640 Florida       Miramar          TRUE          3 REI   Lauren     Bishop      1063703494 New York      New York         TRUE          4 MFM   Jamie      Szczepanski 1376862383 New York      Buffalo          TRUE"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"step-4-get-the-valid-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 4: Get the Valid Data","title":"Gather Physician Data Starting With NPI Numbers","text":"function return dataframe containing valid NPI numbers.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"step-5-validating-npi-numbers-is-needed-before-searching-by-npi-number-in-the-cms-databases-","dir":"Articles","previous_headings":"Example Usage","what":"Step 5: Validating NPI numbers is needed before searching by NPI number in the CMS databases.","title":"Gather Physician Data Starting With NPI Numbers","text":"error can break results error handling beyond knowledge base now. use case use validate_and_remove_invalid_npi function searching physician demographics (medical school, etc) National Downloadable File CMS (https://data.cms.gov/provider-data/dataset/mj5m-pzi6). database update monthly know data fresh validate_and_remove_invalid_npi makes clean. Fresh clean! step can confidently feed NPI numbers provider::clinicians function without fear NPI number error. Specifically use case. see CSV get read provider::clinicians searched. output people results skips people results.","code":"df_updated <- NULL  retrieve_clinician_data <- function(input_data) {   library(provider)   library(dplyr)   library(purrr)   library(readr)   library(tidyr)   library(lubridate)   library(memoise)   library(zipcodeR)    # Load libraries   #remotes::install_github(\"andrewallenbruce/provider\")    if (is.data.frame(input_data)) {     # Input is a dataframe     df <- input_data   } else if (is.character(input_data)) {     # Input is a file path to a CSV     df <- readr::read_csv(input_data)   } else {     stop(\"Input must be a dataframe or a file path to a CSV.\")   }    # Clean the NPI numbers   df <- validate_and_remove_invalid_npi(df) # Function to retrieve clinician data for a single NPI   get_clinician_data <- function(npi) {     if (!is.numeric(npi) || nchar(npi) != 10) {       cat(\"Invalid NPI:\", npi, \"\\n\")       return(NULL)  # Skip this NPI     }      clinician_info <- provider::clinicians(npi = npi)     if (is.null(clinician_info)) {       cat(\"No results for NPI:\", npi, \"\\n\")     } else {       return(clinician_info)     }     Sys.sleep(1)   }    #df <- df %>% head(5) #test    # Loop through the \"npi\" column and get clinician data   df_updated <- df %>%     dplyr::mutate(row_number = row_number()) %>%     dplyr::mutate(clinician_data = purrr::map(npi, get_clinician_data)) %>%     tidyr::unnest(clinician_data, names_sep = \"_\") %>%     dplyr::distinct(npi, .keep_all = TRUE)    return(df_updated) } # Call the retrieve_clinician_data function with an NPI value input_data <- (\"~/Dropbox (Personal)/workforce/subspecialists_only.csv\") clinician_data <- retrieve_clinician_data(input_data)  Rows: 7498 Columns: 9                                                                                                                                         ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: \",\" chr (8): sub1, first_name, last_name, state, name.x, city, GenderPhysicianCompare, Zip CodePhysicianCompare dbl (1): npi  ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ✖ No results for npi = 1063703494 No results for NPI: 1063703494  ✖ No results for npi = 1104052125 No results for NPI: 1104052125  ✖ No results for npi = 1972745586 No results for NPI: 1972745586  ✖ No results for npi = 1427386804 No results for NPI: 1427386804  ✖ No results for npi = 1942586581 No results for NPI: 1942586581"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Gather Physician Data Starting With NPI Numbers","text":"validate_and_remove_invalid_npi function handy tool cleaning validating datasets NPI numbers. following steps outlined vignette, can ensure data contains valid NPIs analysis processing.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/articles/validate_and_remove_invalid_npi.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Gather Physician Data Starting With NPI Numbers","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tyler Muffly. Maintainer.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Muffly T, Name C (2025). Tyler: Package XYZ. Version 1.2.0, https://www.github.com/mufflyt/tyler.","code":"@Manual{,   title = {Tyler: A Package for XYZ},   author = {Tyler Muffly and Coauthor Name},   year = {2025},   note = {Version 1.2.0},   url = {https://www.github.com/mufflyt/tyler}, }"},{"path":[]},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"and-pull-requests-httpsgithubcommufflyttylerpulls","dir":"","previous_headings":"","what":"and pull requests (https://github.com/mufflyt/tyler/pulls).","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"tyler package provides suite tools conducting mystery caller studies facilitating workforce distribution research obstetrics gynecology (OBGYN) professionals. streamlines process retrieving analyzing National Provider Identifier (NPI) data, demographic information, healthcare access data, also offering resources examining OBGYN residency programs. Key Features - Mystery Caller Studies: Tools analyze patient access healthcare searching processing NPI numbers based names criteria. - OBGYN Workforce Distribution: Functions datasets designed support workforce research, including detailed information OBGYN residency programs United States.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"can install development version tyler GitHub : See package vignette fuller introduction suggestions use tyler() function efficiently.","code":"# install.packages(\"devtools\") devtools::install_github(\"mufflyt/tyler\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tyler-package-data-overview","dir":"","previous_headings":"","what":"Tyler Package Data Overview","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"document describes key datasets included tyler package, focusing ACOG districts, physicians, taxonomy codes used healthcare provider identification.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"data-tyleracgme","dir":"","previous_headings":"","what":"DATA: tyler::acgme","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"dataset tyler::acgme provides detailed information OBGYN residency programs accredited Accreditation Council Graduate Medical Education (ACGME). dataset includes fields program name, location, accreditation date, program director, affiliated hospitals. valuable resource mapping analyzing OBGYN residencies across U.S. Dataframe OBGYN residency programs scraped https://apps.acgme.org/ads/Public/Programs/Search. Name, city, state, accreditation date, program director name, website, rotations, affiliated hospitals included. ‘tyler::acgme’ - dataframe every OBGYN residency ACGME web site. data can used map obgyn residencies, etc.","code":"obgyn_residencies <- tyler::acgme  # View the first few rows head(obgyn_residencies) # A tibble: 318 × 142    program_name       address zip   city  state sponsoring_instituti…¹ sponsoring_instituti…² phone original_accreditati…³    <chr>              <chr>   <chr> <chr> <chr> <chr>                  <chr>                  <chr> <chr>                   1 University of Ala… \"Unive… 35249 Birm… Alab… 010498                 University of Alabama… (205… September 01, 1949      2 USA Health Program \"Unive… 36604 Mobi… Alab… 010406                 USA Health             (251… August 01, 1960         3 University of Ari… \"Banne… 85006 Phoe… Ariz… 038179                 University of Arizona… (602… May 07, 1951      # Example: Filter for residency programs in California ca_residencies <- dplyr::filter(obgyn_residencies, state == \"CA\") print(ca_residencies)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"data-tyleracog_districts","dir":"","previous_headings":"","what":"DATA: ‘tyler::ACOG_Districts’","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"American College Obstetricians Gynecologists (ACOG) professional organization representing obstetricians gynecologists United States. ACOG divides membership various geographical regions known “ACOG Districts.” single-state ACOG Districts (e.g., California, Texas, Florida) also need use US Census Bureau subdivisions. Subdivisions important census statistical purposes help organize categorize population data local level. dataset includes: - State names: Full name U.S. state. - ACOG Districts: corresponding ACOG district state. - Subregions: U.S. Census Bureau subregions help organize population data. - State abbreviations: official two-letter postal abbreviations state. dataset useful research geographic distribution OBGYN professionals affiliations ACOG districts.","code":"acog_districts <- tyler::ACOG_Districts head(tyler::ACOG_Districts) # A tibble: 52 × 4    State                ACOG_District Subregion     State_Abbreviations    <chr>                <chr>         <chr>         <chr>                1 Alabama              District VII  District VII  AL                   2 Alaska               District VIII District VIII AK                   3 Arizona              District VIII District VIII AZ                   4 Arkansas             District VII  District VII  AR                   5 California           District IX   District IX   CA                   6 Colorado             District VIII District VIII CO                   7 Connecticut          District I    District I    CT                   8 Delaware             District IV   District IV   DE                   9 District of Columbia District IV   District IV   DC                  10 Florida              District XII  District XII  FL"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"data-tylerphysicians","dir":"","previous_headings":"","what":"DATA: tyler::physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"dataset contains details OBGYN subspecialists, including names, specialties, geographic coordinates. Physicians.rds file, located tyler/inst/extdata, stores internal dataset. dataset includes: - NPI: National Provider Identifier, unique identifier healthcare providers U.S. - Name: Physician’s full name. - Subspecialty: Physician’s specific area expertise within OBGYN. - Latitude/Longitude: Geographic coordinates physician’s practice.","code":"tyler::physicians # A tibble: 4,659 × 5           NPI name                        subspecialty                                        lat   long         <dbl> <chr>                       <chr>                                             <dbl>  <dbl>  1 1922051358 Katherine Boyd              Female Pelvic Medicine and Reconstructive Surgery  42.6  -82.9  2 1750344388 Thomas Byrne                Maternal-Fetal Medicine                            35.2 -102.   3 1548520133 Bobby Garcia                Female Pelvic Medicine and Reconstructive Surgery  40.8  -73.9"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"data-tylertaxonomy","dir":"","previous_headings":"","what":"DATA: tyler::taxonomy","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Physician Taxonomy Codes NPPES (National Plan Provider Enumeration System) database essential components healthcare system United States. codes play crucial role identifying categorizing healthcare providers various purposes, including billing, insurance, regulatory compliance. tyler::taxonomy dataset dataframe containing NUCC taxonomy codes used NPPES data files. taxonomy code consists unique ten-character identifier helps classify healthcare providers type area expertise. dataset includes OBGYN taxonomy codes, Version 23.1 7/1/2023. can find details NUCC website.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"example","dir":"","previous_headings":"DATA: tyler::taxonomy","what":"Example:","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"","code":"obgyn_taxonomy <- tyler::taxonomy %>%    filter(str_detect(Classification, fixed(\"GYN\", ignore_case = TRUE))) %>%    select(Code, Specialization)  obgyn_taxonomy # A tibble of OBGYN-related taxonomy codes    Code       Specialization                                       <chr>      <chr>                                              1 207V00000X Obstetrics & Gynecology                                                 2 207VC0300X Complex Family Planning                            3 207VC0200X Critical Care Medicine                             4 207VF0040X Female Pelvic Medicine and Reconstructive Surgery  5 207VX0201X Gynecologic Oncology                               6 207VG0400X Gynecology                                         7 207VH0002X Hospice and Palliative Medicine                    8 207VM0101X Maternal & Fetal Medicine                          9 207VB0002X Obesity Medicine                                  10 207VX0000X Obstetrics                                        11 207VE0102X Reproductive Endocrinology"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylersearch_by_taxonomy","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::search_by_taxonomy","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function searches NPI Database healthcare providers based taxonomy description. search_by_taxonomy function wrapper npi::npi_search accessing registry’s Version 2.1 API. Many thanks author maintainers npi package amazing work.helps confirm outside data subspecialist provider counts fill gaps providers board-certified practicing (board-eligible). data can matched databases. Please see Exploratory/workforce/subspecialists_only code . nice thing search results come NPI.","code":"# This will allow us to get subspecialty names and NPI numbers go_data <- search_by_taxonomy(\"Gynecologic Oncology\") fpmrs_data <- search_by_taxonomy(\"Female Pelvic Medicine and Reconstructive Surgery\") rei_data <- search_by_taxonomy(\"Reproductive Endocrinology\") mfm_data <- search_by_taxonomy(\"Maternal & Fetal Medicine\")  # Merge all data frames into one       all_taxonomy_search_data <- bind_rows(         go_data,         fpmrs_data,         rei_data,         mfm_data) %>%         dplyr::distinct(npi, .keep_all = TRUE)  dim(all_taxonomy_search_data) glimpse(all_taxonomy_search_data) # 1200 records requested # Requesting records 0-200... # Requesting records 200-400..."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylersearch_and_process_npi","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::search_and_process_npi","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"National Provider Identifier Search: Search first names, last names, individuals enumeration_type = \"ind\", physicians (\"MD\", \"\") United States NPPES. NPI numbers provide standardized way identify track healthcare providers, including physicians, across United States. Government agencies, Centers Medicare & Medicaid Services (CMS), use NPI-based data plan allocate healthcare resources, including provider reimbursements, medical services, workforce distribution.","code":"search_and_process_npi <- function(input_file,                                    enumeration_type = \"ind\",                                    limit = 5L,                                    country_code = \"US\",                                    filter_credentials = c(\"MD\", \"DO\"))  input_file <- \"/Users/tylermuffly/Dropbox (Personal)/Nomogram/nomogram/data/nppes_search/Lo_R_Author.csv\" output_result <- search_and_process_npi(input_file)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylervalidate_and_remove_invalid_npi","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::validate_and_remove_invalid_npi","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"cleans NPI numbers goes tyler::retrieve_clinician_data one incorrect NPI number inserted screws entire search. Saves find csv file.","code":"input_csv_path <- \"~/Dropbox (Personal)/workforce/subspecialists_only.csv\"  # Replace with the path to your CSV file valid_df <- validate_and_remove_invalid_npi(input_csv_path) Search result saved as: data/search_results_1053601807_20231119192903.csv                                 Search result saved as: data/search_results_1528351640_20231119192911.csv                                 ✖ No results for npi = 1063703494 No results for NPI: 1063703494"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylerretrieve_clinician_data","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::retrieve_clinician_data","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function retrieves clinician data based validated NPI numbers, using validate_and_remove_invalid_npi filter NPIs. logs successful searches, saves results CSV files, flags NPIs data found. results timestamped filenames, ensuring search history maintained. function vital efficiently gathering clinician data external sources storing future use. retrieve_clinician_data function retrieves clinician information Medicare Care Compare system. Previously, data accessed via Physician Compare, sunsetted December 2020. dataset can found CMS Provider Data. Physician Compare sunset December 1, 2020 replaced : https://www.medicare.gov/care-compare/?redirect=true&providerType=Physician. entire data set https://data.cms.gov/provider-data/dataset/mj5m-pzi6. cool library called provider super helpful accessing .","code":"# Call the retrieve_clinician_data function with an NPI value input_csv_path <- (\"~/Dropbox (Personal)/workforce/subspecialists_only.csv\") clinician_data <- tyler::retrieve_clinician_data(input_csv_path) ✖ No results for npi = 1093151441 NULL # A tibble: 3 × 17   npi     pac   enid  first last  gender school grad_year specialty facility_name pac_org members_org address_org city_org   <chr>   <chr> <chr> <chr> <chr> <fct>  <chr>      <int> <chr>     <chr>         <chr>         <int> <chr>       <chr>    1 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 25 MICHIGA… GRAND R… 2 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 4444 KALAM… KENTWOOD 3 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 4069 LAKE … GRAND R… # ℹ 3 more variables: state_org <ord>, zip_org <chr>, phone_org <chr>"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylergenderize_physicians","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::genderize_physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"wrapper around gender package help fill gender physician names. requires csv column called first_name. lot gender data found via Physician Compare past.","code":"tyler::genderize_physicians <- function(input_csv)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"searching-for-data-tylergeocode_unique_addresses","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::geocode_unique_addresses","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Takes csv file addresses prints lat long separate columns. need google_maps_api_key. Geocoding process converting human-readable addresses place names geographic coordinates (latitude longitude) can used locate places map. Google Geocoding API service provided Google allows developers perform geocoding reverse geocoding, process converting coordinates back human-readable addresses.","code":"output_data <-      tyler::geocode_unique_addresses(file_path = \"/Users/tylermuffly/Dropbox (Personal)/Tannous/data/address_for_geocoding.csv\",      google_maps_api_key = \"????\",      output_file_path = \"/Users/tylermuffly/Dropbox (Personal)/Tannous/data/geocoded_unique_addresses.csv\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"creating-mystery-caller-dataphase-1-tylercity_state_sample_specialists","dir":"","previous_headings":"","what":"Creating Mystery Caller Data/Phase 1: tyler::city_state_sample_specialists","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script samples specialists based city state data performs analyses based geographic location.","code":"# Example: Sampling specialists based on city and state data <- data.frame(city = c(\"Denver\", \"Chicago\"), state = c(\"CO\", \"IL\"), specialists = c(5, 3)) city_state_sample_specialists(data)  # Example with stratified sampling city_state_sample_specialists(data, stratified = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"creating-mystery-caller-dataphase-1-tylercity_state_assign_scenarios","dir":"","previous_headings":"","what":"Creating Mystery Caller Data/Phase 1: tyler::city_state_assign_scenarios","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script assigns city state data different scenarios, mapping geographic information scenario datasets.","code":"# Example: Assigning city and state to scenarios data <- data.frame(city = c(\"Denver\", \"Chicago\"), state = c(\"CO\", \"IL\")) city_state_assign_scenarios(data)  # Example with multiple scenarios scenarios <- list(scenario1 = data, scenario2 = data) city_state_assign_scenarios(scenarios)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerpoisson_formula_maker","dir":"","previous_headings":"","what":"tyler::poisson_formula_maker","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"poisson_formula_maker function constructs formula Poisson regression model based specified predictor variables. typically used running Poisson models, fit_poisson_models, streamline model fitting count-based data like waiting times.","code":"# Example: Create a Poisson regression formula poisson_formula <- poisson_formula_maker(predictor_vars = c(\"age\", \"gender\", \"insurance\"))  # View the formula print(poisson_formula)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylergenerate_latex_equation","dir":"","previous_headings":"","what":"tyler::generate_latex_equation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates LaTeX-formatted equation input model formula. commonly used fitting statistical models like regression present final equation document report. function complements functions like tyler::linear_regression_summary_sentence tyler::logistic_regression, generate human-readable summaries, allowing text formula outputs.","code":"# Example: Generate LaTeX equation from a linear model model <- lm(mpg ~ cyl + disp, data = mtcars) latex_equation <- generate_latex_equation(model)  # View the LaTeX equation print(latex_equation)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylergenerate_overall_table","dir":"","previous_headings":"","what":"tyler::generate_overall_table","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary table demographics based Table 1 data. supports multiple file formats, including RDS, CSV, XLS, logs key step, inputs, data transformations, file paths. can select specific columns apply custom label translations. ensures output directory exists saves generated table PDF. Error handling ensures function validates data logs issues occur execution.","code":"# Example: Generating an overall table generate_overall_table(input_file_path = \"data/Table1.rds\", output_directory = \"output_tables\")  # Example with selected columns generate_overall_table(input_file_path = \"data/Table1.csv\", output_directory = \"output_tables\", selected_columns = c(\"age\", \"gender\"))  # Example with label translations label_translations <- list(age = \"Age (years)\", gender = \"Gender\") generate_overall_table(input_file_path = \"data/Table1.xlsx\", output_directory = \"output_tables\", label_translations = label_translations)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercount_unique_physicians","dir":"","previous_headings":"","what":"tyler::count_unique_physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script counts number unique physicians dataset based unique identifier NPI.","code":"# Example: Counting unique physicians data <- data.frame(NPI = c(\"12345\", \"67890\", \"12345\", \"54321\")) count_unique_physicians(data)  # Example with a custom identifier column count_unique_physicians(data, id_column = \"NPI\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercalculate_descriptive_stats","dir":"","previous_headings":"","what":"tyler::calculate_descriptive_stats","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function computes descriptive statistics, means, medians, standard deviations datasets, covering categorical numerical variables. can customize calculate statistics selected columns. logs progress saves results structured format, making highly flexible analyzing datasets. function key quickly summarizing large datasets systematically.","code":"# Example: Calculating descriptive statistics data <- data.frame(age = c(23, 35, 40, 29, 50), gender = c(\"M\", \"F\", \"M\", \"F\", \"M\")) calculate_descriptive_stats(data)  # Example with selected columns calculate_descriptive_stats(data, selected_columns = c(\"age\"))"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercalc_percentages","dir":"","previous_headings":"","what":"tyler::calc_percentages","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates percentages given dataset. can calculate percentage category categorical variables optionally handle missing data.","code":"# Example: Calculating percentages data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")) calc_percentages(data)  # Example with missing data handling calc_percentages(data, handle_missing = TRUE)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercalculate_distribution","dir":"","previous_headings":"","what":"tyler::calculate_distribution","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates distribution values numerical categorical variables dataset.","code":"# Example: Calculating distribution data <- data.frame(value = c(1, 2, 2, 3, 3, 3, 4)) calculate_distribution(data)  # Example with categorical variables data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\")) calculate_distribution(data, variable_type = \"categorical\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercalculate_proportion","dir":"","previous_headings":"","what":"tyler::calculate_proportion","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates proportion specific event category dataset.","code":"# Example: Calculating proportion data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\")) calculate_proportion(data, event = \"A\")  # Example with a custom threshold calculate_proportion(data, event = \"B\", threshold = 0.2)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercheck_normality","dir":"","previous_headings":"","what":"tyler::check_normality","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script checks normality numeric dataset using statistical tests graphical methods QQ plots.","code":"# Example: Checking normality of a dataset data <- data.frame(value = c(10, 12, 15, 13, 14, 15, 18)) check_normality(data)  # Example with a specific significance level check_normality(data, significance_level = 0.05)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerpoisson_wait_time_stats","dir":"","previous_headings":"","what":"tyler::poisson_wait_time_stats","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function analyzes wait time statistics using Poisson model. designed model count-based data, like number appointments waiting times. complements poisson_formula_maker fit_poisson_models, helps understanding distribution determinants wait times.","code":"# Example: Calculate Poisson wait time statistics wait_time_stats <- poisson_wait_time_stats(df = appointments_data, target_variable = \"wait_time\")  # View the wait time statistics print(wait_time_stats)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_and_plot_interaction","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_forest_plot","dir":"","previous_headings":"","what":"tyler::create_forest_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_forest_plot function creates forest plot significant predictors, displaying coefficients confidence intervals. used fitting statistical models, Poisson logistic regression, visualize key predictors effects. function often paired model-fitting functions like fit_poisson_models fit_mixed_model_with_logging.","code":"# Example: Creating a forest plot for significant predictors df <- data.frame(predictor = c(\"age\", \"gender\", \"income\"), estimate = c(0.2, -0.5, 0.3), lower = c(0.1, -0.7, 0.2), upper = c(0.3, -0.3, 0.5)) create_forest_plot(df, \"target_variable\", significant_vars = df)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_insurance_by_insurance_scatter_plot","dir":"","previous_headings":"","what":"tyler::create_insurance_by_insurance_scatter_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_insurance_by_insurance_scatter_plot function creates scatter plot comparing waiting times two insurance types. complements data cleaning processing functions prepare appointment data visualization, determine_direction. plot useful visually comparing performance insurance providers terms wait times.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_line_plot","dir":"","previous_headings":"","what":"tyler::create_line_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_line_plot function generates line plots visualizing trends time continuous variables. commonly used calculating descriptive statistics analyzing trends across time-based data. works well summarized grouped data functions like calculate_descriptive_stats.","code":"# Example: Creating a line plot create_line_plot(df, x = \"year\", y = \"appointments\", group = \"insurance\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_scatter_plot","dir":"","previous_headings":"","what":"tyler::create_scatter_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function creates scatter plots, allowing comparison two continuous variables, optionally linear regression line. useful exploratory data analysis visualizing relationships variables. function complements descriptive statistics correlation analysis functions.","code":"# Example: Creating a scatter plot create_scatter_plot(df, x = \"income\", y = \"appointment_days\", add_regression = TRUE)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_and_plot_interaction-1","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerlinear_regression_summary_sentence","dir":"","previous_headings":"","what":"tyler::linear_regression_summary_sentence","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary sentence linear regression results, making easier present findings clear concise manner. typically used fitting linear regression model complements functions like generate_latex_equation providing textual formulaic summary results.","code":"# Example: Generate summary sentence for linear regression model <- lm(mpg ~ wt + hp, data = mtcars) summary_sentence <- linear_regression_summary_sentence(model)  # View the summary sentence print(summary_sentence)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerlogistic_regression","dir":"","previous_headings":"","what":"tyler::logistic_regression","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary sentence linear regression results, making easier present findings clear concise manner. typically used fitting linear regression model complements functions like generate_latex_equation providing textual formulaic summary results.","code":"# Example: Generate summary sentence for linear regression model <- lm(mpg ~ wt + hp, data = mtcars) summary_sentence <- linear_regression_summary_sentence(model)  # View the summary sentence print(summary_sentence)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylermaxtable","dir":"","previous_headings":"","what":"tyler::MaxTable","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"tyler::MaxTable function creates summary table highlights maximum values key metrics across categories. often used conjunction descriptive analysis functions, generate_overall_table, provide deeper insights maximum performance metrics.","code":"# Example: Generate a MaxTable for the dataset max_table <- MaxTable(df = mtcars, group_variable = \"cyl\", value_variable = \"mpg\")  # View the MaxTable print(max_table)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylermintable","dir":"","previous_headings":"","what":"tyler::MinTable","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"MinTable function creates summary table highlighting minimum values key metrics across different categories. often used alongside MaxTable provide complete range insights showing minimum performance outcomes within group. especially useful comparative studies.","code":"# Example: Generate a MinTable for the dataset min_table <- MinTable(df = mtcars, group_variable = \"cyl\", value_variable = \"mpg\")  # View the MinTable print(min_table)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylermost_common_gender_training_academic","dir":"","previous_headings":"","what":"tyler::most_common_gender_training_academic","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function identifies common gender training type among academic physicians. complements demographic analysis functions like physician_age useful studies focused characteristics healthcare providers, particularly academic settings.","code":"# Example: Identify the most common gender and training type among academic physicians common_gender_training <- most_common_gender_training_academic(df = physicians_data)  # View the results print(common_gender_training)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerphysician_age","dir":"","previous_headings":"","what":"tyler::physician_age","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"physician_age function calculates age physicians based birth year similar data. core function used demographic analyses often paired functions like most_common_gender_training_academic provide complete understanding physician workforce.","code":"Example: Calculate physician age physician_ages <- physician_age(df = physicians_data, birth_year_col = \"birth_year\")  # View the ages print(physician_ages)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerplot_and_save_emmeans","dir":"","previous_headings":"","what":"tyler::plot_and_save_emmeans","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function creates saves plots estimated marginal means (EMMeans) fitted model. often used running statistical models, Poisson mixed-effects models, visualize group differences. function complements fit_poisson_models fit_mixed_model_with_logging.","code":"# Example: Plot and save EMMeans from a fitted model emmeans_plot <- plot_and_save_emmeans(model = fitted_model, variables = c(\"age\", \"gender\"), output_dir = \"plots/\")  # View the plot print(emmeans_plot)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_and_plot_interaction-2","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_and_plot_interaction-3","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylercreate_and_plot_interaction-4","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerinstall_missing_packages","dir":"","previous_headings":"","what":"tyler::install_missing_packages","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"utility function checks missing R packages installs necessary. often used start analysis script ensure required packages installed. function complements function relies external packages, ensuring smooth workflow without interruptions.","code":"# Example: Install missing packages required_packages <- c(\"ggplot2\", \"dplyr\", \"readr\") install_missing_packages(required_packages)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylerload_data","dir":"","previous_headings":"","what":"tyler::load_data","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function loads data various file formats (e.g., CSV, RDS, Excel) prepares analysis. one first steps analysis workflow, providing clean data subsequent steps like model fitting visualization. complements functions rely dataset loaded memory.","code":"# Example: Load data from a CSV file data <- load_data(\"data/my_data.csv\")  # Example: Load data from an RDS file data <- load_data(\"data/my_data.rds\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"tylertm_write2pdf","dir":"","previous_headings":"","what":"tyler::tm_write2pdf","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function saves arsenal-generated table object PDF file. logs PDF saved ensures PDF written without unnecessary output. function streamlines process converting arsenal tables PDF files, retaining markdown elements ensuring efficient saving multiple summaries.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"welcome contributions! ’d like help improve tyler package, feel free submit issues pull requests.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"use package, appreciate citation.","code":"citation(\"tyler\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":null,"dir":"Reference","previous_headings":"","what":"ACOG Districts Data — ACOG_Districts","title":"ACOG Districts Data — ACOG_Districts","text":"dataset provides mapping U.S. states respective districts defined American College Obstetricians Gynecologists (ACOG). also includes state abbreviations subregion details additional geographic context.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACOG Districts Data — ACOG_Districts","text":"","code":"ACOG_Districts"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACOG Districts Data — ACOG_Districts","text":"tibble 52 rows 4 variables: State name U.S. state (e.g., \"Alabama\", \"California\"). ACOG_District district designation assigned ACOG (e.g.,\"District VII\"). Subregion subregion state within district(e.g.,\"District VII\"). State_Abbreviations two-letter postal abbreviation state(e.g., \"AL\", \"CA\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACOG Districts Data — ACOG_Districts","text":"Derived official ACOG district mappings documentation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ACOG Districts Data — ACOG_Districts","text":"dataset useful mapping regional analyses ACOG districts. Districts defined align organization's geographic administrative structure. Subregions often match ACOG districts can provide additional context needed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACOG Districts Data — ACOG_Districts","text":"","code":"# Load the dataset data(ACOG_Districts)  # View the first few rows head(ACOG_Districts) #> # A tibble: 6 × 4 #>   State      ACOG_District Subregion     State_Abbreviations #>   <chr>      <chr>         <chr>         <chr>               #> 1 Alabama    District VII  District VII  AL                  #> 2 Alaska     District VIII District VIII AK                  #> 3 Arizona    District VIII District VIII AZ                  #> 4 Arkansas   District VII  District VII  AR                  #> 5 California District IX   District IX   CA                  #> 6 Colorado   District VIII District VIII CO                   # Summarize the number of states in each ACOG district table(ACOG_Districts$ACOG_District) #>  #>    District I   District II  District III   District IV   District IX  #>             6             1             2             9             1  #>    District V   District VI  District VII District VIII   District XI  #>             4             7             8            12             1  #>  District XII  #>             1   # Filter for states in District VIII subset(ACOG_Districts, ACOG_District == \"District VIII\") #> # A tibble: 12 × 4 #>    State      ACOG_District Subregion     State_Abbreviations #>    <chr>      <chr>         <chr>         <chr>               #>  1 Alaska     District VIII District VIII AK                  #>  2 Arizona    District VIII District VIII AZ                  #>  3 Colorado   District VIII District VIII CO                  #>  4 Hawaii     District VIII District VIII HI                  #>  5 Idaho      District VIII District VIII ID                  #>  6 Montana    District VIII District VIII MT                  #>  7 Nevada     District VIII District VIII NV                  #>  8 New Mexico District VIII District VIII NM                  #>  9 Oregon     District VIII District VIII OR                  #> 10 Utah       District VIII District VIII UT                  #> 11 Washington District VIII District VIII WA                  #> 12 Wyoming    District VIII District VIII WY"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":null,"dir":"Reference","previous_headings":"","what":"ACOG Districts Spatial Data — ACOG_Districts_sf","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"dataset provides spatial geometries American College Obstetricians Gynecologists (ACOG) districts United States, including corresponding state names district designations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"","code":"ACOG_Districts_sf"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"sf object following columns: ACOG_District name ACOG district (e.g., \"District \", \"District VIII\"). geometry spatial geometry (polygons) district boundaries.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"State geometries obtained TIGER/Line shapefiles via tigris R package. ACOG district information sourced official ACOG website: https://www.acog.org/community/districts--sections.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"dataset structured sf object includes merged geometries ACOG district, allowing visualization spatial analysis district boundaries. ACOG_Districts_sf object spatial dataset can used map ACOG districts perform geospatial operations. district boundaries created merging state geometries according ACOG district mapping.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ACOG_Districts_sf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACOG Districts Spatial Data — ACOG_Districts_sf","text":"","code":"# Load the ACOG Districts Spatial Data data(ACOG_Districts_sf)  # View the structure of the dataset str(ACOG_Districts_sf) #> sf [11 × 2] (S3: sf/tbl_df/tbl/data.frame) #>  $ ACOG_District: chr [1:11] \"District I\" \"District II\" \"District III\" \"District IV\" ... #>  $ geometry     :sfc_MULTIPOLYGON of length 11; first list element: List of 270 #>   ..$ :List of 1 #>   .. ..$ : num [1:53, 1:2] -67.3 -67.3 -67.3 -67.3 -67.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.3 -67.3 -67.3 -67.3 -67.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.3 -67.3 -67.3 -67.3 -67.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:9, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:14, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:5, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.4 -67.4 -67.4 -67.4 -67.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:39, 1:2] -67.5 -67.5 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:27, 1:2] -67.6 -67.6 -67.5 -67.5 -67.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:67, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:11, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:18, 1:2] -67.6 -67.6 -67.6 -67.6 -67.6 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:9, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:12, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:5, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.7 -67.7 -67.7 -67.7 -67.7 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:21, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:15, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -67.8 -67.8 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -67.9 -67.9 -67.8 -67.8 -67.8 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -67.9 -67.9 -67.9 -67.9 -67.9 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:24, 1:2] -67.9 -67.9 -67.9 -67.9 -67.9 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68 -68 -68 -68 -68 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68.1 -68.1 -68.1 -68.1 -68.1 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68.1 -68.1 -68.1 -68.1 -68.1 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:15, 1:2] -68.1 -68.1 -68.1 -68.1 -68.1 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:14, 1:2] -68.1 -68.1 -68.1 -68.1 -68.1 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -68.2 -68.2 -68.2 -68.1 -68.1 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:11, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:9, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:20, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -68.2 -68.2 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:14, 1:2] -68.3 -68.3 -68.2 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68.3 -68.3 -68.3 -68.2 -68.2 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.3 -68.3 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:18, 1:2] -68.3 -68.3 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:10, 1:2] -68.3 -68.3 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:21, 1:2] -68.3 -68.3 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:11, 1:2] -68.3 -68.3 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -68.4 -68.4 -68.3 -68.3 -68.3 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:37, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:15, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:8, 1:2] -68.4 -68.4 -68.4 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -68.5 -68.5 -68.5 -68.5 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:44, 1:2] -68.5 -68.5 -68.5 -68.4 -68.4 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:6, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:9, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:7, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:13, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:12, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:5, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:84, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:23, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   ..$ :List of 1 #>   .. ..$ : num [1:26, 1:2] -68.5 -68.5 -68.5 -68.5 -68.5 ... #>   .. [list output truncated] #>   ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\" #>  - attr(*, \"sf_column\")= chr \"geometry\" #>  - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA #>   ..- attr(*, \"names\")= chr \"ACOG_District\"  # Plot the district boundaries using ggplot2 library(ggplot2) ggplot(ACOG_Districts_sf) +   geom_sf() +   ggtitle(\"ACOG Districts in the United States\") +   theme_minimal()"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"function calculates maximum frequency factor variable returns level(s) corresponding maximum value(s). can return either single maximum levels maximum frequency.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"","code":"MaxTable(InVec, mult = FALSE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"InVec Input vector, expected factor variable convertible factor. mult Logical value indicating whether return multiple maximum levels just first one. Default FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"character vector. mult FALSE, returns level highest frequency. mult TRUE, returns levels maximum frequency.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"function handles factor variables can converted factors. uses frequency counts determine level(s) maximum frequency.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MaxTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"","code":"# Example 1: Single maximum level vec <- factor(c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\")) MaxTable(vec) # Returns \"B\" #> [1] \"B\"  # Example 2: Multiple maximum levels vec <- factor(c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\")) MaxTable(vec, mult = TRUE) # Returns c(\"A\", \"B\") #> [1] \"B\"  # Example 3: Handling a non-factor input vec <- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\") MaxTable(vec) # Returns \"B\" #> [1] \"B\"  # Example 4: Empty input vector vec <- factor(character(0)) MaxTable(vec) # Returns character(0) #> Error: Input vector must not be empty."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MinTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"function returns level(s) corresponding minimum value(s) factor variable.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MinTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"","code":"MinTable(InVec, mult = FALSE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MinTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"InVec Input vector, expected factor variable convertible factor. mult Logical value indicating whether return multiple minimum values just first one. Default FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MinTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"mult FALSE, returns level corresponding minimum value factor variable. mult TRUE, returns character vector containing levels minimum value.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/MinTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"","code":"vec <- factor(c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\")) MinTable(vec) # Returns \"C\" #> [1] \"C\" MinTable(vec, mult = TRUE) # Returns \"C\" #> [1] \"C\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":null,"dir":"Reference","previous_headings":"","what":"ACGME OBGYN Residency Program Data — acgme","title":"ACGME OBGYN Residency Program Data — acgme","text":"dataset provides comprehensive information Obstetrics Gynecology (OBGYN) residency programs accredited Accreditation Council Graduate Medical Education (ACGME). includes program details addresses, accreditation status, program leadership, participating sites, rotation details.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACGME OBGYN Residency Program Data — acgme","text":"","code":"acgme"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACGME OBGYN Residency Program Data — acgme","text":"tibble 318 rows 142 variables: program_name name OBGYN residency program. address program's mailing address. zip program's ZIP code. city city program located. state state program located. sponsoring_institution_code code sponsoring institution. sponsoring_institution_name name sponsoring institution. phone main contact phone number program. original_accreditation_date date program first received accreditation. accreditation_status current accreditation status (e.g., \"Continued Accreditation\"). director_name name program director. director_date_appointed date program director appointed. coordinator_name_1 name program coordinator. coordinator_phone_1 phone number program coordinator. coordinator_email_1 email address program coordinator. participation_site_code_X code participating site X (X ranges 1 18). participation_site_name_X name participating site X (X ranges 1 18). rotation_required_X Indicates rotation required site X (X ranges 1 18, values \"Yes\" \"\"). rotation_months_yY_X number months allocated rotations site X year Y (X ranges 1 18 Y ranges 1 4).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACGME OBGYN Residency Program Data — acgme","text":"Data obtained ACGME website: https://apps.acgme.org/ads/Public/Programs/Search","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ACGME OBGYN Residency Program Data — acgme","text":"dataset particularly useful understanding structure requirements OBGYN residency programs across United States. Rotations detailed site year, allowing comprehensive planning analysis. Data includes program leadership details facilitate communication networking.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acgme.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACGME OBGYN Residency Program Data — acgme","text":"","code":"# Load the ACGME OBGYN Residency Data data(acgme)  # View the first few rows of the dataset head(acgme) #> # A tibble: 6 × 142 #>   program_name                  address zip   city  state sponsoring_instituti…¹ #>   <chr>                         <chr>   <chr> <chr> <chr> <chr>                  #> 1 University of Alabama Medica… \"Unive… 35249 Birm… Alab… 010498                 #> 2 USA Health Program            \"Unive… 36604 Mobi… Alab… 010406                 #> 3 University of Arizona Colleg… \"Banne… 85006 Phoe… Ariz… 038179                 #> 4 University of Arizona Colleg… \"Unive… 85724 Tucs… Ariz… 030509                 #> 5 Creighton University School … \"Creig… 85008 Phoe… Ariz… 309502                 #> 6 University of Arkansas for M… \"Unive… 72205 Litt… Arka… 049501                 #> # ℹ abbreviated name: ¹​sponsoring_institution_code #> # ℹ 136 more variables: sponsoring_institution_name <chr>, phone <chr>, #> #   original_accreditation_date <chr>, accreditation_status <chr>, #> #   director_name <chr>, director_date_appointed <chr>, #> #   coordinator_name_1 <chr>, coordinator_phone_1 <chr>, #> #   coordinator_email_1 <chr>, participation_site_code_1 <chr>, #> #   participation_site_name_1 <chr>, rotation_required_1 <chr>, …  # Summarize the dataset summary(acgme) #>  program_name         address              zip                city           #>  Length:318         Length:318         Length:318         Length:318         #>  Class :character   Class :character   Class :character   Class :character   #>  Mode  :character   Mode  :character   Mode  :character   Mode  :character   #>                                                                              #>                                                                              #>                                                                              #>                                                                              #>     state           sponsoring_institution_code sponsoring_institution_name #>  Length:318         Length:318                  Length:318                  #>  Class :character   Class :character            Class :character            #>  Mode  :character   Mode  :character            Mode  :character            #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>     phone           original_accreditation_date accreditation_status #>  Length:318         Length:318                  Length:318           #>  Class :character   Class :character            Class :character     #>  Mode  :character   Mode  :character            Mode  :character     #>                                                                      #>                                                                      #>                                                                      #>                                                                      #>  director_name      director_date_appointed coordinator_name_1 #>  Length:318         Length:318              Length:318         #>  Class :character   Class :character        Class :character   #>  Mode  :character   Mode  :character        Mode  :character   #>                                                                #>                                                                #>                                                                #>                                                                #>  coordinator_phone_1 coordinator_email_1 participation_site_code_1 #>  Length:318          Length:318          Length:318                #>  Class :character    Class :character    Class :character          #>  Mode  :character    Mode  :character    Mode  :character          #>                                                                    #>                                                                    #>                                                                    #>                                                                    #>  participation_site_name_1 rotation_required_1 rotation_months_y1_1 #>  Length:318                Length:318          Min.   : 0.000       #>  Class :character          Class :character    1st Qu.: 9.425       #>  Mode  :character          Mode  :character    Median :11.750       #>                                                Mean   :10.277       #>                                                3rd Qu.:12.000       #>                                                Max.   :13.000       #>                                                                     #>  rotation_months_y2_1 rotation_months_y3_1 rotation_months_y4_1 #>  Min.   : 0.000       Min.   : 0.000       Min.   : 0.000       #>  1st Qu.: 8.000       1st Qu.: 7.000       1st Qu.: 8.000       #>  Median :10.550       Median : 9.000       Median :10.500       #>  Mean   : 9.581       Mean   : 8.895       Mean   : 9.696       #>  3rd Qu.:12.000       3rd Qu.:11.000       3rd Qu.:12.000       #>  Max.   :13.000       Max.   :13.000       Max.   :13.000       #>                                                                 #>  participation_site_code_2 participation_site_name_2 rotation_required_2 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_2 rotation_months_y2_2 rotation_months_y3_2 #>  Min.   : 0.000       Min.   : 0.0         Min.   : 0.000       #>  1st Qu.: 0.000       1st Qu.: 0.0         1st Qu.: 0.500       #>  Median : 0.200       Median : 1.0         Median : 1.200       #>  Mean   : 1.494       Mean   : 1.9         Mean   : 2.066       #>  3rd Qu.: 2.000       3rd Qu.: 3.0         3rd Qu.: 3.000       #>  Max.   :13.000       Max.   :12.0         Max.   :12.000       #>  NA's   :39           NA's   :39           NA's   :39           #>  rotation_months_y4_2 participation_site_code_3 participation_site_name_3 #>  Min.   : 0.000       Length:318                Length:318                #>  1st Qu.: 0.000       Class :character          Class :character          #>  Median : 1.000       Mode  :character          Mode  :character          #>  Mean   : 1.719                                                           #>  3rd Qu.: 2.000                                                           #>  Max.   :12.000                                                           #>  NA's   :39                                                               #>  rotation_required_3 rotation_months_y1_3 rotation_months_y2_3 #>  Length:318          Min.   : 0.0000      Min.   : 0.000       #>  Class :character    1st Qu.: 0.0000      1st Qu.: 0.000       #>  Mode  :character    Median : 0.0000      Median : 0.300       #>                      Mean   : 0.7106      Mean   : 1.009       #>                      3rd Qu.: 1.0000      3rd Qu.: 1.500       #>                      Max.   :12.0000      Max.   :10.000       #>                      NA's   :91           NA's   :91           #>  rotation_months_y3_3 rotation_months_y4_3 participation_site_code_4 #>  Min.   :0.000        Min.   : 0.000       Length:318                #>  1st Qu.:0.000        1st Qu.: 0.000       Class :character          #>  Median :1.000        Median : 0.000       Mode  :character          #>  Mean   :1.298        Mean   : 1.041                                 #>  3rd Qu.:2.000        3rd Qu.: 1.500                                 #>  Max.   :9.000        Max.   :10.000                                 #>  NA's   :91           NA's   :91                                     #>  participation_site_name_4 rotation_required_4 rotation_months_y1_4 #>  Length:318                Length:318          Min.   :0.0000       #>  Class :character          Class :character    1st Qu.:0.0000       #>  Mode  :character          Mode  :character    Median :0.0000       #>                                                Mean   :0.3675       #>                                                3rd Qu.:0.1000       #>                                                Max.   :5.0000       #>                                                NA's   :149          #>  rotation_months_y2_4 rotation_months_y3_4 rotation_months_y4_4 #>  Min.   :0.0000       Min.   :0.0000       Min.   : 0.0000      #>  1st Qu.:0.0000       1st Qu.:0.0000       1st Qu.: 0.0000      #>  Median :0.0000       Median :0.5000       Median : 0.0000      #>  Mean   :0.6509       Mean   :0.8882       Mean   : 0.7787      #>  3rd Qu.:1.0000       3rd Qu.:1.0000       3rd Qu.: 1.0000      #>  Max.   :5.0000       Max.   :6.0000       Max.   :12.0000      #>  NA's   :149          NA's   :149          NA's   :149          #>  participation_site_code_5 participation_site_name_5 rotation_required_5 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_5 rotation_months_y2_5 rotation_months_y3_5 #>  Min.   : 0.0000      Min.   : 0.0000      Min.   : 0.0000      #>  1st Qu.: 0.0000      1st Qu.: 0.0000      1st Qu.: 0.0000      #>  Median : 0.0000      Median : 0.0000      Median : 0.5000      #>  Mean   : 0.2991      Mean   : 0.6487      Mean   : 0.9584      #>  3rd Qu.: 0.0000      3rd Qu.: 1.0000      3rd Qu.: 1.0000      #>  Max.   :12.0000      Max.   :12.0000      Max.   :12.0000      #>  NA's   :205          NA's   :205          NA's   :205          #>  rotation_months_y4_5 participation_site_code_6 rotation_required_6 #>  Min.   : 0.0000      Length:318                Length:318          #>  1st Qu.: 0.0000      Class :character          Class :character    #>  Median : 0.1000      Mode  :character          Mode  :character    #>  Mean   : 0.6912                                                    #>  3rd Qu.: 1.0000                                                    #>  Max.   :12.0000                                                    #>  NA's   :205                                                        #>  rotation_months_y1_6 rotation_months_y2_6 rotation_months_y3_6 #>  Min.   :0.000        Min.   :0.0000       Min.   :0.0000       #>  1st Qu.:0.000        1st Qu.:0.0000       1st Qu.:0.0000       #>  Median :0.000        Median :0.0000       Median :0.1000       #>  Mean   :0.175        Mean   :0.4789       Mean   :0.7421       #>  3rd Qu.:0.000        3rd Qu.:0.6000       3rd Qu.:1.0000       #>  Max.   :2.500        Max.   :5.0000       Max.   :6.0000       #>  NA's   :242          NA's   :242          NA's   :242          #>  rotation_months_y4_6 participation_site_code_7 participation_site_name_7 #>  Min.   :0.0000       Length:318                Length:318                #>  1st Qu.:0.0000       Class :character          Class :character          #>  Median :0.3000       Mode  :character          Mode  :character          #>  Mean   :0.7474                                                           #>  3rd Qu.:1.0000                                                           #>  Max.   :7.0000                                                           #>  NA's   :242                                                              #>  rotation_required_7 rotation_months_y1_7 rotation_months_y2_7 #>  Length:318          Min.   :0.0000       Min.   :0.0000       #>  Class :character    1st Qu.:0.0000       1st Qu.:0.0000       #>  Mode  :character    Median :0.0000       Median :0.0000       #>                      Mean   :0.1907       Mean   :0.4628       #>                      3rd Qu.:0.0000       3rd Qu.:0.5000       #>                      Max.   :4.0000       Max.   :4.5000       #>                      NA's   :275          NA's   :275          #>  rotation_months_y3_7 rotation_months_y4_7 participation_site_code_8 #>  Min.   :0.0000       Min.   :0.0000       Length:318                #>  1st Qu.:0.0000       1st Qu.:0.0000       Class :character          #>  Median :0.5000       Median :0.2000       Mode  :character          #>  Mean   :0.8605       Mean   :0.7814                                 #>  3rd Qu.:1.0000       3rd Qu.:1.0000                                 #>  Max.   :4.5000       Max.   :7.0000                                 #>  NA's   :275          NA's   :275                                    #>  participation_site_name_8 rotation_required_8 rotation_months_y1_8 #>  Length:318                Length:318          Min.   :0.0          #>  Class :character          Class :character    1st Qu.:0.0          #>  Mode  :character          Mode  :character    Median :0.0          #>                                                Mean   :0.3          #>                                                3rd Qu.:0.0          #>                                                Max.   :3.2          #>                                                NA's   :297          #>  rotation_months_y2_8 rotation_months_y3_8 rotation_months_y4_8 #>  Min.   :0.0000       Min.   :0.0000       Min.   :0.000        #>  1st Qu.:0.0000       1st Qu.:0.0000       1st Qu.:0.000        #>  Median :0.0000       Median :0.5000       Median :0.500        #>  Mean   :0.4857       Mean   :0.5524       Mean   :0.819        #>  3rd Qu.:1.0000       3rd Qu.:1.0000       3rd Qu.:1.000        #>  Max.   :3.0000       Max.   :2.0000       Max.   :4.000        #>  NA's   :297          NA's   :297          NA's   :297          #>  participation_site_code_9 participation_site_name_9 rotation_required_9 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_9 rotation_months_y2_9 rotation_months_y3_9 #>  Min.   :0.00000      Min.   :0.0000       Min.   :0.0000       #>  1st Qu.:0.00000      1st Qu.:0.0000       1st Qu.:0.0000       #>  Median :0.00000      Median :0.0000       Median :0.5000       #>  Mean   :0.06923      Mean   :0.2692       Mean   :0.5154       #>  3rd Qu.:0.00000      3rd Qu.:0.4000       3rd Qu.:1.0000       #>  Max.   :0.90000      Max.   :1.0000       Max.   :1.0000       #>  NA's   :305          NA's   :305          NA's   :305          #>  rotation_months_y4_9 participation_site_code_10 participation_site_name_10 #>  Min.   :0.0000       Length:318                 Length:318                 #>  1st Qu.:0.1000       Class :character           Class :character           #>  Median :1.0000       Mode  :character           Mode  :character           #>  Mean   :0.7769                                                             #>  3rd Qu.:1.0000                                                             #>  Max.   :3.0000                                                             #>  NA's   :305                                                                #>  rotation_required_10 rotation_months_y1_10 rotation_months_y2_10 #>  Length:318           Min.   :0.0000        Min.   :0.0000        #>  Class :character     1st Qu.:0.0000        1st Qu.:0.0000        #>  Mode  :character     Median :0.0000        Median :0.0000        #>                       Mean   :0.1111        Mean   :0.2333        #>                       3rd Qu.:0.0000        3rd Qu.:0.1000        #>                       Max.   :1.0000        Max.   :1.0000        #>                       NA's   :309           NA's   :309           #>  rotation_months_y3_10 rotation_months_y4_10 participation_site_code_11 #>  Min.   :0.0000        Min.   :0.0000        Length:318                 #>  1st Qu.:0.1000        1st Qu.:0.0000        Class :character           #>  Median :0.7000        Median :1.0000        Mode  :character           #>  Mean   :0.5889        Mean   :0.6778                                   #>  3rd Qu.:1.0000        3rd Qu.:1.0000                                   #>  Max.   :1.0000        Max.   :2.0000                                   #>  NA's   :309           NA's   :309                                      #>  participation_site_name_11 rotation_required_11 rotation_months_y1_11 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :313           #>  rotation_months_y2_11 rotation_months_y3_11 rotation_months_y4_11 #>  Min.   :0.0           Min.   :0.00          Min.   :0.0           #>  1st Qu.:0.0           1st Qu.:0.10          1st Qu.:0.0           #>  Median :1.0           Median :0.70          Median :0.0           #>  Mean   :0.6           Mean   :0.56          Mean   :0.4           #>  3rd Qu.:1.0           3rd Qu.:1.00          3rd Qu.:1.0           #>  Max.   :1.0           Max.   :1.00          Max.   :1.0           #>  NA's   :313           NA's   :313           NA's   :313           #>  participation_site_code_12 participation_site_name_12 rotation_required_12 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_12 rotation_months_y2_12 rotation_months_y3_12 #>  Min.   : 0.100        Min.   : 0.000        Min.   : 0.000        #>  1st Qu.: 0.775        1st Qu.: 0.075        1st Qu.: 0.075        #>  Median : 6.000        Median : 5.050        Median : 5.050        #>  Mean   : 5.775        Mean   : 5.025        Mean   : 5.025        #>  3rd Qu.:11.000        3rd Qu.:10.000        3rd Qu.:10.000        #>  Max.   :11.000        Max.   :10.000        Max.   :10.000        #>  NA's   :314           NA's   :314           NA's   :314           #>  rotation_months_y4_12 participation_site_code_13 participation_site_name_13 #>  Min.   : 0.000        Length:318                 Length:318                 #>  1st Qu.: 0.075        Class :character           Class :character           #>  Median : 6.050        Mode  :character           Mode  :character           #>  Mean   : 6.025                                                              #>  3rd Qu.:12.000                                                              #>  Max.   :12.000                                                              #>  NA's   :314                                                                 #>  rotation_required_13 rotation_months_y1_13 rotation_months_y2_13 #>  Length:318           Min.   :0             Min.   :0.0           #>  Class :character     1st Qu.:0             1st Qu.:0.1           #>  Mode  :character     Median :0             Median :0.2           #>                       Mean   :0             Mean   :0.2           #>                       3rd Qu.:0             3rd Qu.:0.3           #>                       Max.   :0             Max.   :0.4           #>                       NA's   :316           NA's   :316           #>  rotation_months_y3_13 rotation_months_y4_13 participation_site_code_14 #>  Min.   :0.000         Min.   :0.000         Length:318                 #>  1st Qu.:0.025         1st Qu.:0.025         Class :character           #>  Median :0.050         Median :0.050         Mode  :character           #>  Mean   :0.050         Mean   :0.050                                    #>  3rd Qu.:0.075         3rd Qu.:0.075                                    #>  Max.   :0.100         Max.   :0.100                                    #>  NA's   :316           NA's   :316                                      #>  participation_site_name_14 rotation_required_14 rotation_months_y1_14 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :317           #>  rotation_months_y2_14 rotation_months_y3_14 rotation_months_y4_14 #>  Min.   :0             Min.   :0.1           Min.   :0             #>  1st Qu.:0             1st Qu.:0.1           1st Qu.:0             #>  Median :0             Median :0.1           Median :0             #>  Mean   :0             Mean   :0.1           Mean   :0             #>  3rd Qu.:0             3rd Qu.:0.1           3rd Qu.:0             #>  Max.   :0             Max.   :0.1           Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  participation_site_code_15 participation_site_name_15 rotation_required_15 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_15 rotation_months_y2_15 rotation_months_y3_15 #>  Min.   :0.1           Min.   :0             Min.   :0             #>  1st Qu.:0.1           1st Qu.:0             1st Qu.:0             #>  Median :0.1           Median :0             Median :0             #>  Mean   :0.1           Mean   :0             Mean   :0             #>  3rd Qu.:0.1           3rd Qu.:0             3rd Qu.:0             #>  Max.   :0.1           Max.   :0             Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  rotation_months_y4_15 participation_site_code_16 participation_site_name_16 #>  Min.   :0             Length:318                 Length:318                 #>  1st Qu.:0             Class :character           Class :character           #>  Median :0             Mode  :character           Mode  :character           #>  Mean   :0                                                                   #>  3rd Qu.:0                                                                   #>  Max.   :0                                                                   #>  NA's   :317                                                                 #>  rotation_required_16 rotation_months_y1_16 rotation_months_y2_16 #>  Length:318           Min.   :0             Min.   :0             #>  Class :character     1st Qu.:0             1st Qu.:0             #>  Mode  :character     Median :0             Median :0             #>                       Mean   :0             Mean   :0             #>                       3rd Qu.:0             3rd Qu.:0             #>                       Max.   :0             Max.   :0             #>                       NA's   :317           NA's   :317           #>  rotation_months_y3_16 rotation_months_y4_16 participation_site_code_17 #>  Min.   :0             Min.   :0             Length:318                 #>  1st Qu.:0             1st Qu.:0             Class :character           #>  Median :0             Median :0             Mode  :character           #>  Mean   :0             Mean   :0                                        #>  3rd Qu.:0             3rd Qu.:0                                        #>  Max.   :0             Max.   :0                                        #>  NA's   :317           NA's   :317                                      #>  participation_site_name_17 rotation_required_17 rotation_months_y1_17 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :317           #>  rotation_months_y2_17 rotation_months_y3_17 rotation_months_y4_17 #>  Min.   :0             Min.   :0             Min.   :0             #>  1st Qu.:0             1st Qu.:0             1st Qu.:0             #>  Median :0             Median :0             Median :0             #>  Mean   :0             Mean   :0             Mean   :0             #>  3rd Qu.:0             3rd Qu.:0             3rd Qu.:0             #>  Max.   :0             Max.   :0             Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  participation_site_code_18 participation_site_name_18 rotation_required_18 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_18 rotation_months_y2_18 rotation_months_y3_18 #>  Min.   :0             Min.   :0             Min.   :0.6           #>  1st Qu.:0             1st Qu.:0             1st Qu.:0.6           #>  Median :0             Median :0             Median :0.6           #>  Mean   :0             Mean   :0             Mean   :0.6           #>  3rd Qu.:0             3rd Qu.:0             3rd Qu.:0.6           #>  Max.   :0             Max.   :0             Max.   :0.6           #>  NA's   :317           NA's   :317           NA's   :317           #>  rotation_months_y4_18   website           program_code       #>  Min.   :0.3           Length:318         Min.   :2.200e+09   #>  1st Qu.:0.3           Class :character   1st Qu.:2.202e+09   #>  Median :0.3           Mode  :character   Median :2.203e+09   #>  Mean   :0.3                              Mean   :2.203e+09   #>  3rd Qu.:0.3                              3rd Qu.:2.204e+09   #>  Max.   :0.3                              Max.   :2.206e+09   #>  NA's   :317                                                   # Analyze the number of programs by state table(acgme$state) #>  #>              Alabama              Arizona             Arkansas  #>                    2                    3                    1  #>           California             Colorado          Connecticut  #>                   27                    2                    6  #>             Delaware District of Columbia              Florida  #>                    1                    4                   14  #>              Georgia               Hawaii             Illinois  #>                    7                    2                   15  #>              Indiana                 Iowa               Kansas  #>                    2                    1                    3  #>             Kentucky            Louisiana                Maine  #>                    3                    5                    1  #>             Maryland        Massachusetts             Michigan  #>                    7                    6                   24  #>            Minnesota          Mississippi             Missouri  #>                    2                    1                    5  #>             Nebraska               Nevada        New Hampshire  #>                    2                    2                    1  #>           New Jersey           New Mexico             New York  #>                   16                    1                   42  #>       North Carolina                 Ohio             Oklahoma  #>                    9                   16                    5  #>               Oregon         Pennsylvania          Puerto Rico  #>                    1                   21                    3  #>         Rhode Island       South Carolina            Tennessee  #>                    1                    4                    8  #>                Texas                 Utah              Vermont  #>                   24                    1                    1  #>             Virginia           Washington        West Virginia  #>                    7                    3                    3  #>            Wisconsin  #>                    3   # Filter programs in California subset(acgme, state == \"California\") #> # A tibble: 27 × 142 #>    program_name                 address zip   city  state sponsoring_instituti…¹ #>    <chr>                        <chr>   <chr> <chr> <chr> <chr>                  #>  1 HCA Healthcare Riverside  P… \"River… 92501 Rive… Cali… 059514                 #>  2 University of California Ri… \"River… 92501 Rive… Cali… 059511                 #>  3 Arrowhead Regional Medical … \"400 N… 92324 Colt… Cali… 050207                 #>  4 Marian Regional Medical Cen… \"1400 … 93454 Sant… Cali… 059593                 #>  5 UHS Southern California Med… \"25500… 25500 Murr… Cali… 059802                 #>  6 Naval Medical Center (San D… \"Naval… 34730 San … Cali… 050386                 #>  7 University of Southern Cali… \"LAC+U… 90033 Los … Cali… 058116                 #>  8 Kaiser Permanente Southern … \"Kaise… 90027 Los … Cali… 058072                 #>  9 Kaiser Permanente Northern … \"Kaise… 94611 Oakl… Cali… 058090                 #> 10 Kaiser Permanente Northern … \"Kaise… 94115 San … Cali… 058090                 #> # ℹ 17 more rows #> # ℹ abbreviated name: ¹​sponsoring_institution_code #> # ℹ 136 more variables: sponsoring_institution_name <chr>, phone <chr>, #> #   original_accreditation_date <chr>, accreditation_status <chr>, #> #   director_name <chr>, director_date_appointed <chr>, #> #   coordinator_name_1 <chr>, coordinator_phone_1 <chr>, #> #   coordinator_email_1 <chr>, participation_site_code_1 <chr>, …"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":null,"dir":"Reference","previous_headings":"","what":"ACOG Districts — acog_districts_df","title":"ACOG Districts — acog_districts_df","text":"dataset maps U.S. states respective districts defined American College Obstetricians Gynecologists (ACOG). ACOG districts used organize support members based geographic regions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACOG Districts — acog_districts_df","text":"","code":"acog_districts_df"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACOG Districts — acog_districts_df","text":"data frame 52 rows 2 variables: State name U.S. state (e.g., \"California\", \"Texas\"). ACOG_District corresponding ACOG district (e.g., \"District \", \"District VII\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACOG Districts — acog_districts_df","text":"ACOG district mappings official website: https://www.acog.org/community/districts--sections. State-level geographic data U.S. Census Bureau: https://www.census.gov.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ACOG Districts — acog_districts_df","text":"Districts include states, U.S. territories, specific regions like Washington, D.C. District XII includes Florida standalone district. Districts commonly used organizing conferences, membership activities, policy efforts.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ACOG Districts — acog_districts_df","text":"American College Obstetricians Gynecologists (ACOG). \"Districts Sections.\" Retrieved https://www.acog.org/community/districts--sections.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_districts_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACOG Districts — acog_districts_df","text":"","code":"# Load the dataset data(acog_districts_df)  # Summarize the number of states in each ACOG district table(acog_districts_df$ACOG_District) #>  #>    District I   District II  District III   District IV    District V  #>             6             1             3             8             4  #>   District VI  District VII District VIII   District IX    District X  #>             8             8            11             1             0  #>   District XI  District XII  #>             1             1   # Filter for states in District VIII subset(acog_districts_df, ACOG_District == \"District VIII\") #>         State ACOG_District #> 2      Alaska District VIII #> 3     Arizona District VIII #> 6    Colorado District VIII #> 11     Hawaii District VIII #> 25    Montana District VIII #> 27     Nevada District VIII #> 30 New Mexico District VIII #> 36     Oregon District VIII #> 44       Utah District VIII #> 47 Washington District VIII #> 51      Idaho District VIII"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_presidents.html","id":null,"dir":"Reference","previous_headings":"","what":"ACOG Presidents Dataset — acog_presidents","title":"ACOG Presidents Dataset — acog_presidents","text":"dataset contains information past presidents American College Obstetricians Gynecologists (ACOG).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_presidents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACOG Presidents Dataset — acog_presidents","text":"","code":"acog_presidents"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_presidents.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACOG Presidents Dataset — acog_presidents","text":"tibble 71 rows 6 variables: first first name president. last last name president. middle middle name initial president, available. President full name president. honorrific honorific degree president (e.g., \"MD\"). Presidency year presidency.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/acog_presidents.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACOG Presidents Dataset — acog_presidents","text":"","code":"data(acog_presidents) head(acog_presidents) #> # A tibble: 6 × 6 #>   first   last     middle President          honorrific Presidency #>   <chr>   <chr>    <chr>  <chr>              <chr>           <dbl> #> 1 J       Tucker   Martin J. Martin Tucker   MD               2021 #> 2 Eva     Chalas   NA     Eva Chalas         MD               2020 #> 3 Ted     Anderson L      Ted L. Anderson    MD               2019 #> 4 Lisa    Hollier  M      Lisa M. Hollier    MD               2018 #> 5 Haywood Brown    L      Haywood L. Brown   MD               2017 #> 6 Thomas  Gellhaus M      Thomas M. Gellhaus MD               2016"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/apply_and_save_tint.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply and Save Image Tint — apply_and_save_tint","title":"Apply and Save Image Tint — apply_and_save_tint","text":"function applies tint image saves tinted image. tints determined given color palette.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/apply_and_save_tint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply and Save Image Tint — apply_and_save_tint","text":"","code":"apply_and_save_tint(   image_path,   alpha = 0.5,   palette_name = \"viridis\",   save_dir = \".\",   n_colors = 5 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/apply_and_save_tint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply and Save Image Tint — apply_and_save_tint","text":"image_path Character string specifying file path image tinted. alpha Numeric value 0 1 specifying degree tint. Higher values result stronger tints. palette_name Character string specifying color palette use tint. Default 'viridis'. save_dir Character string specifying directory save tinted images. n_colors Integer specifying number different colors use color palette.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/apply_and_save_tint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply and Save Image Tint — apply_and_save_tint","text":"Saves tinted images specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2pdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"function takes Arsenal Table object saves PDF file \"tables\" directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2pdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"","code":"arsenal_tables_write2pdf(object, filename)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2pdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"object Arsenal Table object. filename string representing desired filename without extension.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2pdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"","code":"if (FALSE) { # \\dontrun{ overall <- summary(   overall_arsenal_table,   text = T,   title = \"Table: Characteristics of Obstetrics and Gynecology Subspecialists Practicing at Obstetrics and Gynecology Residency Programs\",   pfootnote = FALSE ) arsenal_tables_write2pdf(overall, \"arsenal_overall_table_one\") } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2word.html","id":null,"dir":"Reference","previous_headings":"","what":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","title":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","text":"function exports Arsenal table object Word document ease review sharing, logging step handling errors.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2word.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","text":"","code":"arsenal_tables_write2word(object, filename)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2word.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","text":"object Arsenal table object export, typically created using arsenal::tableby. filename string representing filename (without extension) output Word document.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2word.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","text":"None. Outputs Word document specified location.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/arsenal_tables_write2word.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write Arsenal Table Object to Word Document with Error Handling and Logging — arsenal_tables_write2word","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Export a table to Word arsenal_tables_write2word(my_table, \"physician_summary\")  # Example 2: Saving with a custom filename arsenal_tables_write2word(my_table, \"custom_output\")  # Example 3: Exporting a different Arsenal table object another_table <- arsenal::tableby(~ var1 + var2, data = sample_data) arsenal_tables_write2word(another_table, \"analysis_output\") } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"function calculates percentage frequent value specified categorical variable within data frame. identifies common value, count, proportion relative total number observations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"","code":"calcpercentages(df, variable)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"df data frame containing categorical variable analyze. variable character string specifying name categorical variable df.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"data frame following columns: variable common value specified variable. n count common value. percentage percentage total count represented common value.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"function first converts specified variable character type factor. counts occurrences unique value variable, identifies frequent value, calculates percentage total. ties common value, one returned.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calcpercentages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"","code":"# Example 1: Basic usage with a simple dataset df <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"A\", \"B\", \"B\", \"A\")) result <- calcpercentages(df, \"category\") print(result) #>   \"category\" n percentage #> 1   category 8        100  # Example 2: Dataset with ties for the most common value df_tie <- data.frame(category = c(\"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"C\", \"A\", \"B\")) result <- calcpercentages(df_tie, \"category\") print(result) #>   \"category\" n percentage #> 1   category 9        100  # Example 3: Dataset with missing values df_na <- data.frame(category = c(\"A\", NA, \"A\", \"C\", \"A\", \"B\", \"B\", NA)) result <- calcpercentages(df_na, \"category\") print(result) #>   \"category\" n percentage #> 1   category 8        100"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_descriptive_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"function calculates median, 25th percentile (Q1), 75th percentile (Q3) specified column dataframe. function includes detailed logging inputs, outputs, data transformation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_descriptive_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"","code":"calculate_descriptive_stats(df, column, verbose = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_descriptive_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"df dataframe containing data. column string representing column name calculate descriptive statistics. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_descriptive_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"list containing median, 25th percentile (Q1), 75th percentile (Q3) specified column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_descriptive_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"","code":"# Example: Calculate descriptive statistics for a column with logging stats <- calculate_descriptive_stats(df, \"business_days_until_appointment\", verbose = TRUE) #> Error: The `df` argument must be a data frame."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","title":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","text":"function calculates distribution categorical variable within data frame, including counts percentages. also logs inputs, outputs, transformations transparency debugging purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","text":"","code":"calculate_distribution(df, column)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","text":"df data frame containing data. column string representing name column distribution calculated.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","text":"data frame count, total, percentage level specified column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Demographic Distribution with Robust Logging — calculate_distribution","text":"","code":"df <- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Female\", NA)) result <- calculate_distribution(df, \"gender\") #> Starting calculate_distribution... #> Input Data Frame: #>   gender #> 1   Male #> 2 Female #> 3 Female #> 4   Male #> 5   Male #> 6 Female #> Column to Calculate Distribution For: gender  #> Filtered Data Frame (NA removed): #>   gender #> 1   Male #> 2 Female #> 3 Female #> 4   Male #> 5   Male #> 6 Female #> Final Distribution Result: #> # A tibble: 1 × 4 #>   gender count total percent #>   <chr>  <int> <int>   <dbl> #> 1 Female     3     6      50 print(result) #> # A tibble: 1 × 4 #>   gender count total percent #>   <chr>  <int> <int>   <dbl> #> 1 Female     3     6      50"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_intersection_overlap_and_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"function calculates intersection block groups isochrones specific drive time saves results shapefile.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_intersection_overlap_and_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"","code":"calculate_intersection_overlap_and_save(   block_groups,   isochrones_joined,   drive_time,   output_dir )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_intersection_overlap_and_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"block_groups sf object representing block groups. isochrones_joined sf object representing isochrones. drive_time drive time value calculate intersection. output_dir directory intersection shapefile saved.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_intersection_overlap_and_save.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"None. function saves intersection shapefile provides logging.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_intersection_overlap_and_save.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"","code":"calculate_intersection_overlap_and_save(block_groups, isochrones_joined, 30L, \"data/shp/\") #> Error: object 'block_groups' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_population_in_isochrones_by_race.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Aggregated Population Counts in Isochrones by Race/Ethnicity — calculate_population_in_isochrones_by_race","title":"Calculate Aggregated Population Counts in Isochrones by Race/Ethnicity — calculate_population_in_isochrones_by_race","text":"Aggregates population counts (e.g., ACS data) race/ethnicity based spatial overlap unified isochrones specified year travel time. Outputs include detailed metadata population insights, including uncovered populations coverage percentages.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_population_in_isochrones_by_race.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Aggregated Population Counts in Isochrones by Race/Ethnicity — calculate_population_in_isochrones_by_race","text":"","code":"calculate_population_in_isochrones_by_race(   population_data,   isochrone_geometries,   year,   travel_time = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_population_in_isochrones_by_race.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Aggregated Population Counts in Isochrones by Race/Ethnicity — calculate_population_in_isochrones_by_race","text":"population_data tibble columns: id: unique identifier population area (e.g., county). population: aggregated population count area. race_ethnicity: character column indicating racial/ethnic group. geometry: geographic boundary area (sf object). year: year associated population data. must sf object. isochrone_geometries tibble columns: id: unique identifier isochrone. year: year associated isochrone. travel_time: travel time minutes isochrone. wkt: polygon geometry isochrone WKT format. year integer representing year filter data (e.g., 2013, 2014). mandatory. travel_time numeric value specifying travel time filter isochrones (e.g., 30, 60, 120, 180) NULL include travel times. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_population_in_isochrones_by_race.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Aggregated Population Counts in Isochrones by Race/Ethnicity — calculate_population_in_isochrones_by_race","text":"tibble population aggregates race/ethnicity additional metadata.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"function calculates proportion level specified categorical variable within data frame. returns data frame counts percentages level, logging process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"","code":"calculate_proportion(df, variable_name, log_file = \"calculate_proportion.log\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"df data frame containing categorical variable. variable_name name categorical variable proportions calculated, passed unquoted expression. log_file path log file logs saved.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"data frame two columns: n (count level) percent (percentage total count represented level).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"function counts occurrences unique value specified variable calculates percentage value represents total count. percentages rounded two decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Proportion Variable — calculate_proportion_variable","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"function calculates returns proportion variable, maximum percentage tabulation result.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"","code":"calculate_proportion_variable(tabyl_result)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"tabyl_result tabulation result data frame.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/calculate_proportion_variable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"proportion variable percentage.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/check_normality.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Normality and Summarize Data — check_normality","title":"Check Normality and Summarize Data — check_normality","text":"function checks normality specified variable dataframe using Shapiro-Wilk test provides summary statistics (mean standard deviation normal, median IQR normal).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/check_normality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Normality and Summarize Data — check_normality","text":"","code":"check_normality(data, variable)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/check_normality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Normality and Summarize Data — check_normality","text":"data dataframe containing data. variable string specifying column name variable checked summarized.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/check_normality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Normality and Summarize Data — check_normality","text":"list containing summary statistics (mean standard deviation normal, median IQR normal).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/check_normality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Normality and Summarize Data — check_normality","text":"","code":"# Example usage with a dataframe 'df' and outcome variable 'business_days_until_appointment' check_normality(df, \"business_days_until_appointment\") #> Error: Error: 'data' must be a data frame."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":null,"dir":"Reference","previous_headings":"","what":"City and State to Latitude and Longitude — cityStateToLatLong","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"dataset maps U.S. cities states geographic coordinates (latitude longitude). provides detailed location information approximately 31,909 cities across United States, making valuable resource geographic analyses, mapping, location-based studies.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"","code":"cityStateToLatLong"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"tibble 31,909 rows 4 variables: state name U.S. state (e.g., \"Alabama\", \"California\"). city name city within state (e.g., \"Los Angeles\", \"Denver\"). latitude latitude city decimal degrees (e.g., 34.0522). longitude longitude city decimal degrees (e.g., -118.2437).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"Derived U.S. Census Bureau's Gazetteer Files OpenStreetMap geocoding data. U.S. Census Bureau OpenStreetMap","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"Geographic data sourced authoritative mapping datasets, ensuring accurate coordinates. dataset can used : Mapping visualization city-level data. Location-based research geographic clustering. Integration geospatial tools analysis. Note: Latitude longitude provided WGS 84 coordinate system, standard global mapping.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/cityStateToLatLong.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"City and State to Latitude and Longitude — cityStateToLatLong","text":"","code":"# Load the dataset data(cityStateToLatLong)  # View the first few rows head(cityStateToLatLong) #> # A tibble: 6 × 4 #>   state   city       latitude longitude #>   <chr>   <chr>         <dbl>     <dbl> #> 1 Alabama Abanda         33.1     -85.5 #> 2 Alabama Abbeville      31.6     -85.3 #> 3 Alabama Adamsville     33.6     -87.0 #> 4 Alabama Addison        34.2     -87.2 #> 5 Alabama Akron          32.9     -87.7 #> 6 Alabama Alabaster      33.2     -86.8  # Filter for cities in Alabama subset(cityStateToLatLong, state == \"Alabama\") #> # A tibble: 593 × 4 #>    state   city           latitude longitude #>    <chr>   <chr>             <dbl>     <dbl> #>  1 Alabama Abanda             33.1     -85.5 #>  2 Alabama Abbeville          31.6     -85.3 #>  3 Alabama Adamsville         33.6     -87.0 #>  4 Alabama Addison            34.2     -87.2 #>  5 Alabama Akron              32.9     -87.7 #>  6 Alabama Alabaster          33.2     -86.8 #>  7 Alabama Albertville        34.3     -86.2 #>  8 Alabama Alexander City     32.9     -85.9 #>  9 Alabama Alexandria         33.8     -85.9 #> 10 Alabama Aliceville         33.1     -88.2 #> # ℹ 583 more rows  # Plot cities in a specific state library(ggplot2) ggplot(   subset(cityStateToLatLong, state == \"California\"),   aes(x = longitude, y = latitude) ) +   geom_point() +   labs(title = \"Cities in California\", x = \"Longitude\", y = \"Latitude\") +   theme_minimal()   # Find the coordinates of a specific city subset(cityStateToLatLong, city == \"Denver\" & state == \"Colorado\") #> # A tibble: 1 × 4 #>   state    city   latitude longitude #>   <chr>    <chr>     <dbl>     <dbl> #> 1 Colorado Denver     39.8     -105."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"city_state_assign_scenarios function designed assign cases professionals based specialty location (city state). function particularly useful managing scenarios professionals, physicians healthcare workers, need assigned cases administrative analytical purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"","code":"city_state_assign_scenarios(   data,   generalist = \"General Dermatology\",   specialty = \"Pediatric Dermatology\",   case_names = c(\"Case Alpha\", \"Case Beta\", \"Case Gamma\"),   output_csv_path = \"Lizzy/data/city_state_assign_scenarios.csv\",   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. generalist character string specifying specialty name generalists. Default \"Generalist\". specialty character string specifying specialty name specialists. Default \"Specialist\". case_names character vector case names assign. Default c(\"Alpha\", \"Beta\", \"Gamma\"). output_csv_path character string specifying file path save output CSV. Default \"output/city_state_assign_scenarios.csv\". seed optional integer value set random seed reproducibility. Default NULL (seed set).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"data frame assigned cases.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"Generalists vs. Specialists: function differentiates generalists specialists, assigning cases accordingly. CSV Output: final output, including case assignments professional, saved CSV file using write_output_csv function.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"use-cases-","dir":"Reference","previous_headings":"","what":"Use Cases:","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"Healthcare Assignment: Assigning different types cases healthcare professionals based specialties cities/states practice. Research Studies: Managing scenarios research studies professionals need randomly assigned cases.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_assign_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"","code":"# Example 1: Using default parameters data <- data.frame(   city = c(\"CityA\", \"CityA\", \"CityB\", \"CityB\"),   state_code = c(\"State1\", \"State1\", \"State2\", \"State2\"),   specialty_primary = c(\"Generalist\", \"Specialist\", \"Generalist\", \"Specialist\"),   stringsAsFactors = FALSE ) result <- city_state_assign_scenarios(data) #> File successfully saved to: Lizzy/data/city_state_assign_scenarios.csv  #> [1] \"Please check the column called `case_assigned` for the assigned scenario. Scenarios will be spread out across generalists but specialists will get every scenario.\" print(result) #> # A tibble: 4 × 4 #>   city  state_code specialty_primary case_assigned #>   <chr> <chr>      <chr>             <chr>         #> 1 CityA State1     Generalist        NA            #> 2 CityA State1     Specialist        NA            #> 3 CityB State2     Generalist        NA            #> 4 CityB State2     Specialist        NA"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_check_specialty_generalist_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"function checks city-state combination required number generalists specialists. logs inputs, transformations, outputs, returns two data frames: one failing city-state-specialty combinations one successful combinations. Optionally, results can saved CSV files.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_check_specialty_generalist_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"","code":"city_state_check_specialty_generalist_counts(   data,   min_generalists,   min_specialists,   generalist_name = \"General Dermatology\",   specialist_name = \"Pediatric Dermatology\",   failing_csv_path = NULL,   successful_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_check_specialty_generalist_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. min_generalists integer specifying minimum number generalists required per city-state combination. min_specialists integer specifying minimum number specialists required per city-state combination. generalist_name string specifying specialty name generalists. Default \"General Dermatology\". specialist_name string specifying specialty name specialists. Default \"Pediatric Dermatology\". failing_csv_path optional string specifying file path save failing combinations CSV. Default NULL (file saved). successful_csv_path optional string specifying file path save successful combinations CSV. Default NULL (file saved).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_check_specialty_generalist_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"list containing two data frames: failing_combinations successful_combinations.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_check_specialty_generalist_counts.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"Generalists vs. Specialists: can specify names generalists specialists, function checks city-state combination required number . Logging: Extensive logging ensures inputs, transformations, results tracked. CSV Output: Optionally, function writes failing successful city-state-specialty combinations separate CSV files. Summary Logging: summary min_generalists, min_specialists, results logged end.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_sample_specialists.html","id":null,"dir":"Reference","previous_headings":"","what":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"function samples specialists generalists given dataset based city-state combinations. allows sampling three types specialists generalists customizable sample sizes . results can saved CSV file returned dataframe.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_sample_specialists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"","code":"city_state_sample_specialists(   data,   generalist = \"General Dermatology\",   specialist1 = \"Pediatric Dermatology\",   general_sample_size = 4,   specialist1_sample_size = 2,   specialist2 = NULL,   specialist2_sample_size = 0,   specialist3 = NULL,   specialist3_sample_size = 0,   same_phone_number = TRUE,   output_csv_path = NULL,   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_sample_specialists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"data dataframe containing specialist information. must columns: city, state_code, specialty_primary, phone_number. generalist character string specifying generalist specialty sample. Default \"General Dermatology\". specialist1 character string specifying first specialist specialty sample. Default \"Pediatric Dermatology\". general_sample_size integer specifying many generalists sample city-state combination. Default 4. specialist1_sample_size integer specifying many first specialists sample city-state combination. Default 1. specialist2 character string specifying second specialist specialty sample. Optional. Default NULL. specialist2_sample_size integer specifying many second specialists sample. Default 0. specialist3 character string specifying third specialist specialty sample. Optional. Default NULL. specialist3_sample_size integer specifying many third specialists sample. Default 0. same_phone_number logical value indicating whether sample generalists specialists phone number (TRUE) different phone numbers (FALSE). Default TRUE. output_csv_path character string specifying path save output CSV. provided, result returned. seed integer setting seed reproducibility. Default 1978.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_sample_specialists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"dataframe containing sampled generalists specialists city-state combination.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/city_state_sample_specialists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"","code":"# Example 1: Basic usage with default generalist and specialist data <- data.frame(   city = rep(c(\"New York\", \"Los Angeles\"), each = 6),   state_code = rep(c(\"NY\", \"CA\"), each = 6),   specialty_primary = c(     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\"   ),   phone_number = rep(c(\"123\", \"456\", \"789\"), 4) ) result <- city_state_sample_specialists(data) print(result) #> # A tibble: 0 × 4 #> # Groups:   phone_number [0] #> # ℹ 4 variables: city <chr>, state_code <chr>, specialty_primary <chr>, #> #   phone_number <chr>"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_column_unwrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility to unwrap a list column to its first value — clean_column_unwrap","title":"Utility to unwrap a list column to its first value — clean_column_unwrap","text":"Utility unwrap list column first value","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_column_unwrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility to unwrap a list column to its first value — clean_column_unwrap","text":"","code":"clean_column_unwrap(x)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_column_unwrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility to unwrap a list column to its first value — clean_column_unwrap","text":"x list vector unwrap","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_column_unwrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility to unwrap a list column to its first value — clean_column_unwrap","text":"first element character string, NA list empty","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_npi_entries.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean NPI Entries Function — clean_npi_entries","title":"Clean NPI Entries Function — clean_npi_entries","text":"function cleans NPI search results normalizing credentials, applying filters taxonomies, summarizing entries NPI. includes console logging key steps.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_npi_entries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean NPI Entries Function — clean_npi_entries","text":"","code":"clean_npi_entries(   npi_entries,   basic_credentials = c(\"MD\", \"DO\"),   taxonomy_filter = \"Obstetrics & Gynecology\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_npi_entries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean NPI Entries Function — clean_npi_entries","text":"npi_entries dataframe containing NPI search results. basic_credentials character vector credentials filter (default c(\"MD\", \"\")). taxonomy_filter string filtering taxonomies (default \"Obstetrics & Gynecology\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_npi_entries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean NPI Entries Function — clean_npi_entries","text":"cleaned dataframe summarized NPI entries.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_npi_entries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean NPI Entries Function — clean_npi_entries","text":"","code":"# Example 1: Basic cleaning of NPI entries with default parameters clean_npi_entries(npi_results) #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat'  # Example 2: Cleaning NPI entries, filtering for a specific taxonomy clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\") #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat'  # Example 3: Cleaning NPI entries, specifying different credentials clean_npi_entries(npi_results, basic_credentials = c(\"PA\", \"NP\")) #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat'"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_phase_2_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and Process Phase 2 Data — clean_phase_2_data","title":"Clean and Process Phase 2 Data — clean_phase_2_data","text":"function reads data file data frame, cleans column names, applies renaming based specified criteria facilitate data analysis. function logs step process, including data loading, column cleaning, renaming transparency.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_phase_2_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and Process Phase 2 Data — clean_phase_2_data","text":"","code":"clean_phase_2_data(   data_or_path,   required_strings,   standard_names,   output_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_phase_2_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and Process Phase 2 Data — clean_phase_2_data","text":"data_or_path Path data file data frame. required_strings Vector substrings search column names. standard_names Vector new names apply matched columns. output_csv_path Optional. provided, cleaned data saved path.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_phase_2_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and Process Phase 2 Data — clean_phase_2_data","text":"data frame processed data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/clean_phase_2_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and Process Phase 2 Data — clean_phase_2_data","text":"","code":"# Example 1: Cleaning data from a CSV file input_path <- \"path_to_your_data.csv\" required_strings <- c(\"physician_information\", \"able_to_contact_office\") standard_names <- c(\"physician_info\", \"contact_office\") cleaned_data <- clean_phase_2_data(input_path, required_strings, standard_names) #> Error: Error: If 'data_or_path' is a string, it must be a valid, readable file path.  # Example 2: Directly using a data frame df <- data.frame(DocInfo = 1:5, ContactData = 6:10) required_strings <- c(\"doc_info\", \"contact_data\") standard_names <- c(\"doctor_info\", \"patient_contact_info\") cleaned_df <- clean_phase_2_data(df, required_strings, standard_names) #> --- Starting data cleaning process --- #> Input data or path:   #> Error in cat(\"Input data or path: \", data_or_path, \"\\n\"): argument 2 (type 'list') cannot be handled by 'cat' print(cleaned_df) # Should show updated column names #> Error: object 'cleaned_df' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/convert_list_to_df_expanded.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"helper function converts named list column names, grouped table, expanded data frame column name placed separate column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/convert_list_to_df_expanded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"","code":"convert_list_to_df_expanded(column_list)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/convert_list_to_df_expanded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"column_list named list element contains column names table.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/convert_list_to_df_expanded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"data frame table name corresponding columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/convert_list_to_df_expanded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"","code":"# Example 1: Convert a list of column names to an expanded data frame test_list <- list(table1 = c(\"col1\", \"col2\"), table2 = c(\"col1\", \"col2\", \"col3\")) expanded_df <- convert_list_to_df_expanded(test_list) #> Error: 'is.named' is not an exported object from 'namespace:assertthat' print(expanded_df) #> Error: object 'expanded_df' not found  # Example 2: Handling missing columns in some tables test_list <- list(table1 = c(\"col1\", \"col2\"), table2 = c(\"col1\")) expanded_df <- convert_list_to_df_expanded(test_list) #> Error: 'is.named' is not an exported object from 'namespace:assertthat' print(expanded_df) #> Error: object 'expanded_df' not found  # Example 3: Convert an empty list empty_list <- list() expanded_df_empty <- convert_list_to_df_expanded(empty_list) #> Error: 'is.named' is not an exported object from 'namespace:assertthat' print(expanded_df_empty) # Should return an empty data frame #> Error: object 'expanded_df_empty' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_physicians_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","title":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","text":"function counts physicians grouped either state U.S. Census subdivision, using us_census_bureau_regions_df dataset. logs inputs, transformations, outputs. results can optionally saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_physicians_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","text":"","code":"count_physicians_by_group(   data,   state_name_column = \"state_code\",   phone_column = \"phone_number\",   first_name_column = \"first\",   last_name_column = \"last\",   group_by = \"state\",   output_to_csv = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_physicians_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","text":"data dataframe containing physician data, including state, phone, name columns. state_name_column string specifying column name state information dataset. Default \"state_code\". phone_column string specifying column name phone numbers dataset. Default \"phone_number\". first_name_column string specifying column name physicians' first names. Default \"first\". last_name_column string specifying column name physicians' last names. Default \"last\". group_by string specifying group counts. Options \"state\" state-level counts \"subdivision\" U.S. Census subdivision counts. Default \"state\". output_to_csv string specifying file path save counts CSV file. NULL, file saved. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_physicians_by_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","text":"dataframe counts physicians grouped state subdivision.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_physicians_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count Physicians by State or Subdivision with Logging — count_physicians_by_group","text":"","code":"# Example 1: Count physicians by state state_counts <- count_physicians_by_group(   data = physicians_data,   state_name_column = \"state_code\",   phone_column = \"phone\",   first_name_column = \"first_name\",   last_name_column = \"last_name\",   group_by = \"state\" ) #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'physicians_data' not found  # Example 2: Count physicians by U.S. Census subdivision subdivision_counts <- count_physicians_by_group(   data = physicians_data,   state_name_column = \"state_code\",   phone_column = \"phone\",   first_name_column = \"first_name\",   last_name_column = \"last_name\",   group_by = \"subdivision\" ) #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'physicians_data' not found  # Example 3: Save counts to a CSV file count_physicians_by_group(   data = physicians_data,   group_by = \"state\",   output_to_csv = \"state_counts.csv\" ) #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'physicians_data' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_unique_physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"function filters dataframe physician data based insurance type, reason exclusion, appointment availability, counts number unique physicians meet criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_unique_physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"","code":"count_unique_physicians(   df,   insurance_type,   reason_for_exclusion = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_unique_physicians.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"df dataframe containing physician data. Must include columns 'insurance', 'reason_for_exclusions', 'business_days_until_appointment', 'phone'. insurance_type string specifying insurance type filter (e.g., \"Medicaid\"). reason_for_exclusion string specifying reason exclusion filter . Default NULL, includes rows regardless exclusion reason. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_unique_physicians.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"integer representing number unique physicians meet specified criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/count_unique_physicians.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"","code":"# Example 1: Counting unique physicians with specific insurance and reason for exclusion df <- data.frame(   insurance = c(\"Medicaid\", \"Medicaid\", \"Blue Cross/Blue Shield\", \"Medicaid\"),   reason_for_exclusions = c(     \"Able to contact\", \"Not available\", \"Able to contact\", \"Able to contact\"   ),   business_days_until_appointment = c(5, 0, 10, 3),   phone = c(\"123-456-7890\", \"123-456-7890\", \"098-765-4321\", \"234-567-8901\") ) unique_count <- count_unique_physicians(   df,   insurance_type = \"Medicaid\",   reason_for_exclusion = \"Able to contact\" ) #> Starting count_unique_physicians function... #> Insurance Type: Medicaid #> Reason for Exclusion: Able to contact #> Filtered rows by insurance type: 3 remaining. #> Filtered rows by reason for exclusion: 2 remaining. #> Filtered rows with positive business days until appointment: 2 remaining. #> Number of unique physicians: 2 #> Function count_unique_physicians completed successfully. print(unique_count) # Expected output: 1 #> [1] 2  # Example 2: Counting unique physicians without specifying a reason for exclusion df2 <- data.frame(   insurance = c(\"Blue Cross/Blue Shield\", \"Blue Cross/Blue Shield\", \"Medicaid\"),   reason_for_exclusions = c(\"Able to contact\", \"Not available\", \"Able to contact\"),   business_days_until_appointment = c(3, 5, 1),   phone = c(\"321-654-0987\", \"321-654-0987\", \"654-321-0987\") ) unique_count2 <- count_unique_physicians(   df2,   insurance_type = \"Blue Cross/Blue Shield\" ) #> Starting count_unique_physicians function... #> Insurance Type: Blue Cross/Blue Shield #> No specific reason for exclusion is provided. #> Filtered rows by insurance type: 2 remaining. #> Filtered rows with positive business days until appointment: 2 remaining. #> Number of unique physicians: 1 #> Function count_unique_physicians completed successfully. print(unique_count2) # Expected output: 1 #> [1] 1  # Example 3: Using verbose logging to see detailed steps df3 <- data.frame(   insurance = c(\"Medicaid\", \"Medicaid\", \"Medicaid\", \"Medicaid\"),   reason_for_exclusions = c(     \"Able to contact\", \"Able to contact\", \"Not available\", \"Able to contact\"   ),   business_days_until_appointment = c(2, 1, 0, 4),   phone = c(\"111-222-3333\", \"111-222-3333\", \"222-333-4444\", \"333-444-5555\") ) unique_count3 <- count_unique_physicians(   df3,   insurance_type = \"Medicaid\",   verbose = TRUE ) #> Starting count_unique_physicians function... #> Insurance Type: Medicaid #> No specific reason for exclusion is provided. #> Filtered rows by insurance type: 4 remaining. #> Filtered rows with positive business days until appointment: 3 remaining. #> Number of unique physicians: 2 #> Function count_unique_physicians completed successfully. print(unique_count3) # Expected output: 2 #> [1] 2"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_plot_interaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"function reads data specified file, fits generalized linear mixed model (GLMM) specified interaction term, creates plot visualize interaction effects. plot saved specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_plot_interaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"","code":"create_and_plot_interaction(   data_path,   response_variable,   variable_of_interest,   interaction_variable,   random_intercept,   output_path,   resolution = 100 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_plot_interaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"data_path character string specifying path .rds file containing dataset. response_variable character string specifying name response variable dataset. variable_of_interest character string specifying first categorical predictor variable interaction. interaction_variable character string specifying second categorical predictor variable interaction. random_intercept character string specifying variable used random intercept model (e.g., \"city\"). output_path character string specifying directory interaction plot saved. resolution integer specifying resolution (DPI) saving plot. Defaults 100.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_plot_interaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"list containing fitted GLMM (model) summarized data used effects plot (effects_plot_data).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_plot_interaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"","code":"if (FALSE) { # \\dontrun{ result <- create_and_plot_interaction(   data_path = \"data/phase2_analysis.rds\",   response_variable = \"business_days_until_appointment\",   variable_of_interest = \"appointment_center\",   interaction_variable = \"gender\",   random_intercept = \"city\",   output_path = \"results/figures\",   resolution = 100 ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_save_physician_dot_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"function creates Leaflet dot map physicians using longitude latitude coordinates. also adds ACOG district boundaries map saves HTML file accompanying PNG screenshot.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_save_physician_dot_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"","code":"create_and_save_physician_dot_map(   physician_data,   jitter_range = 0.05,   color_palette = \"magma\",   popup_var = \"name\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_save_physician_dot_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"physician_data sf object containing physician data \"long\" \"lat\" columns. jitter_range range adding jitter latitude longitude coordinates. color_palette color palette ACOG district colors. popup_var variable use popup text.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_save_physician_dot_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_and_save_physician_dot_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"","code":"if (FALSE) { # \\dontrun{ # Load required libraries library(viridis) library(leaflet)  # Generate physician data (replace with your own data) physician_data <- data.frame(   long = c(-95.363271, -97.743061, -98.493628, -96.900115, -95.369803),   lat = c(29.763283, 30.267153, 29.424349, 32.779167, 29.751808),   name = c(\"Physician 1\", \"Physician 2\", \"Physician 3\", \"Physician 4\", \"Physician 5\"),   ACOG_District = c(\"District I\", \"District II\", \"District III\", \"District IV\", \"District V\") )  # Create and save the dot map create_and_save_physician_dot_map(physician_data) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_bar_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"function generates bar plot based categorical variable facets plot grouping variable. plot title automatically includes total sample size (N). function also supports custom axis labels, returns plot object manipulation saving.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_bar_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"","code":"create_bar_plot(   input_data,   category_var,   grouping_var,   title = NULL,   x_axis_label = NULL,   y_axis_label = \"Count\",   output_directory = \"output\",   filename_prefix = \"bar_plot\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_bar_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"input_data dataframe containing data plotted. category_var string representing column name categorical variable x-axis (e.g., insurance type). grouping_var string representing column name facet wrap (grouping variable). title string specifying title plot. Default NULL, function generate title based category_var grouping_var. x_axis_label string specifying label x-axis. Default NULL, function use column name x-axis variable. y_axis_label string specifying label y-axis. Default \"Count\". output_directory string representing directory plot files saved. Default \"output\". filename_prefix string used prefix generated plot filenames. Default \"bar_plot\". verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_bar_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"function returns plot object manipulation saving.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_bar_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"","code":"# Example 1: Basic usage with a categorical and facet variable create_bar_plot(   input_data = my_data,   category_var = \"insurance_type\",   grouping_var = \"region\",   title = \"Insurance Type Distribution by Region\",   x_axis_label = \"Insurance Type\",   y_axis_label = \"Number of Observations\" ) #> Error: object 'my_data' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_base_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Base Leaflet Map — create_base_map","title":"Create a Base Leaflet Map — create_base_map","text":"function creates base Leaflet map custom title.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_base_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Base Leaflet Map — create_base_map","text":"","code":"create_base_map(title)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_base_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Base Leaflet Map — create_base_map","text":"title character string containing HTML title map.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_base_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Base Leaflet Map — create_base_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_base_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Base Leaflet Map — create_base_map","text":"","code":"if (FALSE) { # \\dontrun{ # Create a base map with a custom title my_map <- create_base_map(\"<h1>Custom Map Title<\/h1>\")  # Display the map and add circle markers my_map <- my_map %>%   leaflet::addCircleMarkers(lng = ~longitude,                            lat = ~latitude,                            data = data_points,                            popup = ~popup_text,                            radius = ~radius,                            color = ~color,                            fill = TRUE,                            stroke = FALSE,                            fillOpacity = 0.8) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_block_group_overlap_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"function creates map displays block groups overlap isochrones.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_block_group_overlap_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"","code":"create_block_group_overlap_map(   bg_data,   isochrones_data,   output_dir = \"figures/\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_block_group_overlap_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"bg_data SpatialPolygonsDataFrame representing block group data. isochrones_data SpatialPolygonsDataFrame representing isochrone data. output_dir Directory path exporting map files. Default \"figures/\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_block_group_overlap_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_block_group_overlap_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"","code":"if (FALSE) { # \\dontrun{ # Create and export the map with the default output directory create_block_group_overlap_map(block_groups, isochrones_joined_map)  # Create and export the map with a custom output directory create_block_group_overlap_map(block_groups, isochrones_joined_map, \"custom_output/\") } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_density_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"function generates density plot designed mystery caller studies, allowing visualization waiting times similar outcomes across different categories, insurance types. function supports transformations x-axis custom labels.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_density_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"","code":"create_density_plot(   data,   x_var,   fill_var,   x_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"density_plot\",   x_label = NULL,   y_label = \"Density\",   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_density_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"data dataframe containing data plotted. Must contain variables specified x_var fill_var. x_var string representing column name x-axis variable. numeric variable (e.g., waiting time days). fill_var string representing column name fill variable. categorical factor variable (e.g., insurance type). x_transform string specifying transformation x-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"density_plot\". x_label string specifying label x-axis. Default NULL (uses x_var). y_label string specifying label y-axis. Default \"Density\". plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_density_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_density_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"","code":"# Example 1: Basic density plot with log transformation create_density_plot(   data = df3,   x_var = \"business_days_until_appointment\",   fill_var = \"insurance\",   x_transform = \"log\",   dpi = 100,   output_dir = \"figures\",   file_prefix = \"waiting_time_density\",   x_label = \"Log (Waiting Times in Days)\",   y_label = \"Density\",   plot_title = \"Density Plot of Waiting Times by Insurance\" ) #> Error: object 'df3' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_dot_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"function generates dot plot visualizing median values error bars across different categories, insurance types. includes error handling, meaningful variable names, default behaviors ease use. Extensive logging tracks inputs, transformations, outputs.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_dot_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"","code":"create_dot_plot(   dataset,   category_var,   value_var = \"median_days\",   lower_bound_var = \"q1\",   upper_bound_var = \"q3\",   dpi = 100,   output_directory = \"output\",   filename_prefix = \"dot_plot\",   x_label = NULL,   y_label = NULL,   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_dot_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"dataset dataframe containing data plotted. Must contain variables specified category_var, value_var, lower_bound_var, upper_bound_var. category_var string representing column name categorical variable x-axis (e.g., insurance type). value_var string representing column name numeric variable y-axis (e.g., median days). lower_bound_var string representing column name lower bound error bars (e.g., first quartile). upper_bound_var string representing column name upper bound error bars (e.g., third quartile). dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_directory string representing directory plot files saved. Default \"output\". filename_prefix string used prefix generated plot filenames. filenames timestamp appended uniqueness. x_label string specifying label x-axis. Default NULL (uses category_var). y_label string specifying label y-axis. Default NULL (uses value_var). plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_dot_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_dot_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"","code":"create_dot_plot(   dataset = df_plot,   category_var = \"insurance\",   value_var = \"median_days\",   lower_bound_var = \"q1\",   upper_bound_var = \"q3\",   dpi = 100,   output_directory = \"output/plots\",   filename_prefix = \"insurance_vs_days\",   x_label = \"Insurance\",   y_label = \"Median Business Days\",   plot_title = \"Comparison of Business Days by Insurance\" ) #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat'"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"function generates forest plot displaying significant predictors' coefficients confidence intervals Poisson regression model. logs step process, providing insights predictors included, coefficients, confidence intervals. forest plot highlights direction significance effects analyzed variables.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"","code":"create_forest_plot(df, target_variable, significant_vars)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"df data frame containing dataset used regression analysis. target_variable string representing target variable (dependent variable) regression model. significant_vars data frame containing significant predictors variable names (Variable), coefficients (Coefficient), confidence intervals (Lower_CI, Upper_CI).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"ggplot object representing forest plot coefficients confidence intervals significant predictor.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"function loops significant predictors fit individual Poisson regression models predictor target variable. extracts coefficients confidence intervals, compiles results, visualizes forest plot. vertical dashed red line x = 0 indicates effect.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_forest_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"","code":"# Example: Creating a forest plot for significant predictors forest_plot <- create_forest_plot(   df = data,   target_variable = \"accepts_medicaid\",   significant_vars = significant_predictors ) #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat' print(forest_plot) #> Error: object 'forest_plot' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_formula.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Dynamic Formula for Mixed Effects Models — create_formula","title":"Create a Dynamic Formula for Mixed Effects Models — create_formula","text":"Dynamically generates formula mixed-effects models excluding specified columns quoting variables special characters. Handles fixed-effects mixed-effects models making grouping variable optional. function creates formula Poisson model based provided data, response variable, optional random effect.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_formula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Dynamic Formula for Mixed Effects Models — create_formula","text":"","code":"create_formula(data, response_var, random_effect = NULL)  create_formula(data, response_var, random_effect = NULL)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_formula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Dynamic Formula for Mixed Effects Models — create_formula","text":"data dataframe containing predictor response variables. response_var name response variable dataframe. random_effect Optional. name random effect variable formula. dataset dataframe containing variables formula. outcome_var character string specifying outcome variable. Defaults \"business_days_until_appointment\". group_var character string specifying grouping variable. NULL, creates fixed-effects formula. Defaults NULL. exclude_columns character vector column names exclude formula. Defaults pre-defined list.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_formula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Dynamic Formula for Mixed Effects Models — create_formula","text":"formula object representing specified model. formula object suitable modeling R.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_formula.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Dynamic Formula for Mixed Effects Models — create_formula","text":"","code":"# Example 1: Default behavior with pre-defined exclude_columns dataset <- data.frame(   business_days_until_appointment = rpois(100, lambda = 5),   NPI = sample(letters, 100, replace = TRUE),   age = rnorm(100, mean = 50, sd = 10),   gender = sample(c(\"Male\", \"Female\"), 100, replace = TRUE) ) create_formula(dataset) #> Error in create_formula(dataset): argument \"response_var\" is missing, with no default  # Example 2: Custom outcome and grouping variables dataset <- data.frame(   wait_time = rpois(100, lambda = 7),   provider_id = sample(letters, 100, replace = TRUE),   income = rnorm(100, mean = 60000, sd = 15000),   region = sample(c(\"Urban\", \"Rural\"), 100, replace = TRUE) ) create_formula(dataset, outcome_var = \"wait_time\", group_var = \"provider_id\") #> Error in create_formula(dataset, outcome_var = \"wait_time\", group_var = \"provider_id\"): unused arguments (outcome_var = \"wait_time\", group_var = \"provider_id\")  # Example 3: Specifying additional columns to exclude dataset <- data.frame(   business_days_until_appointment = rpois(100, lambda = 3),   NPI = sample(letters, 100, replace = TRUE),   specialty = sample(c(\"Cardiology\", \"Dermatology\"), 100, replace = TRUE),   phone_number = sample(1000000000:1999999999, 100) ) create_formula(dataset, exclude_columns = c(\"phone_number\", \"specialty\")) #> Error in create_formula(dataset, exclude_columns = c(\"phone_number\", \"specialty\")): unused argument (exclude_columns = c(\"phone_number\", \"specialty\")) # Example usage: response_variable <- \"days\" random_effect_term <- \"name\" # Change this to the desired random effect variable df3_filtered <- data.frame(days = c(5, 10, 15), age = c(30, 40, 50), name = c(\"A\", \"B\", \"C\")) formula <- create_formula(df3_filtered, response_variable, random_effect_term) #> Creating formula with response variable: days  #> Predictor variables identified: age, name  #> Predictor variables after formatting: `age`, `name`  #> Initial formula string: days ~ `age` + `name`  #> Formula string with random effect: days ~ `age` + `name` + (1 | name )  #> Final formula object created: #> days ~ age + name + (1 | name) #> <environment: 0x7f9376595590> formula #> days ~ age + name + (1 | name) #> <environment: 0x7f9376595590>"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_geocode.html","id":null,"dir":"Reference","previous_headings":"","what":"Geocode Unique Addresses — create_geocode","title":"Geocode Unique Addresses — create_geocode","text":"function geocodes unique addresses using Google Maps API appends latitude longitude original dataset. Please ensure every data set must column named 'address'.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_geocode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geocode Unique Addresses — create_geocode","text":"","code":"create_geocode(csv_file)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_geocode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geocode Unique Addresses — create_geocode","text":"file_path Path input file (CSV, RDS, XLSX) containing address data. google_maps_api_key Google Maps API key. output_file_path Path output CSV file geocoded data saved. (Optional) location sf object representing location isolines calculated. range numeric vector time ranges seconds. posix_time POSIXct object representing date time calculation. Default \"2023-10-20 08:00:00\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_geocode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geocode Unique Addresses — create_geocode","text":"dataframe containing geocoded address data latitude longitude. list isolines different time ranges, error message calculation fails.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_geocode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geocode Unique Addresses — create_geocode","text":"","code":"if (FALSE) { # \\dontrun{ # Define the input file path, Google Maps API key, and output file path (optional) file_path <- \"input_data.csv\" google_maps_api_key <- \"your_api_key\" output_file_path <- \"output_data.csv\"  # Optional  # Call the geocode_unique_addresses function with or without specifying output_file_path geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key) # or geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key, output_file_path) } # }  if (FALSE) { # \\dontrun{  # Set your HERE API key in your Renviron file using the following steps: # 1. Add key to .Renviron Sys.setenv(HERE_API_KEY = \"your_api_key_here\") # 2. Reload .Renviron readRenviron(\"~/.Renviron\")  # Define a sf object for the location location <- sf::st_point(c(-73.987, 40.757))  # Calculate isolines for the location with a 30-minute, 60-minute, 120-minute, and 180-minute range isolines <- create_isochrones(location = location, range = c(1800, 3600, 7200, 10800))  # Print the isolines print(isolines)  } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_histogram_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Histogram Plot with Logging and Validation — create_histogram_plot","title":"Create a Histogram Plot with Logging and Validation — create_histogram_plot","text":"function generates histogram plot optional faceting includes total sample size plot title. logs inputs, outputs, data transformations, file paths console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_histogram_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Histogram Plot with Logging and Validation — create_histogram_plot","text":"","code":"create_histogram_plot(   df,   x_var,   facet_var,   binwidth = 1,   title = \"\",   x_label = \"\",   y_label = \"Count\",   output_file = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_histogram_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Histogram Plot with Logging and Validation — create_histogram_plot","text":"df data frame containing data plotted. data include columns specified x_var facet_var. x_var string specifying name variable plot x-axis. facet_var string specifying name variable faceting. unique value variable creates separate facet. binwidth numeric value specifying width histogram bins. Default 1. title string specifying title plot. total sample size appended automatically title. x_label string specifying label x-axis. Default empty string. y_label string specifying label y-axis. Default \"Count\". output_file Optional. string specifying file path save plot image. NULL, plot saved. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_histogram_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Histogram Plot with Logging and Validation — create_histogram_plot","text":"ggplot object representing histogram. plot also saved specified output_file provided.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_individual_isochrone_plots.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"function creates individual Leaflet maps shapefiles specified drive times based isochrone data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_individual_isochrone_plots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"","code":"create_individual_isochrone_plots(isochrones, drive_times)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_individual_isochrone_plots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"isochrones sf object containing isochrone data. drive_times vector unique drive times (minutes) maps shapefiles created.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_individual_isochrone_plots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"None. function creates saves individual maps shapefiles.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_individual_isochrone_plots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"","code":"if (FALSE) { # \\dontrun{ # Load required libraries library(sf) library(leaflet) library(tyler)  # Load isochrone data isochrones <- readRDS(\"path_to_isochrones.rds\")  # List of unique drive times for which you want to create plots and shapefiles drive_times <- unique(isochrones$drive_time)  # Create individual isochrone maps and shapefiles create_individual_isochrone_plots(isochrones, drive_times) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_insurance_by_insurance_scatter_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"function creates scatter plot comparing waiting times (days) appointment two different insurance types. plot saved TIFF PNG file specified output directory. function allows customization plot aesthetics, including axis labels, point size, alpha transparency. includes options adding linear fit confidence intervals logs process console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_insurance_by_insurance_scatter_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"","code":"create_insurance_by_insurance_scatter_plot(   df,   unique_variable,   insurance1 = \"medicaid\",   insurance2 = \"blue cross/blue shield\",   output_directory = \"output\",   dpi = 100,   height = 8,   width = 11,   x_label = \"Time in days to appointment\\nBlue Cross Blue Shield (Log Scale)\",   y_label = \"Time in days to appointment\\nMedicaid (Log Scale)\",   plot_title = \"Comparison of Waiting Times: Medicaid vs Blue Cross Blue Shield\",   point_size = 3,   point_alpha = 0.6,   add_confidence_interval = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_insurance_by_insurance_scatter_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"df data frame containing data plotted. Must include insurance, business_days_until_appointment, variable specified unique_variable. unique_variable string representing column name uniquely identifies entity data (e.g., \"phone\" \"npi\"). insurance1 string representing first insurance type compared. Default \"medicaid\". insurance2 string representing second insurance type compared. Default \"blue cross/blue shield\". output_directory string specifying directory plot files saved. Default \"output\". dpi integer specifying resolution saved plot files dots per inch (DPI). Default 100. height numeric value specifying height saved plot files inches. Default 8 inches. width numeric value specifying width saved plot files inches. Default 11 inches. x_label string representing label x-axis. Default \"Time days appointment Cross Blue Shield (Log Scale)\". y_label string representing label y-axis. Default \"Time days appointment (Log Scale)\". plot_title string representing title plot. Default \"Comparison Waiting Times: Medicaid vs Blue Cross Blue Shield\". point_size numeric value specifying size points scatter plot. Default 3. point_alpha numeric value 0 1 specifying transparency level points scatter plot. Default 0.6. add_confidence_interval logical value indicating whether add confidence interval around linear fit line. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_insurance_by_insurance_scatter_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"ggplot2 scatter plot object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_insurance_by_insurance_scatter_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"","code":"# Example 1: Default settings scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3, # Input data frame   unique_variable = \"phone\" # Unique identifier variable ) #> Error: object 'df3' not found print(scatterplot) #> Error: object 'scatterplot' not found  # Example 2: Customized axis labels and output directory scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3, # Input data frame   unique_variable = \"npi\", # Unique identifier variable   insurance1 = \"medicaid\", # First insurance type   insurance2 = \"blue cross/blue shield\", # Second insurance type   x_label = \"Log Time to Appointment (BCBS)\", # Custom x-axis label   y_label = \"Log Time to Appointment (Medicaid)\", # Custom y-axis label   plot_title = \"Custom Waiting Times Comparison\", # Custom plot title   output_directory = \"custom_figures\" # Custom output directory ) #> Error: object 'df3' not found print(scatterplot) #> Error: object 'scatterplot' not found  # Example 3: High-resolution plot with adjusted aesthetics scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3, # Input data frame   unique_variable = \"phone\", # Unique identifier variable   dpi = 300, # High resolution   point_size = 4, # Larger point size   point_alpha = 0.8, # Less transparency   height = 10, # Custom height   width = 15 # Custom width ) #> Error: object 'df3' not found print(scatterplot) #> Error: object 'scatterplot' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Memoized function to try a location with isoline calculations — create_isochrones","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"function calculates isolines given location using hereR package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"","code":"create_isochrones(   location,   range,   posix_time = as.POSIXct(\"2023-10-20 08:00:00\", format = \"%Y-%m-%d %H:%M:%S\") )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"location sf object representing location isolines calculated. range numeric vector time ranges seconds. posix_time POSIXct object representing date time calculation. Default \"2023-10-20 08:00:00\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"list isolines different time ranges, error message calculation fails.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"","code":"if (FALSE) { # \\dontrun{ # Set your HERE API key in your Renviron file using the following steps: # 1. Add key to .Renviron Sys.setenv(HERE_API_KEY = \"your_api_key_here\") # 2. Reload .Renviron readRenviron(\"~/.Renviron\")  # Define a sf object for the location location <- sf::st_point(c(-73.987, 40.757))  # Calculate isolines for the location with a 30-minute, 60-minute, 120-minute, and 180-minute range isolines <- create_isochrones(location = location, range = c(1800, 3600, 7200, 10800))  # Print the isolines print(isolines)  } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones_for_dataframe.html","id":null,"dir":"Reference","previous_headings":"","what":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"function retrieves isochrones point given dataframe looping rows calling create_isochrones function point.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones_for_dataframe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"","code":"create_isochrones_for_dataframe(   input_file,   breaks = c(1800, 3600, 7200, 10800) )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones_for_dataframe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"input_file path input file containing points isochrones retrieved. breaks numeric vector specifying breaks categorizing drive times (default c(1800, 3600, 7200, 10800)).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_isochrones_for_dataframe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"dataframe containing isochrones data added 'name' column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_line_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"function creates line plot using ggplot2 options transforming y-axis, grouping lines, saving plot specified resolution. plot can saved TIFF PNG formats automatic filename generation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_line_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"","code":"create_line_plot(   data,   x_var,   y_var,   y_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"line_plot\",   use_geom_line = FALSE,   geom_line_group = NULL,   point_color = \"viridis\",   line_color = \"red\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_line_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"data dataframe containing data plotted. Must include variables specified x_var y_var. x_var string representing column name x-axis variable. categorical factor variable. y_var string representing column name y-axis variable. numeric variable. y_transform string specifying transformation y-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"line_plot\". use_geom_line boolean indicating whether include lines connecting points grouped data. Default FALSE. geom_line_group string representing column name group lines use_geom_line TRUE. categorical factor variable. point_color string specifying color points. Default \"viridis\", uses viridis color palette. line_color string specifying color summary line (median). Default \"red\". verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_line_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"function saves plot specified directory returns ggplot object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_line_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"","code":"# Example 1: Basic line plot with no transformations create_line_plot(   data = iris,   x_var = \"Species\",   y_var = \"Sepal.Length\",   y_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"iris_sepal_length\" ) #> Plots saved to: output/iris_sepal_length_20250311_064547.tiff and output/iris_sepal_length_20250311_064547.png   # Example 2: Line plot with log transformation and grouped lines create_line_plot(   data = mtcars,   x_var = \"cyl\",   y_var = \"mpg\",   y_transform = \"log\",   dpi = 150,   output_dir = \"plots\",   file_prefix = \"mtcars_log_mpg\",   use_geom_line = TRUE,   geom_line_group = \"gear\" ) #> Plots saved to: plots/mtcars_log_mpg_20250311_064548.tiff and plots/mtcars_log_mpg_20250311_064548.png   # Example 3: Line plot with square root transformation and customized aesthetics create_line_plot(   data = mtcars,   x_var = \"gear\",   y_var = \"hp\",   y_transform = \"sqrt\",   dpi = 300,   output_dir = \"custom_plots\",   file_prefix = \"mtcars_sqrt_hp\",   point_color = \"blue\",   line_color = \"green\",   verbose = TRUE ) #> Plots saved to: custom_plots/mtcars_sqrt_hp_20250311_064548.tiff and custom_plots/mtcars_sqrt_hp_20250311_064548.png"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_region_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","title":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"function generates map U.S. states, coloring regions, divisions, ACOG districts, ENT Board Governors Regions. Users can customize grouping use (ACOG Districts, ENT_Board_of_Governors_Regions, US Census Subdivisions).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_region_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"","code":"create_region_map(   remove_ak_hi = TRUE,   districts_per_group = \"acog_districts\",   save_path = NULL,   alpha_level = 0.4,   title = \"U.S. States by Region/Division or Custom Districts\",   subtitle = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_region_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"remove_ak_hi Logical. Whether exclude Alaska Hawaii map. Default TRUE. districts_per_group Character. grouping use regions. Options : \"acog_districts\" \"ENT_Board_of_Governors_Regions\" \"US_Census_Subdivisions\" save_path Character. optional file path save map image. NULL, map saved. Default NULL. alpha_level Numeric. transparency level map's fill color, 0 fully transparent 1 fully opaque. Default 0.4. title Character. title map. Default \"U.S. States Region/Division Custom Districts\". subtitle Character. optional subtitle map. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_region_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"ggplot object representing U.S. states colored region/division, ACOG districts, ENT_Board_of_Governors_Regions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_region_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"","code":"# Example 1: Create a map of U.S. states by ACOG districts if (FALSE) { # \\dontrun{ map_acog <- create_region_map(   remove_ak_hi = TRUE,   districts_per_group = \"acog_districts\",   save_path = \"acog_district_map.png\",   alpha_level = 0.4,   title = \"Map of U.S. States by ACOG Districts\",   subtitle = \"Excluding Alaska and Hawaii\" ) print(map_acog) } # }  # Example 2: Create a map of U.S. states by ENT Board of Governors Regions if (FALSE) { # \\dontrun{ map_ent <- create_region_map(   remove_ak_hi = FALSE,   districts_per_group = \"ENT_Board_of_Governors_Regions\",   alpha_level = 0.5,   title = \"Map of ENT Board of Governors Regions\",   subtitle = \"Including Alaska and Hawaii\" ) print(map_ent) } # }  # Example 3: Create a map of U.S. states by Census Subdivisions without saving if (FALSE) { # \\dontrun{ map_census <- create_region_map(   remove_ak_hi = TRUE,   districts_per_group = \"US_Census_Subdivisions\",   save_path = NULL,   alpha_level = 0.3,   title = \"Map of U.S. Census Subdivisions\",   subtitle = NULL ) print(map_census) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_scatter_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"function generates scatter plot designed mystery caller studies, allowing visualization waiting times similar outcomes across different categories, insurance types. function supports transformations y-axis, custom jitter, colors category x-axis using viridis color palette. plot automatically displayed saved specified resolution.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_scatter_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"","code":"create_scatter_plot(   data,   x_var,   y_var,   y_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"scatter_plot\",   jitter_width = 0.2,   jitter_height = 0,   point_alpha = 0.6,   x_label = NULL,   y_label = NULL,   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_scatter_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"data dataframe containing data plotted. Must contain variables specified x_var y_var. x_var string representing column name x-axis variable. categorical factor variable (e.g., insurance type). y_var string representing column name y-axis variable. numeric variable (e.g., waiting time days). y_transform string specifying transformation y-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"scatter_plot\". jitter_width numeric value specifying width jitter along x-axis. Default 0.2. jitter_height numeric value specifying height jitter along y-axis. Default 0. point_alpha numeric value specifying transparency level points. Default 0.6. x_label string specifying label x-axis. Default NULL (uses x_var). y_label string specifying label y-axis. Default NULL (uses y_var transformed version). plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_scatter_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_scatter_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"","code":"# Example 1: Basic scatter plot with log transformation create_scatter_plot(   data = df3,   x_var = \"insurance\",   y_var = \"business_days_until_appointment\",   y_transform = \"log\", # Log transformation   dpi = 100,   output_dir = \"ortho_sports_med/Figures\",   file_prefix = \"ortho_sports_vs_insurance\",   x_label = \"Insurance\",   y_label = \"Log (Waiting Times in Days)\",   plot_title = \"Scatter Plot of Waiting Times by Insurance\" ) #> Error: object 'df3' not found  # Example 2: Scatter plot with square root transformation and custom jitter create_scatter_plot(   data = df3,   x_var = \"insurance\",   y_var = \"business_days_until_appointment\",   y_transform = \"sqrt\", # Square root transformation   dpi = 150,   output_dir = \"ortho_sports_med/Figures\",   file_prefix = \"ortho_sports_vs_insurance_sqrt\",   jitter_width = 0.3,   jitter_height = 0.1,   x_label = \"Insurance\",   y_label = \"Square Root (Waiting Times in Days)\",   plot_title = \"Square Root Transformed Scatter Plot\" ) #> Error: object 'df3' not found  # Example 3: Scatter plot without any transformation and increased transparency create_scatter_plot(   data = df3,   x_var = \"insurance\",   y_var = \"business_days_until_appointment\",   y_transform = \"none\", # No transformation   dpi = 200,   output_dir = \"ortho_sports_med/Figures\",   file_prefix = \"ortho_sports_vs_insurance_none\",   point_alpha = 0.8,   x_label = \"Insurance\",   y_label = \"Waiting Times in Days\",   plot_title = \"Scatter Plot Without Transformation\" ) #> Error: object 'df3' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_subspecialty_venn.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","title":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","text":"function generates Venn diagram given subspecialty detailed logging inputs, outputs, transformations. uses tidyverse-style syntax better clarity logger package console-based logging.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_subspecialty_venn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","text":"","code":"create_subspecialty_venn(counts, title, verbose = FALSE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_subspecialty_venn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","text":"counts named list containing counts Venn diagram. must include following elements: businessDays, ableContact, acceptsPediatric, bd_ac, bd_ap, ac_ap, all_three, total. title character string specifying title diagram. verbose logical value. TRUE, logs detailed information process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_subspecialty_venn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","text":"Venn diagram object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_subspecialty_venn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Subspecialty Venn Diagram — create_subspecialty_venn","text":"","code":"# Example Venn Diagram for General Dermatology counts <- list(   businessDays = 92,   ableContact = 137,   acceptsPediatric = 131,   bd_ac = 91,   bd_ap = 92,   ac_ap = 93,   all_three = 91,   total = 269 ) create_subspecialty_venn(counts, \"General Dermatology\", verbose = TRUE) #> INFO [2025-03-11 06:45:50] Inputs validated successfully. #> INFO [2025-03-11 06:45:50] Counts input: #> Error in h(simpleError(msg, call)): `glue` failed in `formatter_glue` on: #>  #> List of 8 #>  $ businessDays    : num 92 #>  $ ableContact     : num 137 #>  $ acceptsPediatric: num 131 #>  $ bd_ac           : num 91 #>  $ bd_ap           : num 92 #>  $ ac_ap           : num 93 #>  $ all_three       : num 91 #>  $ total           : num 269 #>  #> Raw error message: #>  #> All unnamed arguments must be length 1 #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"function generates summary sentence based significant predictor variables Medicaid acceptance rates. logs process, performs error checking, includes default behavior easy testing robust execution.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"","code":"create_summary_sentence(   significant_predictors,   medicaid_acceptance_rate,   accepted_medicaid_count,   total_medicaid_physicians_count )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"significant_predictors data frame containing significant predictor variables, directions, formatted p-values. columns: \"Variable\", \"Direction\", \"Formatted_P_Value\". medicaid_acceptance_rate numeric value representing Medicaid acceptance rate (percentage). accepted_medicaid_count integer representing count physicians accepting Medicaid. total_medicaid_physicians_count integer representing total count physicians considered Medicaid.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"character string representing summary sentence.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/create_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"","code":"# Example 1: Basic usage with default values significant_vars <- data.frame(   Variable = c(\"Specialty\", \"Region\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.01\", \"0.02\") ) create_summary_sentence(significant_vars, 45.6, 100, 200) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>    Variable             Direction Formatted_P_Value #> 1 Specialty positively associated             <0.01 #> 2    Region negatively associated              0.02 #> Input: Medicaid Acceptance Rate: 45.6  #> Input: Count of Physicians Accepting Medicaid: 100  #> Input: Total Count of Physicians Considered for Medicaid: 200  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).\" #> [1] \"Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).\"  # Example 2: Using larger physician counts significant_vars <- data.frame(   Variable = c(\"Experience\", \"Training Level\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.05\", \"0.01\") ) create_summary_sentence(significant_vars, 35.2, 500, 1200) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>         Variable             Direction Formatted_P_Value #> 1     Experience positively associated             <0.05 #> 2 Training Level negatively associated              0.01 #> Input: Medicaid Acceptance Rate: 35.2  #> Input: Count of Physicians Accepting Medicaid: 500  #> Input: Total Count of Physicians Considered for Medicaid: 1200  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).\" #> [1] \"Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).\"  # Example 3: Using a single significant predictor significant_vars <- data.frame(   Variable = c(\"Age\"),   Direction = c(\"positively associated\"),   Formatted_P_Value = c(\"<0.001\") ) create_summary_sentence(significant_vars, 60.0, 750, 1250) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>   Variable             Direction Formatted_P_Value #> 1      Age positively associated            <0.001 #> Input: Medicaid Acceptance Rate: 60  #> Input: Count of Physicians Accepting Medicaid: 750  #> Input: Total Count of Physicians Considered for Medicaid: 1250  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Age positively associated (p = <0.001 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).\" #> [1] \"Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/determine_direction.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"function determines whether effect significant predictor positive (\"Higher\") negative (\"Lower\"). logs process, including inputs, outputs, step analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/determine_direction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"","code":"determine_direction(data, target_var, significant_vars)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/determine_direction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"data data frame containing dataset. target_var string representing name target variable (e.g., outcome dependent variable). significant_vars data frame containing significant predictors column \"Variable\" p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/determine_direction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"data frame additional column \"Direction\" indicating whether significant predictor associated \"Higher\" \"Lower\" effect.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/determine_direction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"","code":"# Example 1: Determine the direction of effects with a basic dataset df <- data.frame(   age = rnorm(100, mean = 50, sd = 10),   gender = factor(sample(c(\"Male\", \"Female\"), 100, replace = TRUE)),   accepts_medicaid = rbinom(100, 1, 0.5) ) significant_vars <- data.frame(Variable = c(\"age\", \"gender\")) determine_direction(df, \"accepts_medicaid\", significant_vars) #> Starting the determine_direction function... #> Target Variable: accepts_medicaid  #> Significant Variables Data Frame: #>   Variable #> 1      age #> 2   gender #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: age  #> Direction for variable age : Lower (Coefficient = -0.002751343 ) #> Processing variable: gender  #> Direction for variable gender : Higher (Coefficient = 0.02710193 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>   Variable Direction #> 1      age     Lower #> 2   gender    Higher #> determine_direction function completed successfully. Returning the result... #>   Variable Direction #> 1      age     Lower #> 2   gender    Higher  # Example 2: A dataset with multiple continuous predictors df2 <- data.frame(   income = rnorm(100, mean = 60000, sd = 15000),   education_years = rnorm(100, mean = 16, sd = 2),   accepts_insurance = rbinom(100, 1, 0.6) ) significant_vars2 <- data.frame(Variable = c(\"income\", \"education_years\")) determine_direction(df2, \"accepts_insurance\", significant_vars2) #> Starting the determine_direction function... #> Target Variable: accepts_insurance  #> Significant Variables Data Frame: #>          Variable #> 1          income #> 2 education_years #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: income  #> Direction for variable income : Lower (Coefficient = -8.614225e-06 ) #> Processing variable: education_years  #> Direction for variable education_years : Lower (Coefficient = -0.0514654 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>          Variable Direction #> 1          income     Lower #> 2 education_years     Lower #> determine_direction function completed successfully. Returning the result... #>          Variable Direction #> 1          income     Lower #> 2 education_years     Lower  # Example 3: Handling a dataset with categorical and continuous predictors df3 <- data.frame(   years_experience = rnorm(100, mean = 10, sd = 5),   specialty = factor(sample(c(\"Cardiology\", \"Neurology\"), 100, replace = TRUE)),   accepts_medicare = rbinom(100, 1, 0.7) ) significant_vars3 <- data.frame(Variable = c(\"years_experience\", \"specialty\")) determine_direction(df3, \"accepts_medicare\", significant_vars3) #> Starting the determine_direction function... #> Target Variable: accepts_medicare  #> Significant Variables Data Frame: #>           Variable #> 1 years_experience #> 2        specialty #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: years_experience  #> Direction for variable years_experience : Higher (Coefficient = 0.005577632 ) #> Processing variable: specialty  #> Direction for variable specialty : Higher (Coefficient = 0.02993891 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>           Variable Direction #> 1 years_experience    Higher #> 2        specialty    Higher #> determine_direction function completed successfully. Returning the result... #>           Variable Direction #> 1 years_experience    Higher #> 2        specialty    Higher"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/diagnose_mixed_effects_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","title":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","text":"Comprehensively assess suitability mixed-effects model analyzing group levels, observations, providing detailed diagnostic information.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/diagnose_mixed_effects_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","text":"","code":"diagnose_mixed_effects_model(   model_dataframe,   grouping_column,   dependent_column,   significance_threshold = 0.2,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/diagnose_mixed_effects_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","text":"model_dataframe data frame containing variables mixed-effects model analysis. grouping_column Name column representing grouping levels. dependent_column Name dependent/response variable column. significance_threshold Threshold identifying significant predictors (default 0.2). verbose Logical indicating whether print detailed diagnostic information (default TRUE).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/diagnose_mixed_effects_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","text":"list containing: suitable: Logical indicating model suitability. diagnostic_details: Detailed information group levels observations. summary_statistics: Descriptive statistics numeric columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/diagnose_mixed_effects_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Diagnose Mixed-Effects Model Suitability — diagnose_mixed_effects_model","text":"","code":"# Example 1: Basic usage with medical research dataset medical_research <- data.frame(   patient_id = rep(1:50, each = 3),   treatment_group = sample(c(\"A\", \"B\"), 150, replace = TRUE),   recovery_time = rnorm(150, mean = 10, sd = 2) ) model_diagnosis_1 <- diagnose_mixed_effects_model(   model_dataframe = medical_research,   grouping_column = \"patient_id\",   dependent_column = \"recovery_time\" ) #> Error: 'is.data.frame' is not an exported object from 'namespace:assertthat'"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":null,"dir":"Reference","previous_headings":"","what":"ENT Board of Governors Regions — ent_bog_regions","title":"ENT Board of Governors Regions — ent_bog_regions","text":"dataset maps U.S. states respective ENT Board Governors (ENT BOG) Regions. regions defined align organizational structure ENT Board. dataset commonly used regional analyses, visualizations, comparisons studies involving ENT professionals geographic distributions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ENT Board of Governors Regions — ent_bog_regions","text":"","code":"ent_bog_regions"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ENT Board of Governors Regions — ent_bog_regions","text":"data frame 50 rows 2 variables: State name U.S. state (e.g., \"California\", \"Texas\"). ENT_BOG_Region corresponding ENT Board Governors Region (e.g., \"Region 1\", \"Region 2\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ENT Board of Governors Regions — ent_bog_regions","text":"ENT Board Governors organizational materials. U.S. Census Bureau shapefiles geographic data: https://www.census.gov/geographies/mapping-files.html","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ENT Board of Governors Regions — ent_bog_regions","text":"Region 1 includes New England states. Region 2 encompasses New York, New Jersey, U.S. territories Caribbean. Regions organized administrative organizational purposes may align standard U.S. Census divisions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ENT Board of Governors Regions — ent_bog_regions","text":"ENT Board Governors. \"Regional Organization Overview.\" Retrieved https://www.entnet.org/-us/board--governors.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/ent_bog_regions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ENT Board of Governors Regions — ent_bog_regions","text":"","code":"# Load the dataset data(ent_bog_regions)  # View the first few rows head(ent_bog_regions) #>           State ENT_BOG_Region #> 1   Connecticut       Region 1 #> 2         Maine       Region 1 #> 3 Massachusetts       Region 1 #> 4 New Hampshire       Region 1 #> 5  Rhode Island       Region 1 #> 6       Vermont       Region 1  # Count the number of states in each ENT BOG Region table(ent_bog_regions$ENT_BOG_Region) #>  #>  Region 1  Region 2  Region 3  Region 4  Region 5  Region 6  Region 7  Region 8  #>         6         4         6         8         6         5         4         6  #>  Region 9 Region 10  #>         4         4"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fetch_population_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Population Data — fetch_population_data","title":"Fetch Population Data — fetch_population_data","text":"Fetches population data specified states, years, race/ethnicity variables ACS. function validates inputs, logs progress various stages, ensures results consistent reliable.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fetch_population_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Population Data — fetch_population_data","text":"","code":"fetch_population_data(   state = c(\"CO\"),   years = 2022,   race_vars = c(White = \"B02001_002E\", Black = \"B02001_003E\", Asian = \"B02001_005E\",     Hispanic = \"B03003_003E\") )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fetch_population_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Population Data — fetch_population_data","text":"state character vector state abbreviations (e.g., c(\"CO\", \"CA\")) single state abbreviation. Default c(\"CO\"). years numeric vector specifying years fetch data (e.g., 2015:2022). Default 2022. race_vars named character vector mapping race/ethnicity names variable codes (e.g., c(\"White\" = \"B02001_002E\")). Default includes common race categories ACS.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fetch_population_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Population Data — fetch_population_data","text":"tibble population data including columns: id Unique identifier geographic area (GEOID). population Estimated population given year, state, race/ethnicity. race_ethnicity Race/ethnicity category population estimate. geometry Geographic boundaries area (sf object). year Year population estimate.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fetch_population_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fetch Population Data — fetch_population_data","text":"","code":"# Example 1: Default behavior (Colorado, year 2022, default race variables) population_tibble <- fetch_population_data() #> INFO [2025-03-11 06:45:51] Starting `fetch_population_data` function. #> Error in h(simpleError(msg, call)): `glue` failed in `formatter_glue` on: #>  #>  'glue' chr [1:4] \"Function phase0_search_npi_by_number called with inputs: records_per_chunk = 2022, save_directory = B02001_002E\" ... #>  #> Raw error message: #>  #> All unnamed arguments must be length 1 #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces. print(population_tibble) #> Error: object 'population_tibble' not found  # Example 2: Fetch data for multiple states and years race_vars <- c(\"White\" = \"B02001_002E\", \"Black\" = \"B02001_003E\") multi_state_population <- fetch_population_data(   state = c(\"CO\", \"CA\"), years = 2015:2020,   race_vars = race_vars ) #> INFO [2025-03-11 06:45:51] Starting `fetch_population_data` function. #> Error: Variables must be length 1 or 6 print(multi_state_population) #> Error: object 'multi_state_population' not found  # Example 3: Use a custom set of race variables custom_race_vars <- c(\"Asian\" = \"B02001_005E\", \"Hispanic\" = \"B03003_003E\") custom_population <- fetch_population_data(state = \"TX\", years = 2018, race_vars = custom_race_vars) #> INFO [2025-03-11 06:45:51] Starting `fetch_population_data` function. #> Error in h(simpleError(msg, call)): `glue` failed in `formatter_glue` on: #>  #>  'glue' chr [1:2] \"Function phase0_search_npi_by_number called with inputs: records_per_chunk = 2018, save_directory = B02001_005E\" ... #>  #> Raw error message: #>  #> All unnamed arguments must be length 1 #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces. print(custom_population) #> Error: object 'custom_population' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/find_common_columns_in_years_of_open_payments_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"function identifies compares column names multiple years Open Payments data stored DuckDB. synchronizes column names across different tables, writes result Excel file, provides detailed logging.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/find_common_columns_in_years_of_open_payments_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"","code":"find_common_columns_in_years_of_open_payments_data(   con,   output_excel_path,   table_names = c(\"OP_DTL_GNRL_PGYR2014_P06302021\", \"OP_DTL_GNRL_PGYR2015_P06302021\",     \"OP_DTL_GNRL_PGYR2016_P01182024\", \"OP_DTL_GNRL_PGYR2017_P01182024\",     \"OP_DTL_GNRL_PGYR2018_P01182024\", \"OP_DTL_GNRL_PGYR2019_P01182024\",     \"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\",     \"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   beep_on_complete = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/find_common_columns_in_years_of_open_payments_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"con DuckDB connection object. output_excel_path string specifying path save output Excel file. table_names character vector specifying table names process. Defaults Open Payments data 2014 2023. beep_on_complete Logical value indicating beep sound played function completes. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/find_common_columns_in_years_of_open_payments_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"tibble column names table.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/find_common_columns_in_years_of_open_payments_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"","code":"# Example with default table names: con <- DBI::dbConnect(duckdb::duckdb(), \"path_to_duckdb.duckdb\") find_common_columns_in_years_of_open_payments_data(con, \"output.xlsx\") #> Starting find_common_columns_in_years_of_open_payments_data with inputs: #> Output Excel Path: output.xlsx #> Tables: OP_DTL_GNRL_PGYR2014_P06302021, OP_DTL_GNRL_PGYR2015_P06302021, OP_DTL_GNRL_PGYR2016_P01182024, OP_DTL_GNRL_PGYR2017_P01182024, OP_DTL_GNRL_PGYR2018_P01182024, OP_DTL_GNRL_PGYR2019_P01182024, OP_DTL_GNRL_PGYR2020_P01182024, OP_DTL_GNRL_PGYR2021_P01182024, OP_DTL_GNRL_PGYR2022_P01182024, OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> Processing table: OP_DTL_GNRL_PGYR2014_P06302021 #> Error in purrr::map(table_names, function(table) {    message(\"Processing table: \", table)    columns <- DBI::dbGetQuery(con, paste0(\"PRAGMA table_info(\",         table, \")\")) %>% dplyr::pull(name)    message(\"Retrieved \", length(columns), \" columns from table: \",         table)    return(columns)}): ℹ In index: 1. #> Caused by error: #> ! rapi_prepare: Failed to prepare query PRAGMA table_info(OP_DTL_GNRL_PGYR2014_P06302021) #> Error: Catalog Error: Table with name OP_DTL_GNRL_PGYR2014_P06302021 does not exist! #> Did you mean \"pg_am\"?  # Example with custom table names: custom_tables <- c(\"OP_DTL_GNRL_PGYR2014_P06302021\", \"OP_DTL_GNRL_PGYR2015_P06302021\") find_common_columns_in_years_of_open_payments_data(con, \"output_custom.xlsx\", custom_tables) #> Starting find_common_columns_in_years_of_open_payments_data with inputs: #> Output Excel Path: output_custom.xlsx #> Tables: OP_DTL_GNRL_PGYR2014_P06302021, OP_DTL_GNRL_PGYR2015_P06302021 #> Processing table: OP_DTL_GNRL_PGYR2014_P06302021 #> Error in purrr::map(table_names, function(table) {    message(\"Processing table: \", table)    columns <- DBI::dbGetQuery(con, paste0(\"PRAGMA table_info(\",         table, \")\")) %>% dplyr::pull(name)    message(\"Retrieved \", length(columns), \" columns from table: \",         table)    return(columns)}): ℹ In index: 1. #> Caused by error: #> ! rapi_prepare: Failed to prepare query PRAGMA table_info(OP_DTL_GNRL_PGYR2014_P06302021) #> Error: Catalog Error: Table with name OP_DTL_GNRL_PGYR2014_P06302021 does not exist! #> Did you mean \"pg_am\"?"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":null,"dir":"Reference","previous_headings":"","what":"FIPS Codes for U.S. States — fips","title":"FIPS Codes for U.S. States — fips","text":"dataset provides Federal Information Processing Standards (FIPS) codes U.S. states.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FIPS Codes for U.S. States — fips","text":"","code":"fips"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"FIPS Codes for U.S. States — fips","text":"data frame 52 rows 3 variables: state name U.S. state (e.g., \"Alabama\"). state_code two-digit FIPS code state (e.g., \"01\"). state_name full name state (e.g., \"Alabama\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"FIPS Codes for U.S. States — fips","text":"Derived U.S. Census Bureau data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"FIPS Codes for U.S. States — fips","text":"dataset maps state FIPS codes names abbreviations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fips.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"FIPS Codes for U.S. States — fips","text":"","code":"# Load the dataset data(fips)  # View the first few rows head(fips) #>     state state_code state_name #> 1      AL         01    Alabama #> 68     AK         02     Alaska #> 97     AZ         04    Arizona #> 112    AR         05   Arkansas #> 187    CA         06 California #> 245    CO         08   Colorado  # Get FIPS code for a specific state subset(fips, state == \"California\") #> [1] state      state_code state_name #> <0 rows> (or 0-length row.names)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fit_poisson_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","title":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","text":"function fits Poisson regression models analyze effect predictors target variable, wait times appointments. returns summary predictors significance levels (p-values).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fit_poisson_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","text":"","code":"fit_poisson_models(data, target_var, predictors)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fit_poisson_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","text":"data dataframe containing variables analysis. target_var string specifying name target variable (e.g., \"wait_time\"). predictors character vector predictor variable names include models.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fit_poisson_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","text":"tibble containing predictors, p-values, formatted p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/fit_poisson_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Poisson Models for Analyzing Wait Times — fit_poisson_models","text":"","code":"# Example: Fitting Poisson models with wait time data wait_data <- data.frame(   wait_time = rpois(100, lambda = 5),   insurance_type = sample(c(\"Medicaid\", \"Private\"), 100, replace = TRUE),   physician_id = sample(1:10, 100, replace = TRUE),   caller_scenario = sample(c(\"Vaginitis\", \"UTI\", \"Pregnancy Test\"), 100, replace = TRUE) )  fit_results <- fit_poisson_models(   data = wait_data,   target_var = \"wait_time\",   predictors = c(\"insurance_type\", \"caller_scenario\") ) #> INFO [2025-03-11 06:45:52] Fitting Poisson models for target variable 'wait_time' #> INFO [2025-03-11 06:45:52] Predictors: insurance_type, caller_scenario #> INFO [2025-03-11 06:45:52] Processing predictor: insurance_type #> INFO [2025-03-11 06:45:52] Model formula: wait_time ~ insurance_type #> INFO [2025-03-11 06:45:52] P-Value for 'insurance_type': 0.408277452631681 #> INFO [2025-03-11 06:45:52] Processing predictor: caller_scenario #> INFO [2025-03-11 06:45:52] Model formula: wait_time ~ caller_scenario #> INFO [2025-03-11 06:45:52] P-Value for 'caller_scenario': 0.778184412763231 #> INFO [2025-03-11 06:45:52] Finished fitting Poisson models. Returning results.  print(fit_results) #> # A tibble: 2 × 3 #>   Predictor       P_Value Formatted_P_Value #>   <chr>             <dbl> <chr>             #> 1 insurance_type    0.408 0.41              #> 2 caller_scenario   0.778 0.78"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":null,"dir":"Reference","previous_headings":"","what":"Format a Numeric Value as a Percentage — format_pct","title":"Format a Numeric Value as a Percentage — format_pct","text":"function formats numeric value vector percentage specified number decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format a Numeric Value as a Percentage — format_pct","text":"","code":"format_pct(x, my_digits = 1)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format a Numeric Value as a Percentage — format_pct","text":"x numeric value vector format percentage. my_digits integer specifying number decimal places include formatted percentage. default 1.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format a Numeric Value as a Percentage — format_pct","text":"character vector representing formatted percentage(s) specified number decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Format a Numeric Value as a Percentage — format_pct","text":"function converts numeric values percentage format desired number decimal places. especially useful ensuring consistent formatting reports, tables, visualizations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/format_pct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format a Numeric Value as a Percentage — format_pct","text":"","code":"# Example 1: Format a single numeric value with default decimal places single_value <- format_pct(0.12345) print(single_value) # Output: \"12.3%\" #> [1] \"12.3%\"  # Example 2: Format a vector of numeric values with 2 decimal places values <- c(0.12345, 0.6789, 0.54321) formatted_values <- format_pct(values, my_digits = 2) print(formatted_values) # Output: \"12.35%\", \"67.89%\", \"54.32%\" #> [1] \"12.35%\" \"67.89%\" \"54.32%\"  # Example 3: Format a single value with no decimal places no_decimal <- format_pct(0.5, my_digits = 0) print(no_decimal) # Output: \"50%\" #> [1] \"50%\"  # Example 4: Format a vector of proportions with varying decimal places proportions <- c(0.1, 0.25, 0.33333, 0.9) formatted_proportions <- format_pct(proportions, my_digits = 3) print(formatted_proportions) # Output: \"10.000%\", \"25.000%\", \"33.333%\", \"90.000%\" #> [1] \"10.000%\" \"25.000%\" \"33.333%\" \"90.000%\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_acog_districts_sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate ACOG Districts sf Object — generate_acog_districts_sf","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"function generates sf object representing ACOG districts grouping states particular district together.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_acog_districts_sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"","code":"generate_acog_districts_sf(filepath = NULL)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_acog_districts_sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"sf object containing grouped ACOG districts.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"function generates interpretative sentences interaction terms regression model (Poisson logistic), detailing combinations predictors impact outcomes wait times.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"","code":"generate_interaction_sentences(   interaction_model,   variable1,   variable2,   model_summary = NULL,   confidence_level = 0.95,   log_transform = TRUE,   output_format = \"text\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"interaction_model fitted regression model object (e.g., glm lmer). variable1 first variable interaction (character string). variable2 second variable interaction (character string). model_summary optional model summary object. provided, calculated interaction_model. confidence_level confidence level reporting intervals. Default 0.95. log_transform Logical. TRUE, log-transformed estimates used. Default TRUE. output_format format output sentences. Options \"text\" (default) \"markdown\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"list strings, interpreting interaction term model.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"function uses estimated marginal means compute interpret interaction effects. Sentences include confidence intervals p-values available.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_interaction_sentences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Interaction Sentences for Model Interpretation — generate_interaction_sentences","text":"","code":"# Example: Generate interaction sentences for a Poisson model interaction_model <- glm(wait_time ~ insurance_type * scenario,   data = poisson_data, family = poisson ) #> Error in eval(mf, parent.frame()): object 'poisson_data' not found  interaction_sentences <- generate_interaction_sentences(   interaction_model = interaction_model,   variable1 = \"insurance_type\",   variable2 = \"scenario\",   output_format = \"text\" ) #> INFO [2025-03-11 06:45:53] Starting generate_interaction_sentences... #> INFO [2025-03-11 06:45:53] Variables: insurance_type, scenario #> INFO [2025-03-11 06:45:53] Confidence level: 0.95, Log transform: TRUE #> INFO [2025-03-11 06:45:53] Output format: text #> Error: object 'interaction_model' not found print(interaction_sentences) #> Error: object 'interaction_sentences' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate LaTeX Equation with Logging — generate_latex_equation","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"function generates LaTeX equation incorporates provided \"patient_scenario_label\". function logs input, processes input string escaping LaTeX characters, constructs LaTeX equation. logs operations can output console log file (provided).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"","code":"generate_latex_equation(   patient_scenario_label = \"Default Patient Scenario\",   log_file = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"patient_scenario_label string representing text (typically \"Patient Scenario\") inserted LaTeX equation. Default \"Default Patient Scenario\". log_file string representing full path file logs written. NULL (default), logs printed console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"LaTeX code string, ready inserted RMarkdown LaTeX document.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"function generates dynamic LaTeX code using provided patient_scenario_label, ensuring special LaTeX characters (underscores) escaped properly. logs entire process, including input validation, transformations, final output. patient_scenario_label provided, default value used.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_latex_equation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"","code":"# Example 1: Basic usage with logging to the console if (FALSE) { # \\dontrun{ generate_latex_equation(\"Patient Scenario\") } # }  # Example 2: Handle underscores in the patient_scenario_label if (FALSE) { # \\dontrun{ generate_latex_equation(\"Patient_Scenario_With_Underscores\") } # }  # Example 3: Logging the process to a file if (FALSE) { # \\dontrun{ log_file_path <- \"latex_generation_log.txt\" generate_latex_equation(\"Patient Scenario\", log_file = log_file_path) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_leaflet_base_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Leaflet Base Map — generate_leaflet_base_map","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"function creates Leaflet BASE map specific configurations, including base tile layer, scale bar, default view settings, layers control.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_leaflet_base_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"","code":"generate_leaflet_base_map()"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_leaflet_base_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_leaflet_base_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"","code":"map <- generate_leaflet_base_map()"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_overall_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Overall Table with Error Handling and Logging — generate_overall_table","title":"Generate Overall Table with Error Handling and Logging — generate_overall_table","text":"function generates overall table summarizing demographics Table 1 data. logs key steps, including inputs, outputs, data transformations, file paths. function supports RDS, CSV, XLS file formats creates overall summary table saved PDF file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_overall_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Overall Table with Error Handling and Logging — generate_overall_table","text":"","code":"generate_overall_table(   input_file_path,   output_directory,   title = \"Overall Table Summary\",   selected_columns = NULL,   label_translations = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_overall_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Overall Table with Error Handling and Logging — generate_overall_table","text":"input_file_path string representing path data file (RDS, CSV, XLS format). output_directory string representing directory output table file saved. title string specifying title overall table summary (default \"Overall Table Summary\"). selected_columns optional vector selected columns include table. Default NULL. label_translations optional named list label translations use table summary.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_overall_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Overall Table with Error Handling and Logging — generate_overall_table","text":"Nothing returned. function saves output table PDF file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/generate_overall_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Overall Table with Error Handling and Logging — generate_overall_table","text":"","code":"if (FALSE) { # \\dontrun{ generate_overall_table(\"data/Table1.rds\", \"output_tables\") generate_overall_table(\"data/Table1.csv\", \"output_tables\", selected_columns = c(\"age\", \"gender\")) label_translations <- list(age = \"Age (years)\", gender = \"Gender\") generate_overall_table(\"data/Table1.xlsx\", \"output_tables\",   title = \"Demographic Summary\",   label_translations = label_translations ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Census Data for State Block Groups — get_census_data","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"function retrieves Census data state block groups looping specified list state FIPS codes. data fetched using Census API specified vintage year aggregated single dataframe.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"","code":"get_census_data(state_fips_codes, vintage_year = 2022)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"state_fips_codes character vector state FIPS codes Census data retrieved. FIPS codes must two-character strings representing U.S. states (e.g., \"01\" Alabama). vintage_year integer specifying vintage year Census data. Default 2022.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"dataframe containing Census data specified state block groups, including metadata.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"function uses censusapi package query Census data via American Community Survey (ACS) API. Data retrieved block groups across specified states aggregated single dataframe. Users must provide valid Census API key via key argument environment variables.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_census_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Census Data for State Block Groups — get_census_data","text":"","code":"# Example 1: Retrieve Census data for two states with the default vintage year if (FALSE) { # \\dontrun{ state_fips_codes <- c(\"01\", \"02\") # Alabama and Alaska census_data <- get_census_data(state_fips_codes) head(census_data) } # }  # Example 2: Retrieve Census data for multiple states with a different vintage year if (FALSE) { # \\dontrun{ state_fips_codes <- c(\"04\", \"05\", \"06\") # Arizona, Arkansas, California census_data <- get_census_data(state_fips_codes, vintage_year = 2021) print(dim(census_data)) # Display dimensions of the retrieved data } # }  # Example 3: Retrieve Census data for a subset of states and save it to a CSV file if (FALSE) { # \\dontrun{ state_fips_codes <- c(\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\") census_data <- get_census_data(state_fips_codes) write.csv(census_data, file = \"census_data_block_groups.csv\", row.names = FALSE) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_most_common.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Most Common Value — get_most_common","title":"Get the Most Common Value — get_most_common","text":"function calculates returns common value tabulation result.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_most_common.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Most Common Value — get_most_common","text":"","code":"get_most_common(tabyl_result, variable_name)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_most_common.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Most Common Value — get_most_common","text":"tabyl_result tabulation result data frame. variable_name name categorical variable.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/get_most_common.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Most Common Value — get_most_common","text":"common value.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/honeycomb_generate_maps.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"function generates hexagon maps ACOG districts, using physician location data. provides extensive logging, error handling, defaults sample data inputs provided.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/honeycomb_generate_maps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"","code":"honeycomb_generate_maps(   physician_locations_sf = NULL,   acog_districts_sf = NULL,   trait_map = \"all\",   honeycomb_map = \"all\",   hex_grid_size = c(0.3, 0.3),   target_district = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/honeycomb_generate_maps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"physician_locations_sf Simple Features (sf) object containing physician data coordinates. NULL, default sample data used. acog_districts_sf Simple Features (sf) object containing grouped ACOG districts. NULL, default ACOG districts used. trait_map string specifying trait map (default \"\"). honeycomb_map string specifying honey map (default \"\"). hex_grid_size numeric vector length 2 specifying grid size hexagon map (default c(0.3, 0.3)). target_district string NULL specify specific district generating map (default NULL, processes districts).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/honeycomb_generate_maps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"list ggplot objects generated maps specified districts. ggplot object hexagon map showing physician distribution.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/honeycomb_generate_maps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"","code":"if (FALSE) { # \\dontrun{ # Generate hexagon maps with default inputs maps <- honeycomb_generate_maps() print(maps[[1]])  # Generate hexagon maps for a specific ACOG district maps <- honeycomb_generate_maps(target_district = \"District I\") print(maps[[1]])  # Customize hex grid size and process all districts maps <- honeycomb_generate_maps(hex_grid_size = c(0.5, 0.5)) print(maps[[1]]) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Hospital Referral Region Shapefile — hrr","title":"Get Hospital Referral Region Shapefile — hrr","text":"function loads hospital referral region shapefile optionally removes Hawaii Alaska.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Hospital Referral Region Shapefile — hrr","text":"","code":"hrr(remove_HI_AK = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Hospital Referral Region Shapefile — hrr","text":"remove_HI_AK Logical, Hawaii Alaska removed? Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Hospital Referral Region Shapefile — hrr","text":"sf object containing hospital referral region data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr_generate_maps.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"function generates hexagon maps hospital referral regions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr_generate_maps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"","code":"hrr_generate_maps(physician_sf, trait_map = \"all\", honey_map = \"all\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr_generate_maps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"physician_sf sf object containing physician data coordinates. trait_map string specifying trait map (default \"\"). honey_map string specifying honey map (default \"\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/hrr_generate_maps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"ggplot object generated map.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"function checks presence specified CRAN packages user's system. package already installed, automatically installs dependencies. packages already installed, function logs version numbers console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"","code":"install_missing_packages(cran_pkgs)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"cran_pkgs character vector containing names CRAN packages verify install missing. element name package available CRAN.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"utility function designed ensure necessary CRAN packages installed project. uses installed.packages check existing packages install.packages install missing packages. already-installed packages, function retrieves displays version number installed package metadata. function automatically handles package dependencies installation passing dependencies = TRUE install.packages.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"function requires active internet connection download install packages CRAN. Ensure R environment write permissions library path.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/install_missing_packages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install Missing CRAN Packages with Version Reporting — install_missing_packages","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Ensure ggplot2 and dplyr are installed install_missing_packages(c(\"ggplot2\", \"dplyr\"))  # Example 2: Install a set of commonly used packages for data wrangling and visualization install_missing_packages(c(\"tidyr\", \"readr\", \"ggplot2\", \"dplyr\"))  # Example 3: Check for and install machine learning packages install_missing_packages(c(\"caret\", \"randomForest\", \"xgboost\"))  # Example 4: Use this function to ensure all required packages for a project are installed required_packages <- c(\"ggplot2\", \"dplyr\", \"tidyr\", \"readr\", \"caret\", \"randomForest\") install_missing_packages(required_packages) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"function sends Poisson regression results, including estimated wait times 95% confidence intervals, ChatGPT via OpenAI API. ChatGPT processes results provides interpretation, highlighting scenarios significantly higher lower estimated wait times offering insights based confidence intervals.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"","code":"interpret_results_with_chatgpt(stat_summary, sentence_summary)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"stat_summary tibble containing Poisson regression statistics group, including: Group name group category. median_wait_time median wait time (e.g., business days) group. wait_time_q1 25th percentile wait time group. wait_time_q3 75th percentile wait time group. sentence_summary tibble containing pre-computed summary sentences group, explaining estimated wait times highlighting significant findings.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"character string containing interpretation Poisson regression results, generated ChatGPT.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"function converts input tibbles JSON-compatible format sends OpenAI GPT-4 model using OpenAI API. ChatGPT analyzes provided data responds concise interpretation, focusing scenarios statistically significant differences estimated wait times. function requires active OpenAI API key make requests. Ensure environment variable OPENAI_API_KEY set using function.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"Use function caution sensitive data, relies third-party APIs interpretation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/interpret_results_with_chatgpt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interpret Poisson Regression Results Using ChatGPT — interpret_results_with_chatgpt","text":"","code":"if (FALSE) { # \\dontrun{ # Example: Using the function to interpret Poisson regression results # Assume `results$stat_summary` and `results$sentence_summary` are precomputed tibbles. interpretation <- interpret_results_with_chatgpt(   stat_summary = results$stat_summary,   sentence_summary = results$sentence_summary )  # Print the interpretation provided by ChatGPT cat(interpretation) } # }  # Example: Constructing tibbles manually for demonstration library(dplyr) stat_summary <- tibble(   Group = c(\"Group A\", \"Group B\", \"Group C\"),   median_wait_time = c(5, 10, 15),   wait_time_q1 = c(4, 8, 12),   wait_time_q3 = c(6, 12, 18) )  sentence_summary <- tibble(   Group = c(\"Group A\", \"Group B\", \"Group C\"),   summary_sentence = c(     \"Group A has the shortest median wait time (5 days).\",     \"Group B has a moderate wait time (10 days).\",     \"Group C has the longest wait time (15 days).\"   ) )  interpretation <- interpret_results_with_chatgpt(   stat_summary = stat_summary,   sentence_summary = sentence_summary ) #> Error in interpret_results_with_chatgpt(stat_summary = stat_summary, sentence_summary = sentence_summary): could not find function \"interpret_results_with_chatgpt\" cat(interpretation) #> Error: object 'interpretation' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"function generates summary sentence using linear regression analyze trend proportion women without access gynecologic oncologist time specified race driving time. includes raw proportions first last years dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"","code":"linear_regression_race_drive_time_generate_summary_sentence(   tabulated_data,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"tabulated_data data frame containing data analyze. Must include columns Driving Time (minutes), Year, columns race proportions like White_prop, Black_prop, etc. driving_time_minutes numeric value specifying driving time minutes filter data. Default 180. race character string specifying race generate summary sentence. Supported values \"White\", \"Black\", \"American Indian/Alaska Native\", \"Asian\", \"Native Hawaiian Pacific Islander\", \"\" generate sentences supported races. Default \"American Indian/Alaska Native\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"character string containing summary sentence, list summary sentences race = \"\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"","code":"# Example usage summary_sentence <- linear_regression_race_drive_time_generate_summary_sentence(   tabulated_data = tabulated_all_years_numeric,   driving_time_minutes = 180,   race = \"White\" ) #> Function linear_regression_race_drive_time_generate_summary_sentence called with inputs: #> Driving Time (minutes): 180 #> Race: White #> Race column used for analysis: White_prop #> Error: object 'tabulated_all_years_numeric' not found print(summary_sentence) #> Error: object 'summary_sentence' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load and Process Data from an RDS File with Robust Logging — load_data","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"function loads data RDS file, renames 'ID' column 'id_number', logs every step process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"","code":"load_data(data_dir, file_name, verbose = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"data_dir string specifying directory RDS file located. file_name string specifying name RDS file load. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"data frame 'ID' column renamed 'id_number'.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"function performs following steps: Constructs full file path data_dir file_name. Validates existence RDS file specified path. Loads data RDS file. Renames 'ID' column 'id_number'. Logs detailed information step verbose = TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/load_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"","code":"# Example 1: Load data from a specified directory with logging if (FALSE) { # \\dontrun{ df <- load_data(data_dir = \"data\", file_name = \"Phase_2.rds\", verbose = TRUE) } # }  # Example 2: Load data without logging if (FALSE) { # \\dontrun{ df <- load_data(data_dir = \"data\", file_name = \"Phase_2.rds\", verbose = FALSE) } # }  # Example 3: Handle missing file error if (FALSE) { # \\dontrun{ tryCatch(   {     df <- load_data(data_dir = \"invalid_path\", file_name = \"missing_file.rds\", verbose = TRUE)   },   error = function(e) {     cat(\"Error encountered:\", e$message, \"\\n\")   } ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"function performs logistic regression multiple predictor variables specified target variable. logs process, including inputs, data transformations, results. function filters predictors based given significance level.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"","code":"logistic_regression(df, target_variable, predictor_vars, significance_level)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"df data frame containing target predictor variables. target_variable string representing name target variable. predictor_vars vector strings representing names predictor variables. significance_level numeric value specifying significance level filtering predictors.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"data frame containing significant predictors p-values formatted p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"","code":"# Assuming df is your data frame target_variable <- \"cleaned_does_the_physician_accept_medicaid_numeric\" predictor_vars <- setdiff(   names(df),   c(     target_variable,     \"does_the_physician_accept_medicaid\",     \"cleaned_does_the_physician_accept_medicaid\"   ) ) significance_logistic_regression <- 0.2  significant_vars <- logistic_regression(   df,   target_variable,   predictor_vars,   significance_level = significance_logistic_regression ) #> Starting logistic_regression... #> Target Variable: cleaned_does_the_physician_accept_medicaid_numeric  #> Predictor Variables:   #> Significance Level: 0.2  #> Significant Predictors: #> [1] Variable          P_Value           Formatted_P_Value #> <0 rows> (or 0-length row.names) print(significant_vars) #> [1] Variable          P_Value           Formatted_P_Value #> <0 rows> (or 0-length row.names)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/map_physicians_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Map Physicians by State or Subdivision — map_physicians_by_group","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"function generates choropleth map based physician counts saves map PNG file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/map_physicians_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"","code":"map_physicians_by_group(   state_counts,   output_file_prefix = \"choropleth_map\",   group_by = \"state\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/map_physicians_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"state_counts tibble containing counts physicians per state subdivision. Must include columns region count. output_file_prefix character string specifying prefix output PNG file choropleth map. Default \"choropleth_map\". group_by character string indicating whether counts grouped \"state\" \"subdivision\". Default \"state\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/map_physicians_by_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"ggplot2 object representing choropleth map.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/map_physicians_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"","code":"# Example 1: Map physicians by state if (FALSE) { # \\dontrun{ state_counts <- count_physicians_by_group(taxonomy_and_aaos_data) map_physicians_by_group(state_counts) } # }  # Example 2: Map physicians by U.S. Census Bureau subdivision if (FALSE) { # \\dontrun{ subdivision_counts <- count_physicians_by_group(taxonomy_and_aaos_data, group_by = \"subdivision\") map_physicians_by_group(subdivision_counts, group_by = \"subdivision\") } # }  # Example 3: Save the map with a custom output file prefix if (FALSE) { # \\dontrun{ state_counts <- count_physicians_by_group(taxonomy_and_aaos_data) map_physicians_by_group(state_counts, output_file_prefix = \"custom_map_prefix\") } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_expansion_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Medicaid Expansion Status by State — medicaid_expansion_status","title":"Medicaid Expansion Status by State — medicaid_expansion_status","text":"dataset indicating whether state expanded Medicaid eligibility Affordable Care Act (ACA).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_expansion_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Medicaid Expansion Status by State — medicaid_expansion_status","text":"","code":"medicaid_expansion_status"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_expansion_status.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Medicaid Expansion Status by State — medicaid_expansion_status","text":"data frame 52 rows 2 columns: Location State name abbreviation. Medicaid_Expansion Logical value indicating whether state expanded Medicaid (TRUE FALSE).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_expansion_status.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Medicaid Expansion Status by State — medicaid_expansion_status","text":"Kaiser Family Foundation: https://www.kff.org/medicaid/issue-brief/status--state-medicaid-expansion-decisions-interactive-map/.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_to_medicare_fee_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","title":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","text":"dataset measuring state's physician fees relative Medicare fees state. Medicaid--Medicare fee index compares Medicaid fee service state Medicare fee services, using surveys conducted Urban Institute.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_to_medicare_fee_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","text":"","code":"medicaid_to_medicare_fee_index"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_to_medicare_fee_index.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","text":"data frame 52 rows 5 columns: Location State name abbreviation. Services Fee index services, relative Medicare, percentage. Primary Care Fee index primary care services eligible ACA \"fee bump,\" percentage. Obstetric Care Fee index obstetric care services, percentage. Services Fee index non-primary care services, percentage.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_to_medicare_fee_index.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","text":"Urban Institute: https://www.kff.org/medicaid/state-indicator/medicaid--medicare-fee-index/.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/medicaid_to_medicare_fee_index.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Medicaid-to-Medicare Fee Index — medicaid_to_medicare_fee_index","text":"numeric values scaled percentages (multiplied 100).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/men_and_women_by_zip.html","id":null,"dir":"Reference","previous_headings":"","what":"Men and Women by ZIP Code — men_and_women_by_zip","title":"Men and Women by ZIP Code — men_and_women_by_zip","text":"dataset containing total number men, women, total population ZIP Code Tabulation Area (ZCTA) United States, based American Community Survey (ACS) 5-Year Estimates.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/men_and_women_by_zip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Men and Women by ZIP Code — men_and_women_by_zip","text":"","code":"men_and_women_by_zip"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/men_and_women_by_zip.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Men and Women by ZIP Code — men_and_women_by_zip","text":"data frame three columns: zip_code ZIP Code Tabulation Area (ZCTA). men estimated total number men ZCTA. women estimated total number women ZCTA. total_population total estimated population ZCTA (men + women).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/men_and_women_by_zip.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Men and Women by ZIP Code — men_and_women_by_zip","text":"U.S. Census Bureau, ACS 5-Year Estimates, 2021.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/men_and_women_by_zip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Men and Women by ZIP Code — men_and_women_by_zip","text":"","code":"data(men_and_women_by_zip) head(men_and_women_by_zip) #> # A tibble: 6 × 4 #>   zip_code total_men total_women total_population #>   <chr>        <dbl>       <dbl>            <dbl> #> 1 00601         8337        8497            16834 #> 2 00602        18405       19237            37642 #> 3 00603        23813       25262            49075 #> 4 00606         2723        2867             5590 #> 5 00610        12317       13225            25542 #> 6 00611          667         648             1315"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"function calculates returns sentence describes common gender, specialty, training, academic affiliation provided dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"","code":"most_common_gender_training_academic(df)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"df data frame containing columns gender, specialty, Provider.Credential.Text, academic_affiliation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"character string summarizing common gender, specialty, training, academic affiliation along respective proportions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"function filters missing values column determining common value. calculates proportion common value relative total non-missing values column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/most_common_gender_training_academic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"","code":"# Example 1: Basic usage with a small dataset df <- data.frame(   gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", \"Cardiology\", \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\") ) result <- most_common_gender_training_academic(df) print(result) #> [1] \"The most common gender in the dataset was male (60%). The most common specialty was Cardiology (60%). The most common training was MD (60%). The academic affiliation status most frequently occurring was yes (60%).\"  # Example 2: Handling missing data df_with_na <- data.frame(   gender = c(\"Male\", NA, \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", NA, \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", NA) ) result <- most_common_gender_training_academic(df_with_na) print(result) #> [1] \"The most common gender in the dataset was male (75%). The most common specialty was Cardiology (50%). The most common training was MD (60%). The academic affiliation status most frequently occurring was no (50%).\"  # Example 3: Different proportions with a larger dataset df_large <- data.frame(   gender = c(rep(\"Male\", 70), rep(\"Female\", 30)),   specialty = c(rep(\"Cardiology\", 50), rep(\"Neurology\", 30), rep(\"Orthopedics\", 20)),   Provider.Credential.Text = c(rep(\"MD\", 60), rep(\"DO\", 40)),   academic_affiliation = c(rep(\"Yes\", 40), rep(\"No\", 60)) ) result <- most_common_gender_training_academic(df_large) print(result) #> [1] \"The most common gender in the dataset was male (70%). The most common specialty was Cardiology (50%). The most common training was MD (60%). The academic affiliation status most frequently occurring was no (60%).\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":null,"dir":"Reference","previous_headings":"","what":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"function processes single year's NPPES (National Plan Provider Enumeration System) data, filtering specified taxonomy codes saving cleaned data CSV file. data processed chunks handle large datasets efficiently, optional functionality allows saving sample columns Excel file inspection.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"","code":"nppes_get_data_for_one_year(   npi_file_path,   output_csv_path,   duckdb_file_path =     \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\",   taxonomy_codes_1 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   taxonomy_codes_2 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   save_column_in_each_nppes_year = FALSE,   excel_file_path = NULL )  nppes_get_data_for_one_year(   npi_file_path,   output_csv_path,   duckdb_file_path =     \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\",   taxonomy_codes_1 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   taxonomy_codes_2 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   save_column_in_each_nppes_year = FALSE,   excel_file_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"npi_file_path character string specifying path raw NPPES data CSV file. output_csv_path character string specifying full file path cleaned data saved. duckdb_file_path character string specifying path DuckDB database file. Default \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\". taxonomy_codes_1 character vector taxonomy codes used filter rows based Healthcare Provider Taxonomy Code_1. taxonomy_codes_2 character vector taxonomy codes used filter rows based Healthcare Provider Taxonomy Code_2. save_column_in_each_nppes_year logical value indicating whether save sample data Excel file inspection. Default FALSE. excel_file_path character string specifying path save Excel file save_column_in_each_nppes_year TRUE. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"data frame containing cleaned NPPES data one year, saved specified file path. dataframe containing cleaned NPPES data one year. data also saved specified CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"function processes large NPPES datasets leveraging DuckDB efficient chunk-wise processing. data filtered based taxonomy codes cleaned appended output CSV file. function optionally saves sample data Excel file inspection.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/nppes_get_data_for_one_year.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"","code":"# Example 1: Process NPPES data for one year and save to CSV if (FALSE) { # \\dontrun{ nppes_get_data_for_one_year(   npi_file_path = \"nppes_raw_2022.csv\",   output_csv_path = \"nppes_cleaned_2022.csv\" ) } # }  # Example 2: Process NPPES data and filter by taxonomy codes if (FALSE) { # \\dontrun{ nppes_get_data_for_one_year(   npi_file_path = \"nppes_raw_2022.csv\",   output_csv_path = \"nppes_cleaned_filtered_2022.csv\",   taxonomy_codes_1 = c(\"207V00000X\", \"207VG0400X\"),   taxonomy_codes_2 = c(\"207V00000X\", \"207VG0400X\") ) } # }  # Example 3: Save sample data to Excel for inspection if (FALSE) { # \\dontrun{ nppes_get_data_for_one_year(   npi_file_path = \"nppes_raw_2022.csv\",   output_csv_path = \"nppes_cleaned_2022.csv\",   save_column_in_each_nppes_year = TRUE,   excel_file_path = \"nppes_sample_2022.xlsx\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"function reads filtered OBGYN data CSV file, merges crosswalk data RDS file NPI column, adds prefixes distinguish columns dataset, writes merged data CSV file. Detailed logging performed step process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"","code":"open_payments_collect_and_convert(   duckdb_file_path,   specialty_csv_path,   crosswalk_rds_path,   log_path,   output_csv =     \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/end_csv_output_open_payments_collect_and_convert.csv\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"duckdb_file_path string representing full path DuckDB database file. used logging purposes, although data read directly DuckDB function. specialty_csv_path string representing full path CSV file containing filtered OBGYN data Open Payments dataset. file contain \"Covered_Recipient_NPI\" column. crosswalk_rds_path string representing full path RDS file containing crosswalk data. crosswalk data contain \"NPI\" column matches \"Covered_Recipient_NPI\" OBGYN data. log_path string representing full path text file logging details execution function written. output_csv string representing full path output CSV file merged data written. default \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/end_csv_output_open_payments_collect_and_convert.csv\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"data frame containing merged data OBGYN crosswalk datasets. columns OBGYN dataset prefixed \"OP_\" columns crosswalk dataset prefixed \"lo_data_\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"function follows steps: Reads OBGYN data provided CSV file renames \"Covered_Recipient_NPI\" column \"NPI\". Adds prefix \"OP_\" columns OBGYN data avoid column name conflicts final merged data. Reads crosswalk data provided RDS file, ensures unique NPI values retained using distinct(), adds prefix \"lo_data_\" columns. Performs left join OBGYN data crosswalk data \"NPI\" column (now prefixed \"OP_NPI\" \"lo_data_NPI\"). Logs number rows merged data writes merged data specified output CSV file. Logs steps, including DuckDB connection, data loading, column renaming, merging, output writing.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_collect_and_convert.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"","code":"# Example 1: Using default output path and logging to a file if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\",   specialty_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv\",   crosswalk_rds_path = \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/merged_into_a_crosswalk.rds\",   log_path = \"/path/to/log_file.txt\" ) } # }  # Example 2: Custom output path for CSV and detailed logging if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/path/to/duckdb_file.duckdb\",   specialty_csv_path = \"/path/to/specialty_data.csv\",   crosswalk_rds_path = \"/path/to/crosswalk_data.rds\",   log_path = \"/path/to/detailed_log.txt\",   output_csv = \"/path/to/merged_output_data.csv\" ) } # }  # Example 3: Using different file locations for each argument if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/my_data/nppes_historical_downloads/duckdb_file.duckdb\",   specialty_csv_path = \"/my_data/open_payments_data/filtered_data.csv\",   crosswalk_rds_path = \"/my_data/crosswalk_data/crosswalk_file.rds\",   log_path = \"/my_data/logs/processing_log.txt\",   output_csv = \"/my_data/output/merged_data.csv\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_processed_per_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"function reads data CSV file chunks, processes grouping data Covered_Recipient_NPI, writes result specified CSV file current date file name. includes logging, error handling, system beeps progress completion.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_processed_per_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"","code":"open_payments_processed_per_npi(   input_csv_path,   output_csv_path =     \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/end_payments_per_npi.csv\",   chunk_size = 1e+05,   log_function = message,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_processed_per_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"input_csv_path file path read input CSV file. output_csv_path file path write resulting CSV file (current date appended). path checked existence writability writing. chunk_size Integer, size chunks process. Defaults 100,000 rows per chunk. log_function logging function, defaults message. verbose Logical, whether display detailed logging. Defaults TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_processed_per_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"list containing total rows processed output CSV file path.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_processed_per_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"","code":"# Example 1: Process payments per NPI from CSV and write output with date result <- open_payments_processed_per_npi(   input_csv_path = \"path/to/input.csv\",   output_csv_path = \"path/to/output.csv\" ) #> [2024-10-06 19:54:36.361408] Starting open_payments_processed_per_npi. #> Error in open_payments_processed_per_npi(input_csv_path = \"path/to/input.csv\",     output_csv_path = \"path/to/output.csv\"): The input file 'path/to/input.csv' does not exist.  # Example 2: Custom logging function and different output path result <- open_payments_processed_per_npi(   input_csv_path = \"path/to/input.csv\",   output_csv_path = \"custom/output/path.csv\",   log_function = print ) #> [2024-10-06 19:54:36.363739] Starting open_payments_processed_per_npi. #> Error in open_payments_processed_per_npi(input_csv_path = \"path/to/input.csv\",     output_csv_path = \"custom/output/path.csv\", log_function = print): The input file 'path/to/input.csv' does not exist."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_specialty_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"function connects DuckDB database, processes tables containing Open Payments data, filters records based specified medical specialties. data processed chunks minimize memory usage, filtered results saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_specialty_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"","code":"open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(),     \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\",     \"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   output_csv_path =     \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv\",   specialties = c(\"Specialist\",     \"Student in an Organized Health Care Education/Training Program\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Critical Care Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Female Pelvic Medicine and Reconstructive Surgery\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecologic Oncology\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecology\",           \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Hospice and Palliative Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Obstetrics\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology\"),   chunk_size = 1e+05 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_specialty_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"con Database connection DuckDB database. Defaults connection using provided DuckDB file path. table_names character vector table names DuckDB processed. table processed chunks reduce memory load. output_csv_path file path cleaned filtered Open Payments data saved. file path directory exist, created. specialties character vector medical specialties filter data. function return rows Covered_Recipient_Specialty_X columns match one given specialties. chunk_size size data chunk process time. allows function handle large datasets efficiently breaking query smaller parts. Defaults 100,000 rows per chunk.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_specialty_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"list containing names processed tables.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/open_payments_specialty_cleaning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"","code":"# Example 1: Process two Open Payments tables for specific specialties and save to a CSV open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(), \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",                   \"Student in an Organized Health Care Education/Training Program\"),   chunk_size = 50000 ) #> [2024-10-06 19:54:36.657799] Function inputs: #> [2024-10-06 19:54:36.658138] Tables: OP_DTL_GNRL_PGYR2020_P01182024, OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:54:36.658427] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:36.658718] Specialties: Allopathic & Osteopathic Physicians|Obstetrics & Gynecology, Student in an Organized Health Care Education/Training Program #> [2024-10-06 19:54:36.659018] Processing table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:38.958986] Table OP_DTL_GNRL_PGYR2020_P01182024 has 5835911 total rows #> [2024-10-06 19:54:38.959226] Processing rows 1 to 50000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:40.421807] Filtered data collected for rows 1 to 50000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:40.627625] Data for rows 1 to 50000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:40.89257] Processing rows 50001 to 100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:44.522848] Filtered data collected for rows 50001 to 100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:44.623869] Data for rows 50001 to 100000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:44.887473] Processing rows 100001 to 150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:48.903924] Filtered data collected for rows 100001 to 150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:48.991548] Data for rows 100001 to 150000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:49.251499] Processing rows 150001 to 200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.684632] Filtered data collected for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.685222] No data to write for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.945733] Processing rows 200001 to 250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.419465] Filtered data collected for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.419875] No data to write for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.68299] Processing rows 250001 to 300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.11445] Filtered data collected for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.114688] No data to write for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.374917] Processing rows 300001 to 350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:55.763748] Filtered data collected for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:55.763973] No data to write for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:56.027827] Processing rows 350001 to 400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.458076] Filtered data collected for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.458312] No data to write for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.719885] Processing rows 400001 to 450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.143698] Filtered data collected for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.143927] No data to write for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.438829] Processing rows 450001 to 500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:00.865342] Filtered data collected for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:00.86556] No data to write for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:01.126091] Processing rows 500001 to 550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.599896] Filtered data collected for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.600117] No data to write for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.86258] Processing rows 550001 to 600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.308192] Filtered data collected for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.308429] No data to write for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.572263] Processing rows 600001 to 650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.00788] Filtered data collected for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.008109] No data to write for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.269521] Processing rows 650001 to 700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.711779] Filtered data collected for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.712027] No data to write for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.973281] Processing rows 700001 to 750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.471128] Filtered data collected for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.471354] No data to write for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.731884] Processing rows 750001 to 800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.208401] Filtered data collected for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.208644] No data to write for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.468094] Processing rows 800001 to 850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:12.909468] Filtered data collected for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:12.909686] No data to write for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:13.169408] Processing rows 850001 to 900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.575704] Filtered data collected for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.575945] No data to write for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.840325] Processing rows 900001 to 950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.311841] Filtered data collected for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.312085] No data to write for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.572346] Processing rows 950001 to 1000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:17.998696] Filtered data collected for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:17.998961] No data to write for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:18.291436] Processing rows 1000001 to 1050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.700554] Filtered data collected for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.700817] No data to write for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.960134] Processing rows 1050001 to 1100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.427145] Filtered data collected for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.427369] No data to write for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.691807] Processing rows 1100001 to 1150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.132789] Filtered data collected for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.133026] No data to write for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.395072] Processing rows 1150001 to 1200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:24.841939] Filtered data collected for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:24.842164] No data to write for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:25.104086] Processing rows 1200001 to 1250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.613225] Filtered data collected for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.613458] No data to write for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.872022] Processing rows 1250001 to 1300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.355621] Filtered data collected for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.355847] No data to write for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.615198] Processing rows 1300001 to 1350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.01858] Filtered data collected for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.018818] No data to write for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.276112] Processing rows 1350001 to 1400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.726144] Filtered data collected for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.726379] No data to write for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.986853] Processing rows 1400001 to 1450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.455851] Filtered data collected for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.456073] No data to write for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.71734] Processing rows 1450001 to 1500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.175378] Filtered data collected for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.175613] No data to write for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.437781] Processing rows 1500001 to 1550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:36.874159] Filtered data collected for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:36.874383] No data to write for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:37.135656] Processing rows 1550001 to 1600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.582873] Filtered data collected for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.583103] No data to write for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.842219] Processing rows 1600001 to 1650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.276898] Filtered data collected for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.277125] No data to write for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.536771] Processing rows 1650001 to 1700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:41.97152] Filtered data collected for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:41.971752] No data to write for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:42.231693] Processing rows 1700001 to 1750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.64521] Filtered data collected for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.645442] No data to write for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.938619] Processing rows 1750001 to 1800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.425223] Filtered data collected for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.425454] No data to write for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.684846] Processing rows 1800001 to 1850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.191397] Filtered data collected for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.191633] No data to write for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.452163] Processing rows 1850001 to 1900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:48.847165] Filtered data collected for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:48.847405] No data to write for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:49.107323] Processing rows 1900001 to 1950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.541264] Filtered data collected for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.541499] No data to write for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.801466] Processing rows 1950001 to 2000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.206171] Filtered data collected for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.206402] No data to write for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.467806] Processing rows 2000001 to 2050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:53.902115] Filtered data collected for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:53.902334] No data to write for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:54.162373] Processing rows 2050001 to 2100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.625052] Filtered data collected for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.625291] No data to write for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.889374] Processing rows 2100001 to 2150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.362016] Filtered data collected for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.362248] No data to write for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.621066] Processing rows 2150001 to 2200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.015406] Filtered data collected for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.015647] No data to write for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.278348] Processing rows 2200001 to 2250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.731394] Filtered data collected for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.731655] No data to write for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.990942] Processing rows 2250001 to 2300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.442929] Filtered data collected for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.443162] No data to write for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.704529] Processing rows 2300001 to 2350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.153256] Filtered data collected for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.153492] No data to write for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.414202] Processing rows 2350001 to 2400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:05.890313] Filtered data collected for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:05.890594] No data to write for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:06.198068] Processing rows 2400001 to 2450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.662549] Filtered data collected for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.662792] No data to write for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.921897] Processing rows 2450001 to 2500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.30842] Filtered data collected for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.308645] No data to write for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.600486] Processing rows 2500001 to 2550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.000366] Filtered data collected for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.000603] No data to write for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.259308] Processing rows 2550001 to 2600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.72769] Filtered data collected for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.727932] No data to write for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.990421] Processing rows 2600001 to 2650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.431199] Filtered data collected for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.431422] No data to write for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.693147] Processing rows 2650001 to 2700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.143099] Filtered data collected for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.143351] No data to write for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.405428] Processing rows 2700001 to 2750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:17.829377] Filtered data collected for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:17.829597] No data to write for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:18.08851] Processing rows 2750001 to 2800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.578764] Filtered data collected for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.579129] No data to write for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.84311] Processing rows 2800001 to 2850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.265371] Filtered data collected for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.265602] No data to write for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.522713] Processing rows 2850001 to 2900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.006424] Filtered data collected for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.006644] No data to write for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.263785] Processing rows 2900001 to 2950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.704706] Filtered data collected for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.704945] No data to write for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.962184] Processing rows 2950001 to 3000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.490486] Filtered data collected for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.490789] No data to write for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.753399] Processing rows 3000001 to 3050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.235956] Filtered data collected for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.236199] No data to write for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.501364] Processing rows 3050001 to 3100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:29.992328] Filtered data collected for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:29.992553] No data to write for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:30.250914] Processing rows 3100001 to 3150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.662256] Filtered data collected for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.662489] No data to write for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.923192] Processing rows 3150001 to 3200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.417629] Filtered data collected for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.417838] No data to write for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.677806] Processing rows 3200001 to 3250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.082577] Filtered data collected for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.082967] No data to write for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.38358] Processing rows 3250001 to 3300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:36.799019] Filtered data collected for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:36.799253] No data to write for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:37.059796] Processing rows 3300001 to 3350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.542105] Filtered data collected for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.542336] No data to write for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.804422] Processing rows 3350001 to 3400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.228064] Filtered data collected for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.228296] No data to write for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.48674] Processing rows 3400001 to 3450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.038292] Filtered data collected for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.038532] No data to write for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.297583] Processing rows 3450001 to 3500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.706204] Filtered data collected for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.706608] No data to write for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.966891] Processing rows 3500001 to 3550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.424101] Filtered data collected for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.424379] No data to write for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.692744] Processing rows 3550001 to 3600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.210792] Filtered data collected for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.211031] No data to write for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.473136] Processing rows 3600001 to 3650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:48.882321] Filtered data collected for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:48.88255] No data to write for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:49.143464] Processing rows 3650001 to 3700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.626803] Filtered data collected for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.627024] No data to write for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.887584] Processing rows 3700001 to 3750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.346517] Filtered data collected for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.346749] No data to write for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.608893] Processing rows 3750001 to 3800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.077097] Filtered data collected for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.077329] No data to write for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.337561] Processing rows 3800001 to 3850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:55.789885] Filtered data collected for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:55.790121] No data to write for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:56.051101] Processing rows 3850001 to 3900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.51004] Filtered data collected for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.51028] No data to write for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.769909] Processing rows 3900001 to 3950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.222468] Filtered data collected for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.222702] No data to write for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.486793] Processing rows 3950001 to 4000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:00.959999] Filtered data collected for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:00.960235] No data to write for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:01.224419] Processing rows 4000001 to 4050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.659759] Filtered data collected for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.659979] No data to write for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.920195] Processing rows 4050001 to 4100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.384705] Filtered data collected for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.384959] No data to write for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.647682] Processing rows 4100001 to 4150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.051038] Filtered data collected for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.051277] No data to write for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.353998] Processing rows 4150001 to 4200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:07.773527] Filtered data collected for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:07.773765] No data to write for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:08.034433] Processing rows 4200001 to 4250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.500248] Filtered data collected for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.501363] No data to write for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.770062] Processing rows 4250001 to 4300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.172485] Filtered data collected for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.172717] No data to write for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.433908] Processing rows 4300001 to 4350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:12.950702] Filtered data collected for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:12.95094] No data to write for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:13.215777] Processing rows 4350001 to 4400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.655557] Filtered data collected for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.655787] No data to write for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.924993] Processing rows 4400001 to 4450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.369574] Filtered data collected for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.369901] No data to write for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.635862] Processing rows 4450001 to 4500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.090189] Filtered data collected for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.090429] No data to write for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.351101] Processing rows 4500001 to 4550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:19.869756] Filtered data collected for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:19.869998] No data to write for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:20.132582] Processing rows 4550001 to 4600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.578934] Filtered data collected for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.579203] No data to write for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.841353] Processing rows 4600001 to 4650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.325049] Filtered data collected for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.325288] No data to write for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.589463] Processing rows 4650001 to 4700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.071945] Filtered data collected for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.072168] No data to write for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.336121] Processing rows 4700001 to 4750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:26.797755] Filtered data collected for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:26.797985] No data to write for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:27.061469] Processing rows 4750001 to 4800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.564097] Filtered data collected for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.56441] No data to write for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.862098] Processing rows 4800001 to 4850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.327391] Filtered data collected for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.327623] No data to write for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.59112] Processing rows 4850001 to 4900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.141984] Filtered data collected for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.142339] No data to write for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.40635] Processing rows 4900001 to 4950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:33.866783] Filtered data collected for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:33.867007] No data to write for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:34.130757] Processing rows 4950001 to 5000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.575404] Filtered data collected for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.575638] No data to write for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.838615] Processing rows 5000001 to 5050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.278492] Filtered data collected for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.278778] No data to write for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.543004] Processing rows 5050001 to 5100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:38.988772] Filtered data collected for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:38.989153] No data to write for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:39.252715] Processing rows 5100001 to 5150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.677503] Filtered data collected for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.677743] No data to write for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.979359] Processing rows 5150001 to 5200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.408938] Filtered data collected for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.409188] No data to write for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.671386] Processing rows 5200001 to 5250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.142124] Filtered data collected for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.142352] No data to write for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.403373] Processing rows 5250001 to 5300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:45.894613] Filtered data collected for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:45.906492] No data to write for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:46.172643] Processing rows 5300001 to 5350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.70332] Filtered data collected for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.703562] No data to write for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.966771] Processing rows 5350001 to 5400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.384105] Filtered data collected for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.38434] No data to write for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.646742] Processing rows 5400001 to 5450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.084234] Filtered data collected for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.08463] No data to write for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.346518] Processing rows 5450001 to 5500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:52.758238] Filtered data collected for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:52.75854] No data to write for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:53.023334] Processing rows 5500001 to 5550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.47876] Filtered data collected for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.478999] No data to write for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.742678] Processing rows 5550001 to 5600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.177177] Filtered data collected for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.17745] No data to write for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.479924] Processing rows 5600001 to 5650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:57.976968] Filtered data collected for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:57.977227] No data to write for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:58.24307] Processing rows 5650001 to 5700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:59.76101] Filtered data collected for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:59.761252] No data to write for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:00.026108] Processing rows 5700001 to 5750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.455962] Filtered data collected for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.456197] No data to write for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.71507] Processing rows 5750001 to 5800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.252373] Filtered data collected for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.252606] No data to write for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.52362] Processing rows 5800001 to 5835911 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:04.993605] Filtered data collected for rows 5800001 to 5835911 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:04.993843] No data to write for rows 5800001 to 5835911 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:05.337747] Processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:05.401432] Table OP_DTL_GNRL_PGYR2021_P01182024 has 11496362 total rows #> [2024-10-06 19:58:05.401686] Processing rows 1 to 50000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:07.582037] Filtered data collected for rows 1 to 50000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:07.800402] Data for rows 1 to 50000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:08.072737] Processing rows 50001 to 100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:12.528184] Filtered data collected for rows 50001 to 100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:12.635881] Data for rows 50001 to 100000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:12.90246] Processing rows 100001 to 150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:17.348593] Filtered data collected for rows 100001 to 150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:17.484077] Data for rows 100001 to 150000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:17.751745] Processing rows 150001 to 200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:24.137358] Filtered data collected for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:24.246593] Data for rows 150001 to 200000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:24.511436] Processing rows 200001 to 250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:27.173458] Filtered data collected for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:27.21521] Data for rows 200001 to 250000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:27.480693] Processing rows 250001 to 300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:29.893727] Filtered data collected for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:29.89397] No data to write for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:30.154751] Processing rows 300001 to 350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.600959] Filtered data collected for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.601204] No data to write for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.868283] Processing rows 350001 to 400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.383161] Filtered data collected for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.383375] No data to write for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.645769] Processing rows 400001 to 450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.265249] Filtered data collected for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.265484] No data to write for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.530297] Processing rows 450001 to 500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.219242] Filtered data collected for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.219488] No data to write for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.488003] Processing rows 500001 to 550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.030537] Filtered data collected for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.030794] No data to write for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.29309] Processing rows 550001 to 600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:46.815922] Filtered data collected for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:46.816153] No data to write for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:47.075839] Processing rows 600001 to 650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.625032] Filtered data collected for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.625268] No data to write for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.884464] Processing rows 650001 to 700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.291134] Filtered data collected for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.291361] No data to write for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.554202] Processing rows 700001 to 750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:54.956599] Filtered data collected for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:54.956834] No data to write for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:55.216258] Processing rows 750001 to 800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.644885] Filtered data collected for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.645125] No data to write for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.905648] Processing rows 800001 to 850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.322526] Filtered data collected for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.322763] No data to write for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.624552] Processing rows 850001 to 900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.074821] Filtered data collected for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.075059] No data to write for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.341261] Processing rows 900001 to 950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:05.749777] Filtered data collected for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:05.750014] No data to write for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:06.011076] Processing rows 950001 to 1000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.454576] Filtered data collected for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.454812] No data to write for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.720477] Processing rows 1000001 to 1050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.127091] Filtered data collected for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.127337] No data to write for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.388277] Processing rows 1050001 to 1100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:13.824014] Filtered data collected for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:13.824264] No data to write for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:14.086795] Processing rows 1100001 to 1150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.509707] Filtered data collected for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.509974] No data to write for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.814213] Processing rows 1150001 to 1200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.291625] Filtered data collected for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.291862] No data to write for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.557403] Processing rows 1200001 to 1250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:21.964573] Filtered data collected for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:21.9648] No data to write for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:22.225079] Processing rows 1250001 to 1300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.725541] Filtered data collected for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.725764] No data to write for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.987933] Processing rows 1300001 to 1350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.408375] Filtered data collected for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.408822] No data to write for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.679193] Processing rows 1350001 to 1400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.18136] Filtered data collected for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.181601] No data to write for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.443418] Processing rows 1400001 to 1450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:32.858113] Filtered data collected for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:32.858364] No data to write for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:33.120106] Processing rows 1450001 to 1500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.626115] Filtered data collected for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.626354] No data to write for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.891241] Processing rows 1500001 to 1550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.581989] Filtered data collected for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.582248] No data to write for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.847952] Processing rows 1550001 to 1600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.365471] Filtered data collected for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.365699] No data to write for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.624983] Processing rows 1600001 to 1650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.590481] Filtered data collected for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.59117] No data to write for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.89358] Processing rows 1650001 to 1700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.687234] Filtered data collected for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.687456] No data to write for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.954553] Processing rows 1700001 to 1750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.597477] Filtered data collected for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.597767] No data to write for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.887989] Processing rows 1750001 to 1800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.481372] Filtered data collected for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.481602] No data to write for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.74289] Processing rows 1800001 to 1850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.187929] Filtered data collected for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.188161] No data to write for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.449193] Processing rows 1850001 to 1900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:58.877325] Filtered data collected for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:58.877564] No data to write for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:59.136961] Processing rows 1900001 to 1950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.617168] Filtered data collected for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.617405] No data to write for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.880214] Processing rows 1950001 to 2000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.297137] Filtered data collected for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.297358] No data to write for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.557476] Processing rows 2000001 to 2050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:06.988654] Filtered data collected for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:06.988889] No data to write for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:07.256729] Processing rows 2050001 to 2100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.712555] Filtered data collected for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.712792] No data to write for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.975483] Processing rows 2100001 to 2150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.471211] Filtered data collected for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.471442] No data to write for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.738418] Processing rows 2150001 to 2200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.202185] Filtered data collected for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.202435] No data to write for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.461956] Processing rows 2200001 to 2250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:17.905893] Filtered data collected for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:17.90613] No data to write for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:18.166549] Processing rows 2250001 to 2300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.560704] Filtered data collected for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.560944] No data to write for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.823409] Processing rows 2300001 to 2350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.337865] Filtered data collected for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.33816] No data to write for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.603997] Processing rows 2350001 to 2400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:25.986081] Filtered data collected for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:25.986314] No data to write for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:26.247822] Processing rows 2400001 to 2450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.67258] Filtered data collected for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.672824] No data to write for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.93457] Processing rows 2450001 to 2500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.335461] Filtered data collected for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.335689] No data to write for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.59521] Processing rows 2500001 to 2550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.039298] Filtered data collected for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.039531] No data to write for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.308888] Processing rows 2550001 to 2600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:36.990506] Filtered data collected for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:36.990732] No data to write for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:37.255638] Processing rows 2600001 to 2650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.032619] Filtered data collected for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.032852] No data to write for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.297722] Processing rows 2650001 to 2700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:42.832194] Filtered data collected for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:42.832421] No data to write for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:43.094306] Processing rows 2700001 to 2750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.607377] Filtered data collected for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.607688] No data to write for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.902113] Processing rows 2750001 to 2800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.385558] Filtered data collected for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.385767] No data to write for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.64796] Processing rows 2800001 to 2850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.144631] Filtered data collected for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.144866] No data to write for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.40666] Processing rows 2850001 to 2900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:53.938791] Filtered data collected for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:53.939023] No data to write for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:54.207244] Processing rows 2900001 to 2950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.661421] Filtered data collected for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.661657] No data to write for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.928862] Processing rows 2950001 to 3000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.420586] Filtered data collected for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.420823] No data to write for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.684029] Processing rows 3000001 to 3050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.182985] Filtered data collected for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.183226] No data to write for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.446375] Processing rows 3050001 to 3100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:04.895276] Filtered data collected for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:04.895515] No data to write for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:05.187097] Processing rows 3100001 to 3150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:07.830431] Filtered data collected for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:07.830658] No data to write for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:08.089275] Processing rows 3150001 to 3200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.56057] Filtered data collected for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.560807] No data to write for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.819742] Processing rows 3200001 to 3250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.350614] Filtered data collected for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.35082] No data to write for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.612518] Processing rows 3250001 to 3300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.032512] Filtered data collected for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.032761] No data to write for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.292694] Processing rows 3300001 to 3350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:18.781331] Filtered data collected for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:18.781577] No data to write for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:19.049154] Processing rows 3350001 to 3400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.530672] Filtered data collected for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.530994] No data to write for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.801162] Processing rows 3400001 to 3450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.358126] Filtered data collected for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.358357] No data to write for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.621932] Processing rows 3450001 to 3500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.077936] Filtered data collected for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.078302] No data to write for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.382312] Processing rows 3500001 to 3550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:29.80266] Filtered data collected for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:29.80302] No data to write for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:30.063239] Processing rows 3550001 to 3600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.586549] Filtered data collected for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.586981] No data to write for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.848767] Processing rows 3600001 to 3650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.298182] Filtered data collected for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.298417] No data to write for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.558455] Processing rows 3650001 to 3700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.14124] Filtered data collected for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.141453] No data to write for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.403082] Processing rows 3700001 to 3750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:40.891586] Filtered data collected for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:40.891829] No data to write for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:41.15072] Processing rows 3750001 to 3800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.649296] Filtered data collected for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.649533] No data to write for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.94663] Processing rows 3800001 to 3850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.557982] Filtered data collected for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.558226] No data to write for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.820975] Processing rows 3850001 to 3900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.479911] Filtered data collected for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.480158] No data to write for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.74559] Processing rows 3900001 to 3950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.38654] Filtered data collected for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.386779] No data to write for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.653228] Processing rows 3950001 to 4000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.177089] Filtered data collected for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.177329] No data to write for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.443219] Processing rows 4000001 to 4050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:57.988245] Filtered data collected for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:57.988482] No data to write for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:58.248976] Processing rows 4050001 to 4100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:00.798233] Filtered data collected for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:00.798461] No data to write for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:01.057806] Processing rows 4100001 to 4150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.534392] Filtered data collected for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.534617] No data to write for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.797488] Processing rows 4150001 to 4200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.189022] Filtered data collected for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.189256] No data to write for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.479333] Processing rows 4200001 to 4250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:08.876655] Filtered data collected for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:08.876909] No data to write for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:09.137885] Processing rows 4250001 to 4300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.600208] Filtered data collected for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.60058] No data to write for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.861303] Processing rows 4300001 to 4350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.28963] Filtered data collected for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.289853] No data to write for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.550174] Processing rows 4350001 to 4400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.121097] Filtered data collected for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.121341] No data to write for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.382886] Processing rows 4400001 to 4450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:19.932045] Filtered data collected for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:19.932278] No data to write for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:20.191363] Processing rows 4450001 to 4500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.687743] Filtered data collected for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.68798] No data to write for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.953497] Processing rows 4500001 to 4550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.507224] Filtered data collected for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.507459] No data to write for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.768815] Processing rows 4550001 to 4600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.389618] Filtered data collected for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.389847] No data to write for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.69197] Processing rows 4600001 to 4650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.235887] Filtered data collected for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.23612] No data to write for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.500246] Processing rows 4650001 to 4700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.078875] Filtered data collected for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.07911] No data to write for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.344137] Processing rows 4700001 to 4750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:36.817146] Filtered data collected for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:36.817385] No data to write for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:37.084546] Processing rows 4750001 to 4800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.581587] Filtered data collected for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.581834] No data to write for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.842481] Processing rows 4800001 to 4850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.281135] Filtered data collected for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.28144] No data to write for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.546987] Processing rows 4850001 to 4900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:44.957227] Filtered data collected for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:44.957548] No data to write for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:45.225846] Processing rows 4900001 to 4950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.643085] Filtered data collected for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.643307] No data to write for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.943909] Processing rows 4950001 to 5000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.302424] Filtered data collected for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.302648] No data to write for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.5631] Processing rows 5000001 to 5050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:52.971501] Filtered data collected for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:52.971722] No data to write for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:53.231383] Processing rows 5050001 to 5100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.636243] Filtered data collected for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.636464] No data to write for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.89676] Processing rows 5100001 to 5150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.326498] Filtered data collected for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.326735] No data to write for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.589203] Processing rows 5150001 to 5200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.061394] Filtered data collected for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.061623] No data to write for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.322632] Processing rows 5200001 to 5250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:03.787833] Filtered data collected for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:03.788056] No data to write for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:04.054421] Processing rows 5250001 to 5300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.442828] Filtered data collected for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.443064] No data to write for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.733789] Processing rows 5300001 to 5350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.18226] Filtered data collected for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.182485] No data to write for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.443307] Processing rows 5350001 to 5400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:11.842539] Filtered data collected for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:11.842779] No data to write for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:12.107161] Processing rows 5400001 to 5450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.664536] Filtered data collected for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.664918] No data to write for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.934234] Processing rows 5450001 to 5500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.498384] Filtered data collected for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.498619] No data to write for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.763785] Processing rows 5500001 to 5550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.294787] Filtered data collected for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.295022] No data to write for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.561449] Processing rows 5550001 to 5600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.06862] Filtered data collected for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.068848] No data to write for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.329029] Processing rows 5600001 to 5650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.727765] Filtered data collected for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.728001] No data to write for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.98832] Processing rows 5650001 to 5700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.529168] Filtered data collected for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.529397] No data to write for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.788849] Processing rows 5700001 to 5750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.180571] Filtered data collected for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.180792] No data to write for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.442181] Processing rows 5750001 to 5800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:33.931121] Filtered data collected for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:33.931351] No data to write for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:34.191055] Processing rows 5800001 to 5850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.724481] Filtered data collected for rows 5800001 to 5850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.724706] No data to write for rows 5800001 to 5850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.987146] Processing rows 5850001 to 5900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.571637] Filtered data collected for rows 5850001 to 5900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.571876] No data to write for rows 5850001 to 5900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.835622] Processing rows 5900001 to 5950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.63346] Filtered data collected for rows 5900001 to 5950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.633713] No data to write for rows 5900001 to 5950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.898937] Processing rows 5950001 to 6000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.39422] Filtered data collected for rows 5950001 to 6000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.394472] No data to write for rows 5950001 to 6000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.653195] Processing rows 6000001 to 6050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.161039] Filtered data collected for rows 6000001 to 6050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.161291] No data to write for rows 6000001 to 6050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.453952] Processing rows 6050001 to 6100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:50.825356] Filtered data collected for rows 6050001 to 6100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:50.82558] No data to write for rows 6050001 to 6100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:51.466098] Processing rows 6100001 to 6150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:53.901961] Filtered data collected for rows 6100001 to 6150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:53.902193] No data to write for rows 6100001 to 6150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:54.158666] Processing rows 6150001 to 6200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.384405] Filtered data collected for rows 6150001 to 6200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.384677] No data to write for rows 6150001 to 6200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.642812] Processing rows 6200001 to 6250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:58.853581] Filtered data collected for rows 6200001 to 6250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:58.853807] No data to write for rows 6200001 to 6250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:59.111211] Processing rows 6250001 to 6300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.326167] Filtered data collected for rows 6250001 to 6300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.326397] No data to write for rows 6250001 to 6300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.585667] Processing rows 6300001 to 6350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:03.834212] Filtered data collected for rows 6300001 to 6350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:03.834444] No data to write for rows 6300001 to 6350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:04.089694] Processing rows 6350001 to 6400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.353563] Filtered data collected for rows 6350001 to 6400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.353797] No data to write for rows 6350001 to 6400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.612699] Processing rows 6400001 to 6450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:08.83107] Filtered data collected for rows 6400001 to 6450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:08.831307] No data to write for rows 6400001 to 6450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:09.083737] Processing rows 6450001 to 6500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.393956] Filtered data collected for rows 6450001 to 6500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.395072] No data to write for rows 6450001 to 6500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.653569] Processing rows 6500001 to 6550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.448646] Filtered data collected for rows 6500001 to 6550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.449474] No data to write for rows 6500001 to 6550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.837728] Processing rows 6550001 to 6600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.498041] Filtered data collected for rows 6550001 to 6600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.498396] No data to write for rows 6550001 to 6600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.811791] Processing rows 6600001 to 6650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.676279] Filtered data collected for rows 6600001 to 6650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.677482] No data to write for rows 6600001 to 6650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.99056] Processing rows 6650001 to 6700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:52.794072] Filtered data collected for rows 6650001 to 6700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:52.794396] No data to write for rows 6650001 to 6700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:53.165429] Processing rows 6700001 to 6750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:57.852296] Filtered data collected for rows 6700001 to 6750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:57.852637] No data to write for rows 6700001 to 6750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:58.2417] Processing rows 6750001 to 6800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.247403] Filtered data collected for rows 6750001 to 6800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.24777] No data to write for rows 6750001 to 6800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.597244] Processing rows 6800001 to 6850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.06223] Filtered data collected for rows 6800001 to 6850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.062531] No data to write for rows 6800001 to 6850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.431819] Processing rows 6850001 to 6900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:10.793265] Filtered data collected for rows 6850001 to 6900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:10.793545] No data to write for rows 6850001 to 6900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:11.157301] Processing rows 6900001 to 6950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:14.904761] Filtered data collected for rows 6900001 to 6950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:14.90498] No data to write for rows 6900001 to 6950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:15.198621] Processing rows 6950001 to 7000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.501108] Filtered data collected for rows 6950001 to 7000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.501356] No data to write for rows 6950001 to 7000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.789321] Processing rows 7000001 to 7050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.106136] Filtered data collected for rows 7000001 to 7050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.106372] No data to write for rows 7000001 to 7050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.380147] Processing rows 7050001 to 7100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.453639] Filtered data collected for rows 7050001 to 7100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.457893] No data to write for rows 7050001 to 7100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.740413] Processing rows 7100001 to 7150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:28.816216] Filtered data collected for rows 7100001 to 7150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:28.816438] No data to write for rows 7100001 to 7150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:29.118048] Processing rows 7150001 to 7200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.167528] Filtered data collected for rows 7150001 to 7200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.167792] No data to write for rows 7150001 to 7200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.443844] Processing rows 7200001 to 7250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.705371] Filtered data collected for rows 7200001 to 7250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.705609] No data to write for rows 7200001 to 7250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.975689] Processing rows 7250001 to 7300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.042345] Filtered data collected for rows 7250001 to 7300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.04268] No data to write for rows 7250001 to 7300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.328029] Processing rows 7300001 to 7350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.304096] Filtered data collected for rows 7300001 to 7350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.304307] No data to write for rows 7300001 to 7350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.582214] Processing rows 7350001 to 7400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.485503] Filtered data collected for rows 7350001 to 7400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.485856] No data to write for rows 7350001 to 7400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.751265] Processing rows 7400001 to 7450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.678113] Filtered data collected for rows 7400001 to 7450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.678328] No data to write for rows 7400001 to 7450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.948502] Processing rows 7450001 to 7500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.641464] Filtered data collected for rows 7450001 to 7500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.641776] No data to write for rows 7450001 to 7500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.918849] Processing rows 7500001 to 7550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:54.80162] Filtered data collected for rows 7500001 to 7550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:54.801847] No data to write for rows 7500001 to 7550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:55.061429] Processing rows 7550001 to 7600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:57.841741] Filtered data collected for rows 7550001 to 7600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:57.842019] No data to write for rows 7550001 to 7600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:58.106597] Processing rows 7600001 to 7650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:00.761695] Filtered data collected for rows 7600001 to 7650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:00.761913] No data to write for rows 7600001 to 7650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:01.022523] Processing rows 7650001 to 7700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.731331] Filtered data collected for rows 7650001 to 7700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.731605] No data to write for rows 7650001 to 7700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.997652] Processing rows 7700001 to 7750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.664577] Filtered data collected for rows 7700001 to 7750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.664802] No data to write for rows 7700001 to 7750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.926422] Processing rows 7750001 to 7800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.660168] Filtered data collected for rows 7750001 to 7800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.660447] No data to write for rows 7750001 to 7800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.922848] Processing rows 7800001 to 7850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.521521] Filtered data collected for rows 7800001 to 7850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.521738] No data to write for rows 7800001 to 7850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.82235] Processing rows 7850001 to 7900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.567262] Filtered data collected for rows 7850001 to 7900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.567483] No data to write for rows 7850001 to 7900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.828017] Processing rows 7900001 to 7950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.619345] Filtered data collected for rows 7900001 to 7950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.61957] No data to write for rows 7900001 to 7950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.885634] Processing rows 7950001 to 8000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.593661] Filtered data collected for rows 7950001 to 8000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.594] No data to write for rows 7950001 to 8000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.853186] Processing rows 8000001 to 8050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.475005] Filtered data collected for rows 8000001 to 8050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.475222] No data to write for rows 8000001 to 8050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.737007] Processing rows 8050001 to 8100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.473482] Filtered data collected for rows 8050001 to 8100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.473729] No data to write for rows 8050001 to 8100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.735858] Processing rows 8100001 to 8150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.239484] Filtered data collected for rows 8100001 to 8150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.239717] No data to write for rows 8100001 to 8150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.494137] Processing rows 8150001 to 8200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.105973] Filtered data collected for rows 8150001 to 8200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.106207] No data to write for rows 8150001 to 8200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.364896] Processing rows 8200001 to 8250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:35.870011] Filtered data collected for rows 8200001 to 8250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:35.87025] No data to write for rows 8200001 to 8250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:36.127617] Processing rows 8250001 to 8300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:38.738933] Filtered data collected for rows 8250001 to 8300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:38.739149] No data to write for rows 8250001 to 8300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:39.015261] Processing rows 8300001 to 8350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.069889] Filtered data collected for rows 8300001 to 8350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.070111] No data to write for rows 8300001 to 8350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.325792] Processing rows 8350001 to 8400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:44.844419] Filtered data collected for rows 8350001 to 8400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:44.8447] No data to write for rows 8350001 to 8400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:45.100586] Processing rows 8400001 to 8450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.613801] Filtered data collected for rows 8400001 to 8450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.614016] No data to write for rows 8400001 to 8450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.86877] Processing rows 8450001 to 8500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.415857] Filtered data collected for rows 8450001 to 8500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.416076] No data to write for rows 8450001 to 8500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.671575] Processing rows 8500001 to 8550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.177248] Filtered data collected for rows 8500001 to 8550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.177514] No data to write for rows 8500001 to 8550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.435972] Processing rows 8550001 to 8600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:55.937343] Filtered data collected for rows 8550001 to 8600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:55.937631] No data to write for rows 8550001 to 8600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:56.19927] Processing rows 8600001 to 8650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:58.910074] Filtered data collected for rows 8600001 to 8650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:58.910305] No data to write for rows 8600001 to 8650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:59.165329] Processing rows 8650001 to 8700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:01.743389] Filtered data collected for rows 8650001 to 8700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:01.744075] No data to write for rows 8650001 to 8700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:02.017748] Processing rows 8700001 to 8750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.63118] Filtered data collected for rows 8700001 to 8750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.631424] No data to write for rows 8700001 to 8750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.889258] Processing rows 8750001 to 8800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.404293] Filtered data collected for rows 8750001 to 8800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.404526] No data to write for rows 8750001 to 8800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.661279] Processing rows 8800001 to 8850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.110486] Filtered data collected for rows 8800001 to 8850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.1107] No data to write for rows 8800001 to 8850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.364493] Processing rows 8850001 to 8900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:12.903018] Filtered data collected for rows 8850001 to 8900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:12.903237] No data to write for rows 8850001 to 8900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:13.15864] Processing rows 8900001 to 8950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.600643] Filtered data collected for rows 8900001 to 8950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.600862] No data to write for rows 8900001 to 8950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.852644] Processing rows 8950001 to 9000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:18.835178] Filtered data collected for rows 8950001 to 9000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:18.83553] No data to write for rows 8950001 to 9000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:19.093573] Processing rows 9000001 to 9050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.512898] Filtered data collected for rows 9000001 to 9050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.513128] No data to write for rows 9000001 to 9050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.762887] Processing rows 9050001 to 9100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.049797] Filtered data collected for rows 9050001 to 9100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.050015] No data to write for rows 9050001 to 9100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.2987] Processing rows 9100001 to 9150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.585938] Filtered data collected for rows 9100001 to 9150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.58615] No data to write for rows 9100001 to 9150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.837427] Processing rows 9150001 to 9200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.093355] Filtered data collected for rows 9150001 to 9200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.093579] No data to write for rows 9150001 to 9200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.343654] Processing rows 9200001 to 9250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.673651] Filtered data collected for rows 9200001 to 9250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.673867] No data to write for rows 9200001 to 9250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.926033] Processing rows 9250001 to 9300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.217619] Filtered data collected for rows 9250001 to 9300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.217854] No data to write for rows 9250001 to 9300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.472224] Processing rows 9300001 to 9350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:25.638288] Filtered data collected for rows 9300001 to 9350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:25.638607] No data to write for rows 9300001 to 9350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:26.108764] Processing rows 9350001 to 9400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.355573] Filtered data collected for rows 9350001 to 9400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.355857] No data to write for rows 9350001 to 9400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.74703] Processing rows 9400001 to 9450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.311148] Filtered data collected for rows 9400001 to 9450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.31143] No data to write for rows 9400001 to 9450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.716459] Processing rows 9450001 to 9500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:41.888692] Filtered data collected for rows 9450001 to 9500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:41.889001] No data to write for rows 9450001 to 9500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:42.187112] Processing rows 9500001 to 9550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.418082] Filtered data collected for rows 9500001 to 9550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.418342] No data to write for rows 9500001 to 9550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.781777] Processing rows 9550001 to 9600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.179293] Filtered data collected for rows 9550001 to 9600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.179635] No data to write for rows 9550001 to 9600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.581625] Processing rows 9600001 to 9650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.494557] Filtered data collected for rows 9600001 to 9650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.494856] No data to write for rows 9600001 to 9650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.806106] Processing rows 9650001 to 9700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:02.958354] Filtered data collected for rows 9650001 to 9700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:02.95868] No data to write for rows 9650001 to 9700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:03.352408] Processing rows 9700001 to 9750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.134268] Filtered data collected for rows 9700001 to 9750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.134676] No data to write for rows 9700001 to 9750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.432275] Processing rows 9750001 to 9800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:10.718212] Filtered data collected for rows 9750001 to 9800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:10.718474] No data to write for rows 9750001 to 9800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:11.181314] Processing rows 9800001 to 9850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:14.79233] Filtered data collected for rows 9800001 to 9850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:14.792547] No data to write for rows 9800001 to 9850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:15.078969] Processing rows 9850001 to 9900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.56669] Filtered data collected for rows 9850001 to 9900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.566982] No data to write for rows 9850001 to 9900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.968327] Processing rows 9900001 to 9950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:21.986006] Filtered data collected for rows 9900001 to 9950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:21.986236] No data to write for rows 9900001 to 9950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:22.253652] Processing rows 9950001 to 10000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.214574] Filtered data collected for rows 9950001 to 10000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.214858] No data to write for rows 9950001 to 10000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.501288] Processing rows 10000001 to 10050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.292372] Filtered data collected for rows 10000001 to 10050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.292595] No data to write for rows 10000001 to 10050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.579205] Processing rows 10050001 to 10100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:31.795964] Filtered data collected for rows 10050001 to 10100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:31.796287] No data to write for rows 10050001 to 10100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:32.08214] Processing rows 10100001 to 10150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.308541] Filtered data collected for rows 10100001 to 10150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.309336] No data to write for rows 10100001 to 10150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.610509] Processing rows 10150001 to 10200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:41.729988] Filtered data collected for rows 10150001 to 10200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:41.731117] No data to write for rows 10150001 to 10200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:42.083177] Processing rows 10200001 to 10250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:45.653445] Filtered data collected for rows 10200001 to 10250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:45.655502] No data to write for rows 10200001 to 10250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:46.18825] Processing rows 10250001 to 10300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.018669] Filtered data collected for rows 10250001 to 10300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.018905] No data to write for rows 10250001 to 10300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.321174] Processing rows 10300001 to 10350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.720529] Filtered data collected for rows 10300001 to 10350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.720737] No data to write for rows 10300001 to 10350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.998903] Processing rows 10350001 to 10400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.259148] Filtered data collected for rows 10350001 to 10400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.259371] No data to write for rows 10350001 to 10400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.533853] Processing rows 10400001 to 10450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.45627] Filtered data collected for rows 10400001 to 10450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.456564] No data to write for rows 10400001 to 10450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.782147] Processing rows 10450001 to 10500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.077958] Filtered data collected for rows 10450001 to 10500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.078202] No data to write for rows 10450001 to 10500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.379858] Processing rows 10500001 to 10550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.189278] Filtered data collected for rows 10500001 to 10550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.189499] No data to write for rows 10500001 to 10550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.459211] Processing rows 10550001 to 10600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:12.865001] Filtered data collected for rows 10550001 to 10600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:12.865273] No data to write for rows 10550001 to 10600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:13.131536] Processing rows 10600001 to 10650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.003379] Filtered data collected for rows 10600001 to 10650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.003594] No data to write for rows 10600001 to 10650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.34437] Processing rows 10650001 to 10700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.038427] Filtered data collected for rows 10650001 to 10700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.038765] No data to write for rows 10650001 to 10700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.405102] Processing rows 10700001 to 10750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.535904] Filtered data collected for rows 10700001 to 10750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.536271] No data to write for rows 10700001 to 10750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.860672] Processing rows 10750001 to 10800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.349281] Filtered data collected for rows 10750001 to 10800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.349517] No data to write for rows 10750001 to 10800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.630375] Processing rows 10800001 to 10850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:33.792253] Filtered data collected for rows 10800001 to 10850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:33.846302] No data to write for rows 10800001 to 10850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:34.141972] Processing rows 10850001 to 10900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:36.807315] Filtered data collected for rows 10850001 to 10900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:36.807541] No data to write for rows 10850001 to 10900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:37.082464] Processing rows 10900001 to 10950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.560642] Filtered data collected for rows 10900001 to 10950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.561178] No data to write for rows 10900001 to 10950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.883208] Processing rows 10950001 to 11000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.403303] Filtered data collected for rows 10950001 to 11000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.403534] No data to write for rows 10950001 to 11000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.695449] Processing rows 11000001 to 11050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.709253] Filtered data collected for rows 11000001 to 11050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.70955] No data to write for rows 11000001 to 11050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.975699] Processing rows 11050001 to 11100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:50.753806] Filtered data collected for rows 11050001 to 11100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:50.754165] No data to write for rows 11050001 to 11100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:51.058473] Processing rows 11100001 to 11150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.047721] Filtered data collected for rows 11100001 to 11150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.048001] No data to write for rows 11100001 to 11150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.35854] Processing rows 11150001 to 11200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.510817] Filtered data collected for rows 11150001 to 11200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.511082] No data to write for rows 11150001 to 11200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.828075] Processing rows 11200001 to 11250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:01.891811] Filtered data collected for rows 11200001 to 11250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:01.892053] No data to write for rows 11200001 to 11250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:02.165716] Processing rows 11250001 to 11300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.724306] Filtered data collected for rows 11250001 to 11300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.724526] No data to write for rows 11250001 to 11300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.980146] Processing rows 11300001 to 11350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.284282] Filtered data collected for rows 11300001 to 11350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.284544] No data to write for rows 11300001 to 11350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.539788] Processing rows 11350001 to 11400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:10.695716] Filtered data collected for rows 11350001 to 11400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:10.695963] No data to write for rows 11350001 to 11400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:11.00426] Processing rows 11400001 to 11450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.588531] Filtered data collected for rows 11400001 to 11450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.588752] No data to write for rows 11400001 to 11450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.912262] Processing rows 11450001 to 11496362 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.404922] Filtered data collected for rows 11450001 to 11496362 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.405148] No data to write for rows 11450001 to 11496362 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.763311] All tables processed. #> [2024-10-07 07:35:18.900142] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-07 07:35:18.903189] The next function needed is `open_payments_collect_and_convert` #> $OP_DTL_GNRL_PGYR2020_P01182024 #> [1] \"Processed OP_DTL_GNRL_PGYR2020_P01182024\" #>  #> $OP_DTL_GNRL_PGYR2021_P01182024 #> [1] \"Processed OP_DTL_GNRL_PGYR2021_P01182024\" #>   # Example 2: Process multiple years of Open Payments data and filter by multiple OB-GYN specialties open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(), \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",                   \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine\"),   chunk_size = 100000 ) #> [2024-10-07 07:35:18.937755] Function inputs: #> [2024-10-07 07:35:18.938268] Tables: OP_DTL_GNRL_PGYR2022_P01182024, OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> [2024-10-07 07:35:18.938529] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv #> [2024-10-07 07:35:18.938699] Specialties: Allopathic & Osteopathic Physicians|Obstetrics & Gynecology, Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine #> Warning: cannot create dir '/Volumes/Video Projects Muffly 1', reason 'Permission denied' #> [2024-10-07 07:35:18.941362] Processing table: OP_DTL_GNRL_PGYR2022_P01182024 #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.054531] Error processing table: OP_DTL_GNRL_PGYR2022_P01182024 #> [2024-10-07 07:35:19.054712] Error message: cannot open the connection #> [2024-10-07 07:35:19.055706] Processing table: OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> Warning: restarting interrupted promise evaluation #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.107403] Error processing table: OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> [2024-10-07 07:35:19.107644] Error message: cannot open the connection #> [2024-10-07 07:35:19.108283] All tables processed. #> [2024-10-07 07:35:19.225474] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv #> [2024-10-07 07:35:19.234962] The next function needed is `open_payments_collect_and_convert` #> list()  # Example 3: Process data with default connection and specialty filtering open_payments_specialty_cleaning(   table_names = c(\"OP_DTL_GNRL_PGYR2021_P01182024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Gynecologic Oncology\",                   \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology\") ) #> [2024-10-07 07:35:19.237773] Function inputs: #> [2024-10-07 07:35:19.258656] Tables: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:19.259325] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv #> [2024-10-07 07:35:19.26026] Specialties: Allopathic & Osteopathic Physicians|Gynecologic Oncology, Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology #> Warning: cannot create dir '/Volumes/Video Projects Muffly 1', reason 'Permission denied' #> [2024-10-07 07:35:19.262182] Processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.29478] Error processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:19.295025] Error message: cannot open the connection #> [2024-10-07 07:35:19.295195] All tables processed. #> [2024-10-07 07:35:19.388607] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv #> [2024-10-07 07:35:19.389133] The next function needed is `open_payments_collect_and_convert` #> list()"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"phase0_city_state_assign_scenarios function designed assign cases professionals based specialty location (city state). function particularly useful managing scenarios professionals, physicians healthcare workers, need assigned cases administrative analytical purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"","code":"phase0_city_state_assign_scenarios(   data,   generalist = \"General Dermatology\",   specialty = \"Pediatric Dermatology\",   case_names = c(\"Case Alpha\", \"Case Beta\", \"Case Gamma\"),   output_csv_path = \"Lizzy/data/city_state_assign_scenarios.csv\",   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. generalist character string specifying specialty name generalists. Default \"Generalist\". specialty character string specifying specialty name specialists. Default \"Specialist\". case_names character vector case names assign. Default c(\"Alpha\", \"Beta\", \"Gamma\"). output_csv_path character string specifying file path save output CSV. Default \"output/city_state_assign_scenarios.csv\". seed optional integer value set random seed reproducibility. Default NULL (seed set).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"data frame assigned cases.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"Generalists vs. Specialists: function differentiates generalists specialists, assigning cases accordingly. CSV Output: final output, including case assignments professional, saved CSV file using write_output_csv function.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"use-cases-","dir":"Reference","previous_headings":"","what":"Use Cases:","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"Healthcare Assignment: Assigning different types cases healthcare professionals based specialties cities/states practice. Research Studies: Managing scenarios research studies professionals need randomly assigned cases.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_assign_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"","code":"# Example 1: Using default parameters data <- data.frame(   city = c(\"CityA\", \"CityA\", \"CityB\", \"CityB\"),   state_code = c(\"State1\", \"State1\", \"State2\", \"State2\"),   specialty_primary = c(\"Generalist\", \"Specialist\", \"Generalist\", \"Specialist\"),   stringsAsFactors = FALSE ) result <- city_state_assign_scenarios(data) #> Error in city_state_assign_scenarios(data): could not find function \"city_state_assign_scenarios\" print(result) #> Error: object 'result' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_check_specialty_generalist_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"function checks city-state combination required number generalists specialists. logs inputs, transformations, outputs, returns two data frames: one failing city-state-specialty combinations one successful combinations. Optionally, results can saved CSV files.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"","code":"phase0_city_state_check_specialty_generalist_counts(   data,   min_generalists,   min_specialists,   generalist_name = \"General Dermatology\",   specialist_name = \"Pediatric Dermatology\",   failing_csv_path = NULL,   successful_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. min_generalists integer specifying minimum number generalists required per city-state combination. min_specialists integer specifying minimum number specialists required per city-state combination. generalist_name string specifying specialty name generalists. Default \"General Dermatology\". specialist_name string specifying specialty name specialists. Default \"Pediatric Dermatology\". failing_csv_path optional string specifying file path save failing combinations CSV. Default NULL (file saved). successful_csv_path optional string specifying file path save successful combinations CSV. Default NULL (file saved).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"list containing two data frames: failing_combinations successful_combinations.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"Generalists vs. Specialists: can specify names generalists specialists, function checks city-state combination required number . Logging: Extensive logging ensures inputs, transformations, results tracked. CSV Output: Optionally, function writes failing successful city-state-specialty combinations separate CSV files. Summary Logging: summary min_generalists, min_specialists, results logged end.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_sample_specialists.html","id":null,"dir":"Reference","previous_headings":"","what":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"function samples specialists generalists given dataset based city-state combinations. allows sampling three types specialists generalists customizable sample sizes . results can saved CSV file returned dataframe. function samples specialists generalists given dataset based city-state combinations. allows sampling three types specialists generalists customizable sample sizes . results can saved CSV file returned dataframe.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_sample_specialists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"","code":"phase0_city_state_sample_specialists(   data,   generalist = \"General Dermatology\",   specialist1 = \"Pediatric Dermatology\",   general_sample_size = 4,   specialist1_sample_size = 2,   specialist2 = NULL,   specialist2_sample_size = 0,   specialist3 = NULL,   specialist3_sample_size = 0,   same_phone_number = TRUE,   output_csv_path = NULL,   seed = 1978 )  phase0_city_state_sample_specialists(   data,   generalist = \"General Dermatology\",   specialist1 = \"Pediatric Dermatology\",   general_sample_size = 4,   specialist1_sample_size = 2,   specialist2 = NULL,   specialist2_sample_size = 0,   specialist3 = NULL,   specialist3_sample_size = 0,   same_phone_number = TRUE,   output_csv_path = NULL,   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_sample_specialists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"data dataframe containing specialist information. must columns: city, state_code, specialty_primary, phone_number. generalist character string specifying generalist specialty sample. Default \"General Dermatology\". specialist1 character string specifying first specialist specialty sample. Default \"Pediatric Dermatology\". general_sample_size integer specifying many generalists sample city-state combination. Default 4. specialist1_sample_size integer specifying many first specialists sample city-state combination. Default 1. specialist2 character string specifying second specialist specialty sample. Optional. Default NULL. specialist2_sample_size integer specifying many second specialists sample. Default 0. specialist3 character string specifying third specialist specialty sample. Optional. Default NULL. specialist3_sample_size integer specifying many third specialists sample. Default 0. same_phone_number logical value indicating whether sample generalists specialists phone number (TRUE) different phone numbers (FALSE). Default TRUE. output_csv_path character string specifying path save output CSV. provided, result returned. seed integer setting seed reproducibility. Default 1978.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_sample_specialists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"dataframe containing sampled generalists specialists city-state combination. dataframe containing sampled generalists specialists city-state combination.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_city_state_sample_specialists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"","code":"# Example 1: Basic usage with default generalist and specialist data <- data.frame(   city = rep(c(\"New York\", \"Los Angeles\"), each = 6),   state_code = rep(c(\"NY\", \"CA\"), each = 6),   specialty_primary = c(     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\"   ),   phone_number = rep(c(\"123\", \"456\", \"789\"), 4) ) result <- city_state_sample_specialists(data) #> Error in city_state_sample_specialists(data): could not find function \"city_state_sample_specialists\" print(result) #> Error: object 'result' not found  # Example 1: Basic usage with default generalist and specialist data <- data.frame(   city = rep(c(\"New York\", \"Los Angeles\"), each = 6),   state_code = rep(c(\"NY\", \"CA\"), each = 6),   specialty_primary = c(     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\"   ),   phone_number = rep(c(\"123\", \"456\", \"789\"), 4) ) result <- city_state_sample_specialists(data) #> Error in city_state_sample_specialists(data): could not find function \"city_state_sample_specialists\" print(result) #> Error: object 'result' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_clean_npi_entries.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean NPI Entries Function — phase0_clean_npi_entries","title":"Clean NPI Entries Function — phase0_clean_npi_entries","text":"function cleans NPI search results normalizing credentials, applying filters taxonomies, summarizing entries NPI. includes console logging key steps.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_clean_npi_entries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean NPI Entries Function — phase0_clean_npi_entries","text":"","code":"phase0_clean_npi_entries(   npi_entries,   basic_credentials = c(\"MD\", \"DO\"),   taxonomy_filter = \"Obstetrics & Gynecology\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_clean_npi_entries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean NPI Entries Function — phase0_clean_npi_entries","text":"npi_entries dataframe containing NPI search results. basic_credentials character vector credentials filter (default c(\"MD\", \"\")). taxonomy_filter string filtering taxonomies (default \"Obstetrics & Gynecology\"). verbose logical value controls whether detailed logging shown process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_clean_npi_entries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean NPI Entries Function — phase0_clean_npi_entries","text":"cleaned dataframe summarized NPI entries.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_clean_npi_entries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean NPI Entries Function — phase0_clean_npi_entries","text":"","code":"# Example 1: Basic cleaning of NPI entries with default parameters clean_npi_entries(npi_results) #> Error in clean_npi_entries(npi_results): could not find function \"clean_npi_entries\"  # Example 2: Cleaning NPI entries, filtering for a specific taxonomy clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\") #> Error in clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\"): could not find function \"clean_npi_entries\"  # Example 3: Cleaning NPI entries, specifying different credentials clean_npi_entries(npi_results, basic_credentials = c(\"PA\", \"NP\")) #> Error in clean_npi_entries(npi_results, basic_credentials = c(\"PA\", \"NP\")): could not find function \"clean_npi_entries\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_coalesce.html","id":null,"dir":"Reference","previous_headings":"","what":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","title":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","text":"function combines values multiple columns new column. column_coalesce_names provided, columns used. Otherwise, columns selected based column_name_pattern. resulting column contains mode (frequent value) selected columns row, excluding NA values. user can optionally delete columns used coalescing operation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_coalesce.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","text":"","code":"phase0_coalesce(   input_dataframe,   column_name_pattern = NULL,   column_coalesce_names = NULL,   new_column_name = \"coalesced_column\",   delete_columns = FALSE,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_coalesce.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","text":"input_dataframe data.frame tibble process. column_name_pattern character string used identify columns coalesce. Ignored column_coalesce_names provided. column_coalesce_names character vector specifying exact column names coalesce. Overrides column_name_pattern provided. new_column_name character string specifying name new column store coalesced values. Defaults \"coalesced_column\". delete_columns logical flag indicating whether columns used coalescing removed dataframe. Defaults FALSE. verbose logical flag indicating whether detailed logs printed execution. Defaults FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_coalesce.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","text":"data.frame tibble new column added. delete_columns TRUE, columns used coalescing removed dataframe, except new_column_name.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_coalesce.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Coalesce Columns by Mode Excluding NAs — phase0_coalesce","text":"","code":"# Example 1: Coalesce using a pattern and keep original columns example_df <- tibble::tibble(   address_state = c(\"NY\", NA, \"CA\", NA),   license_state = c(NA, \"TX\", \"CA\", NA),   profile_state = c(\"NY\", \"TX\", NA, NA) )  result_df <- phase0_coalesce(   input_dataframe = example_df,   column_name_pattern = \"state\",   verbose = TRUE ) #> Starting phase0_coalesce... #> Input dataframe has 4 rows and 3 columns.Coalescing columns: address_state, license_state, profile_state...Added new column 'coalesced_column' to the dataframe.Updated dataframe has 4 rows and 4 columns. print(result_df) #> # A tibble: 4 × 4 #>   address_state license_state profile_state coalesced_column #>   <chr>         <chr>         <chr>         <chr>            #> 1 NY            NA            NY            NY               #> 2 NA            TX            TX            TX               #> 3 CA            CA            NA            CA               #> 4 NA            NA            NA            NA                # Example 2: Coalesce and delete original columns delete_columns_df <- tibble::tibble(   col1 = c(\"A\", \"B\", NA, \"C\"),   col2 = c(\"X\", \"B\", NA, \"C\"),   col3 = c(NA, \"B\", NA, NA) )  result_delete <- phase0_coalesce(   input_dataframe = delete_columns_df,   column_coalesce_names = c(\"col1\", \"col2\"),   new_column_name = \"combined_col\",   delete_columns = TRUE,   verbose = TRUE ) #> Starting phase0_coalesce... #> Input dataframe has 4 rows and 3 columns.Coalescing columns: col1, col2...Deleted columns: col1, col2Added new column 'combined_col' to the dataframe.Updated dataframe has 4 rows and 2 columns. print(result_delete) #> # A tibble: 4 × 2 #>   col3  combined_col #>   <chr> <chr>        #> 1 NA    A            #> 2 B     B            #> 3 NA    NA           #> 4 NA    C"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_convert_state_abbreviations.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","title":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","text":"function takes vector state abbreviations returns corresponding full state names, including District Columbia (DC) Puerto Rico (PR). function logs input, output, transformations via console. function can handle standard state abbreviations well special cases DC PR.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_convert_state_abbreviations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","text":"","code":"phase0_convert_state_abbreviations(state_abbr)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_convert_state_abbreviations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","text":"state_abbr character vector containing state abbreviations. can include two-letter state codes like \"NY\" New York, \"CA\" California, special cases \"DC\" District Columbia \"PR\" Puerto Rico. Abbreviations must valid state abbreviations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_convert_state_abbreviations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","text":"character vector full state names corresponding input abbreviations. Invalid abbreviations returned NA.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_convert_state_abbreviations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert State Abbreviations to Full State Names with Logging — phase0_convert_state_abbreviations","text":"","code":"# Example 1: A simple case with a mix of state abbreviations state_abbr <- c(\"NY\", \"CA\", \"TX\", \"DC\", \"PR\") convert_state_abbreviations(state_abbr) #> Error in convert_state_abbreviations(state_abbr): could not find function \"convert_state_abbreviations\" # Expected output: # \"New York\" \"California\" \"Texas\" \"District of Columbia\" \"Puerto Rico\"  # Example 2: A vector of state abbreviations including only valid ones state_abbr <- c(\"TX\", \"FL\", \"WI\", \"VA\") convert_state_abbreviations(state_abbr) #> Error in convert_state_abbreviations(state_abbr): could not find function \"convert_state_abbreviations\" # Expected output: # \"Texas\" \"Florida\" \"Wisconsin\" \"Virginia\"  # Example 3: A vector of abbreviations with mixed cases and handling unknown abbreviations state_abbr <- c(\"ny\", \"ca\", \"xx\", \"DC\", \"pr\") convert_state_abbreviations(state_abbr) #> Error in convert_state_abbreviations(state_abbr): could not find function \"convert_state_abbreviations\" # Expected output: # \"New York\" \"California\" NA \"District of Columbia\" \"Puerto Rico\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"function adds new column, academic, provided data frame. column populated based presence specific academic terms (\"University\", \"Medical School\", \"Health System\", etc.) specified columns. Rows match found value \"Private Practice\", rows specified columns NA value NA.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"","code":"phase0_create_academic_column(   input_data,   columns_to_search,   academic_keywords = c(\"Medical College\", \"University of\", \"University\", \"Univ\",     \"Children's\", \"Infirmary\", \"Medical School\", \"Medical Center\", \"Children\",     \"Health System\", \"Foundation\", \"Sch of Med\", \"Mayo\", \"UAB\", \"Cancer Ctr\", \"Penn\",     \"College of Medicine\", \"Cancer\", \"Cleveland Clinic\", \"Henry Ford\", \"Yale\", \"Brigham\",     \"Health Sciences Center\", \"SUNY\", \"UCLA\", \"UAMS\", \"Medical Sciences\", \"Cedars\",     \"Stanford\", \"George Washington\", \"Medstar\", \"USF\", \"GME\", \"Emory\", \"Morehouse\",     \"Mercer\", \"Loyola\", \"Duke\"),   private_practice_keywords = c(\"LLC\", \"INC\", \"P.A.\", \"CORPORATION\", \"PLLC\",     \"PARTNERSHIP\", \"LTD\", \"P C\", \"PS\", \"L L P\", \"INCORPORATED\", \"DO\", \"MD\", \"MBA\", \"PA\"),   verbose = FALSE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"input_data data frame containing data process. columns_to_search character vector column names search academic terms. academic_keywords character vector keywords representing academic institutions terms. default includes common academic terms \"University\", \"Medical College\", etc. verbose Logical. TRUE, detailed logging process printed console. Default FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"input data frame new column academic indicating whether facility academic private practice, NA specified columns NA.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"logic behind categorization follows: \"University\": specified columns contain academic terms (case-insensitive) defined search_strings argument, row labeled \"University\". \"Private Practice\": academic terms found specified columns row, row labeled \"Private Practice\". NA: specified columns row contain NA values, academic column row remain NA. function good example searching data within multiple columns assigning value new output column based whether specific terms present. demonstrates multiple columns can examined simultaneously, new categorical variable (academic) derived based keyword matching columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_academic_column.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Academic Column Based on Facility Names — phase0_create_academic_column","text":"","code":"# Example 1: Basic usage with default academic keywords and verbose logging clinician_data <- data.frame(   clinician_info_facility_name = c(\"University of California\", \"Mayo Clinic\",                                    \"Private Clinic\", NA),   addresses_address_1 = c(\"200 Medical Ave\", \"123 Health St\", \"456 Private Rd\", NA) ) df_with_academic <- phase0_create_academic_column(clinician_data,                                            c(\"clinician_info_facility_name\",                                              \"addresses_address_1\"),                                            verbose = TRUE) #> INFO [2025-03-11 06:45:59] Starting function 'phase0_create_academic_column'. #> INFO [2025-03-11 06:45:59] Input data dimensions: 4 rows, 2 columns. #> INFO [2025-03-11 06:45:59] Columns to search in: clinician_info_facility_name, addresses_address_1 #> INFO [2025-03-11 06:45:59] Academic search keywords: Medical College, University of, University, Univ, Children's, Infirmary, Medical School, Medical Center, Children, Health System, Foundation, Sch of Med, Mayo, UAB, Cancer Ctr, Penn, College of Medicine, Cancer, Cleveland Clinic, Henry Ford, Yale, Brigham, Health Sciences Center, SUNY, UCLA, UAMS, Medical Sciences, Cedars, Stanford, George Washington, Medstar, USF, GME, Emory, Morehouse, Mercer, Loyola, Duke #> INFO [2025-03-11 06:45:59] Private practice search keywords: LLC, INC, P.A., CORPORATION, PLLC, PARTNERSHIP, LTD, P C, PS, L L P, INCORPORATED, DO, MD, MBA, PA #> INFO [2025-03-11 06:45:59] Transformation completed. New column 'academic' added to the data frame. #> INFO [2025-03-11 06:45:59] Output data dimensions: 4 rows, 3 columns. print(df_with_academic) #>   clinician_info_facility_name addresses_address_1         academic #> 1     University of California     200 Medical Ave       University #> 2                  Mayo Clinic       123 Health St       University #> 3               Private Clinic      456 Private Rd Private Practice #> 4                         <NA>                <NA>             <NA>  # Example 2: Using custom academic keywords and suppressing verbose logging custom_keywords <- c(\"Harvard\", \"Stanford\", \"Yale\", \"Medical School\") clinician_data <- data.frame(   clinician_info_facility_name = c(\"Harvard Medical School\", \"Private Clinic\"),   addresses_address_1 = c(\"1 Medical Center\", \"123 Private Rd\") ) df_with_academic <- phase0_create_academic_column(clinician_data,                                            c(\"clinician_info_facility_name\",                                              \"addresses_address_1\"),                                            academic_keywords = custom_keywords,                                            verbose = FALSE) #> INFO [2025-03-11 06:45:59] Starting function 'phase0_create_academic_column'. #> INFO [2025-03-11 06:45:59] Input data dimensions: 2 rows, 2 columns. #> INFO [2025-03-11 06:45:59] Columns to search in: clinician_info_facility_name, addresses_address_1 #> INFO [2025-03-11 06:45:59] Academic search keywords: Harvard, Stanford, Yale, Medical School #> INFO [2025-03-11 06:45:59] Private practice search keywords: LLC, INC, P.A., CORPORATION, PLLC, PARTNERSHIP, LTD, P C, PS, L L P, INCORPORATED, DO, MD, MBA, PA #> INFO [2025-03-11 06:45:59] Transformation completed. New column 'academic' added to the data frame. #> INFO [2025-03-11 06:45:59] Output data dimensions: 2 rows, 3 columns. print(df_with_academic) #>   clinician_info_facility_name addresses_address_1         academic #> 1       Harvard Medical School    1 Medical Center       University #> 2               Private Clinic      123 Private Rd Private Practice  # Example 3: Case with all columns as NA, resulting in NA in the academic column clinician_data <- data.frame(   clinician_info_facility_name = c(NA, NA),   addresses_address_1 = c(NA, NA) ) df_with_academic <- phase0_create_academic_column(clinician_data,                                            c(\"clinician_info_facility_name\",                                              \"addresses_address_1\"),                                            verbose = FALSE) #> INFO [2025-03-11 06:45:59] Starting function 'phase0_create_academic_column'. #> INFO [2025-03-11 06:45:59] Input data dimensions: 2 rows, 2 columns. #> INFO [2025-03-11 06:45:59] Columns to search in: clinician_info_facility_name, addresses_address_1 #> INFO [2025-03-11 06:45:59] Academic search keywords: Medical College, University of, University, Univ, Children's, Infirmary, Medical School, Medical Center, Children, Health System, Foundation, Sch of Med, Mayo, UAB, Cancer Ctr, Penn, College of Medicine, Cancer, Cleveland Clinic, Henry Ford, Yale, Brigham, Health Sciences Center, SUNY, UCLA, UAMS, Medical Sciences, Cedars, Stanford, George Washington, Medstar, USF, GME, Emory, Morehouse, Mercer, Loyola, Duke #> INFO [2025-03-11 06:45:59] Private practice search keywords: LLC, INC, P.A., CORPORATION, PLLC, PARTNERSHIP, LTD, P C, PS, L L P, INCORPORATED, DO, MD, MBA, PA #> INFO [2025-03-11 06:45:59] Transformation completed. New column 'academic' added to the data frame. #> INFO [2025-03-11 06:45:59] Output data dimensions: 2 rows, 3 columns. print(df_with_academic) #>   clinician_info_facility_name addresses_address_1 academic #> 1                           NA                  NA     <NA> #> 2                           NA                  NA     <NA>"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_sampled_by_region.html","id":null,"dir":"Reference","previous_headings":"","what":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"function processes taxonomy AAOS data, merging Census Bureau data create sampled dataset grouped region ortho spine. Outputs saved CSV.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_sampled_by_region.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"","code":"phase0_create_sampled_by_region(   taxonomy_and_aaos_tbl,   census_bureau_tbl,   output_csv_path = \"ortho_spine/phase_0/sampled_by_region_ortho_spine.csv\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_sampled_by_region.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"taxonomy_and_aaos_tbl tibble containing taxonomy AAOS data. census_bureau_tbl tibble containing Census Bureau data regional information. output_csv_path file path save sampled dataset. Defaults \"ortho_spine/phase_0/sampled_by_region_ortho_spine.csv\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_sampled_by_region.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"tibble sampled ortho spine records region.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_create_sampled_by_region.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"","code":"# Example 1: Basic usage with default output path sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl, census_bureau_tbl) #> INFO [2025-03-11 06:46:00] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE),         withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             census_bureau_tbl), logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl), .topenv = `<env>`, namespace = \"tyler\"),         do.call(formatter, c(res$params, list(.logcall = substitute(.logcall),             .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl), .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 29L, 37L, 38L, 37L, 29L, 29L, 42L, 43L,         44L, 45L, 46L, 47L, 48L, 48L, 48L, 51L, 51L, 53L, 53L,         55L, 0L, 57L, 57L, 59L, 60L, 59L, 62L, 44L, 0L, 65L,         66L), visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE), namespace = c(\"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"pkgdown\", \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\",         \"purrr\", \"pkgdown\", \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"downlit\", \"evaluate\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \":::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"local\", \"::\", \"::\", \"local\", \"::\", \"local\", \"::\", \"::\",         \"::\", \"local\", NA, \"local\", \":::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"local\", \"::\")), row.names = c(NA, -67L), version = 2L, class = c(\"rlang_trace\",     \"rlib_trace\", \"tbl\", \"data.frame\")), parent = structure(list(        message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 2: Custom output path sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl, census_bureau_tbl,   output_csv_path = \"custom_path/sample.csv\" ) #> INFO [2025-03-11 06:46:00] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE),         withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),         logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),             .topenv = `<env>`, namespace = \"tyler\"), do.call(formatter,             c(res$params, list(.logcall = substitute(.logcall),                 .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),             .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 29L, 37L, 38L, 37L, 29L, 29L, 42L, 43L,         44L, 45L, 46L, 47L, 48L, 48L, 48L, 51L, 51L, 53L, 53L,         55L, 0L, 57L, 57L, 59L, 60L, 59L, 62L, 44L, 0L, 65L,         66L), visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE), namespace = c(\"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"pkgdown\", \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\",         \"purrr\", \"pkgdown\", \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"downlit\", \"evaluate\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \":::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"local\", \"::\", \"::\", \"local\", \"::\", \"local\", \"::\", \"::\",         \"::\", \"local\", NA, \"local\", \":::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"local\", \"::\")), row.names = c(NA, -67L), version = 2L, class = c(\"rlang_trace\",     \"rlib_trace\", \"tbl\", \"data.frame\")), parent = structure(list(        message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 3: Using a subset of census_bureau_tbl for faster testing sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,   dplyr::filter(census_bureau_tbl, Region == \"West\"),   output_csv_path = \"test/sample.csv\" ) #> INFO [2025-03-11 06:46:00] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-res-f17f339ae2c5\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpjUGsYk/callr-fun-f17f1d22400c\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE),         withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             dplyr::filter(census_bureau_tbl, Region == \"West\"),             output_csv_path = \"test/sample.csv\"), logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 dplyr::filter(census_bureau_tbl, Region == \"West\"),                 output_csv_path = \"test/sample.csv\"), .topenv = `<env>`,             namespace = \"tyler\"), do.call(formatter, c(res$params,             list(.logcall = substitute(.logcall), .topcall = substitute(.topcall),                 .topenv = .topenv))), `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 dplyr::filter(census_bureau_tbl, Region == \"West\"),                 output_csv_path = \"test/sample.csv\"), .topenv = `<env>`),         withCallingHandlers(glue::glue(..., .envir = .topenv),             error = function(e) {                args <- paste0(capture.output(str(...)), collapse = \"\\n\")                stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                   args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                   \"\\n\\nPlease consider using another `log_formatter` or \",                   \"`skip_formatter` on strings with curly braces.\"))            }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 29L, 37L, 38L, 37L, 29L, 29L, 42L, 43L,         44L, 45L, 46L, 47L, 48L, 48L, 48L, 51L, 51L, 53L, 53L,         55L, 0L, 57L, 57L, 59L, 60L, 59L, 62L, 44L, 0L, 65L,         66L), visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, FALSE), namespace = c(\"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"pkgdown\", \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\",         \"purrr\", \"pkgdown\", \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"downlit\", \"evaluate\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \":::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"local\", \"::\", \"::\", \"local\", \"::\", \"local\", \"::\", \"::\",         \"::\", \"local\", NA, \"local\", \":::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"local\", \"::\")), row.names = c(NA, -67L), version = 2L, class = c(\"rlang_trace\",     \"rlib_trace\", \"tbl\", \"data.frame\")), parent = structure(list(        message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_genderize_physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Phase 0: Genderize Physicians Data with Logging and Detailed Data Transformation — phase0_genderize_physicians","title":"Phase 0: Genderize Physicians Data with Logging and Detailed Data Transformation — phase0_genderize_physicians","text":"Phase 0: Genderize Physicians Data Logging Detailed Data Transformation","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_genderize_physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phase 0: Genderize Physicians Data with Logging and Detailed Data Transformation — phase0_genderize_physicians","text":"","code":"phase0_genderize_physicians(   physician_data,   first_name_column = \"first\",   last_name_column = \"last\",   save_directory = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_genderize_physicians.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Phase 0: Genderize Physicians Data with Logging and Detailed Data Transformation — phase0_genderize_physicians","text":"physician_data dataframe containing physician data column named 'first_name' genderization. first_name_column Name column containing first names. Default \"first\". last_name_column Name column containing last names. Default \"last\". save_directory Directory save output. Default NULL. verbose Whether show detailed logging. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_geocode.html","id":null,"dir":"Reference","previous_headings":"","what":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","title":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","text":"function geocodes unique addresses dataset using Google Maps API, appending latitude longitude information unique address. provides detailed logging step, including validations, transformations, geocoding operations, saving results.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_geocode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","text":"","code":"phase0_geocode(   address_table,   address_column_names,   api_key = Sys.getenv(\"GOOGLE_MAPS_API_KEY\"),   save_file_path = NULL,   enable_verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_geocode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","text":"address_table data frame containing address-related columns united geocoding. data frame must include columns specified address_column_names. address_column_names character vector specifying column names address_table united form complete address geocoding. columns vector must exist address_table. api_key character string containing Google Maps API key. Defaults value GOOGLE_MAPS_API_KEY .Renviron file. save_file_path Optional. character string specifying path geocoded data saved CSV file. NULL, data saved file. Default NULL. enable_verbose Logical. TRUE, detailed logging printed console execution. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_geocode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","text":"tibble columns: complete_address: Combined address column used geocoding. latitude: Geocoded latitude address. longitude: Geocoded longitude address. original columns address_table.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_geocode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geocode Unique Addresses with Detailed Logging — phase0_geocode","text":"","code":"# Example 1: Basic usage with verbose logging and no file saving addresses <- tibble::tibble(   street = c(\"1600 Amphitheatre Parkway\", \"1 Infinite Loop\"),   city = c(\"Mountain View\", \"Cupertino\"),   state = c(\"CA\", \"CA\"),   zip_code = c(\"94043\", \"95014\") )  geocoded_addresses <- phase0_geocode(   address_table = addresses,   address_column_names = c(\"street\", \"city\", \"state\", \"zip_code\"),   api_key = Sys.getenv(\"GOOGLE_MAPS_API_KEY\"),   enable_verbose = TRUE ) #> INFO [2025-03-11 06:46:01] Initializing phase0_geocode function #> INFO [2025-03-11 06:46:01] Input 'address_table' validated as a data frame. #> INFO [2025-03-11 06:46:01] Input 'address_column_names' validated as a character vector. #> INFO [2025-03-11 06:46:01] API key provided and valid. #> INFO [2025-03-11 06:46:01] Google Maps API key registered successfully. #> INFO [2025-03-11 06:46:01] All specified address columns found in 'address_table'. #> INFO [2025-03-11 06:46:01] Address columns united into a single column named 'complete_address'. #> INFO [2025-03-11 06:46:01] Starting geocoding process. #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:01] Geocoded '1600 Amphitheatre Parkway, Mountain View, CA, 94043': Lat = 37.4212824, Lon = -122.0846093 #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:01] Geocoded '1 Infinite Loop, Cupertino, CA, 95014': Lat = 37.3318598, Lon = -122.0302485 #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:01] Geocoding completed successfully. Latitude and longitude columns added. #> INFO [2025-03-11 06:46:01] phase0_geocode function completed. #> INFO [2025-03-11 06:46:01] The geocoded table contains 2 rows and 7 columns. print(geocoded_addresses) #> # A tibble: 2 × 7 #>   complete_address                street city  state zip_code latitude longitude #>   <chr>                           <chr>  <chr> <chr> <chr>       <dbl>     <dbl> #> 1 1600 Amphitheatre Parkway, Mou… 1600 … Moun… CA    94043        37.4     -122. #> 2 1 Infinite Loop, Cupertino, CA… 1 Inf… Cupe… CA    95014        37.3     -122.  # Example 2: Save geocoded data to a CSV file addresses <- tibble::tibble(   street = c(\"1600 Amphitheatre Parkway\", \"1 Infinite Loop\"),   city = c(\"Mountain View\", \"Cupertino\"),   state = c(\"CA\", \"CA\"),   zip_code = c(\"94043\", \"95014\") )  geocoded_addresses <- phase0_geocode(   address_table = addresses,   address_column_names = c(\"street\", \"city\", \"state\", \"zip_code\"),   api_key = Sys.getenv(\"GOOGLE_MAPS_API_KEY\"),   save_file_path = \"geocoded_addresses.csv\",   enable_verbose = TRUE ) #> INFO [2025-03-11 06:46:01] Initializing phase0_geocode function #> INFO [2025-03-11 06:46:01] Input 'address_table' validated as a data frame. #> INFO [2025-03-11 06:46:01] Input 'address_column_names' validated as a character vector. #> INFO [2025-03-11 06:46:01] API key provided and valid. #> INFO [2025-03-11 06:46:01] Google Maps API key registered successfully. #> INFO [2025-03-11 06:46:01] All specified address columns found in 'address_table'. #> INFO [2025-03-11 06:46:01] Address columns united into a single column named 'complete_address'. #> INFO [2025-03-11 06:46:01] Starting geocoding process. #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:01] Geocoded '1600 Amphitheatre Parkway, Mountain View, CA, 94043': Lat = 37.4212824, Lon = -122.0846093 #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:01] Geocoded '1 Infinite Loop, Cupertino, CA, 95014': Lat = 37.3318598, Lon = -122.0302485 #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> #> INFO [2025-03-11 06:46:02] Geocoding completed successfully. Latitude and longitude columns added. #> INFO [2025-03-11 06:46:02] Saving geocoded table to file: geocoded_addresses.csv #> INFO [2025-03-11 06:46:02] File saved successfully. #> INFO [2025-03-11 06:46:02] phase0_geocode function completed. #> INFO [2025-03-11 06:46:02] The geocoded table contains 2 rows and 7 columns. print(geocoded_addresses) #> # A tibble: 2 × 7 #>   complete_address                street city  state zip_code latitude longitude #>   <chr>                           <chr>  <chr> <chr> <chr>       <dbl>     <dbl> #> 1 1600 Amphitheatre Parkway, Mou… 1600 … Moun… CA    94043        37.4     -122. #> 2 1 Infinite Loop, Cupertino, CA… 1 Inf… Cupe… CA    95014        37.3     -122.  # Example 3: Minimal logging and no file saving addresses <- tibble::tibble(   street = c(\"1600 Amphitheatre Parkway\", \"1 Infinite Loop\"),   city = c(\"Mountain View\", \"Cupertino\"),   state = c(\"CA\", \"CA\"),   zip_code = c(\"94043\", \"95014\") )  geocoded_addresses <- phase0_geocode(   address_table = addresses,   address_column_names = c(\"street\", \"city\", \"state\", \"zip_code\"),   api_key = Sys.getenv(\"GOOGLE_MAPS_API_KEY\"),   enable_verbose = FALSE ) #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA,+94043&key=xxx-wePaD6q0qUOA> #> ℹ <https://maps.googleapis.com/maps/api/geocode/json?address=1+Infinite+Loop,+Cupertino,+CA,+95014&key=xxx-wePaD6q0qUOA> print(geocoded_addresses) #> # A tibble: 2 × 7 #>   complete_address                street city  state zip_code latitude longitude #>   <chr>                           <chr>  <chr> <chr> <chr>       <dbl>     <dbl> #> 1 1600 Amphitheatre Parkway, Mou… 1600 … Moun… CA    94043        37.4     -122. #> 2 1 Infinite Loop, Cupertino, CA… 1 Inf… Cupe… CA    95014        37.3     -122."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_get_facility_affiliations.html","id":null,"dir":"Reference","previous_headings":"","what":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","title":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","text":"Fetches facility affiliation data CMS Provider Data Catalog API. Returns facility affiliations like hospitals, nursing homes, rehab facilities, etc.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_get_facility_affiliations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","text":"","code":"phase0_get_facility_affiliations(   npi_numbers,   batch_size = 10,   url = \"https://data.cms.gov/provider-data/api/1/datastore/query/27ea-46a8/0\",   limit = 100,   sys_sleep = 0.5,   user_agent = \"r-package-cms-query\",   verbose = FALSE,   csv_save_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_get_facility_affiliations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","text":"npi_numbers Character vector NPI numbers query batch_size Integer. Number NPIs query per batch. Default: 10 url Character. API endpoint URL. Default: CMS data catalog API endpoint limit Integer. Maximum records per API call. Default: 100 sys_sleep Numeric. Sleep time API calls seconds. Default: 0.5 user_agent Character. User agent string API calls. Default: \"r-package-cms-query\" verbose Logical. Whether output detailed logs. Default: FALSE csv_save_path Character. Optional path save results CSV. Default: NULL","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_get_facility_affiliations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","text":"tibble containing facility affiliation data \"FAC_\" prefixed columns. Common returned fields include: FAC_npi - National Provider Identifier FAC_ind_pac_id - Individual PAC ID FAC_provider_name - Provider's name FAC_facility_type - Type facility (Hospital, Nursing Home, etc.) FAC_certification_number - Facility certification number","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_get_facility_affiliations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query CMS Facility Affiliations — phase0_get_facility_affiliations","text":"","code":"# Basic usage with minimal parameters fac_results <- get_facility_affiliations(   npi_numbers = c(\"1234567890\", \"0987654321\"),   verbose = TRUE ) #> Error in get_facility_affiliations(npi_numbers = c(\"1234567890\", \"0987654321\"),     verbose = TRUE): could not find function \"get_facility_affiliations\"  # Example with batch processing fac_with_batches <- get_facility_affiliations(   npi_numbers = c(\"1234567890\", \"0987654321\", \"1122334455\"),   batch_size = 2,   sys_sleep = 1,   verbose = TRUE ) #> Error in get_facility_affiliations(npi_numbers = c(\"1234567890\", \"0987654321\",     \"1122334455\"), batch_size = 2, sys_sleep = 1, verbose = TRUE): could not find function \"get_facility_affiliations\"  # Full example with all parameters and CSV output fac_full <- get_facility_affiliations(   npi_numbers = c(\"1234567890\", \"0987654321\"),   batch_size = 5,   url = \"https://data.cms.gov/provider-data/api/1/datastore/query/27ea-46a8/0\",   limit = 50,   sys_sleep = 1,   user_agent = \"my-research-project\",   verbose = TRUE,   csv_save_path = \"output/cms_data\" ) #> Error in get_facility_affiliations(npi_numbers = c(\"1234567890\", \"0987654321\"),     batch_size = 5, url = \"https://data.cms.gov/provider-data/api/1/datastore/query/27ea-46a8/0\",     limit = 50, sys_sleep = 1, user_agent = \"my-research-project\",     verbose = TRUE, csv_save_path = \"output/cms_data\"): could not find function \"get_facility_affiliations\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_national_downloadable_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","title":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","text":"Fetches provider information CMS National Downloadable File API using NPI numbers. Includes robust error handling, logging, batch processing.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_national_downloadable_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","text":"","code":"phase0_national_downloadable_file(   npi_numbers,   batch_size = 10,   url = \"https://data.cms.gov/provider-data/api/1/datastore/query/mj5m-pzi6/0\",   limit = 100,   sys_sleep = 0.5,   user_agent = \"r-package-cms-query\",   verbose = TRUE,   csv_save_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_national_downloadable_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","text":"npi_numbers Character vector NPI numbers query batch_size Integer. Number NPIs query per batch. Default: 10 url Character. CMS API endpoint URL. Default: CMS data store URL limit Integer. Maximum records per API call. Default: 100 sys_sleep Numeric. Sleep time API calls seconds. Default: 0.5 user_agent Character. User agent string API calls. Default: \"r-package-cms-query\" verbose Logical. Whether output detailed logs. Default: FALSE csv_save_path Character. Optional path save results CSV. Default: NULL","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_national_downloadable_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","text":"tibble containing provider information CMS \"NDF_\" prefixed column names. Common returned fields include: NDF_npi - National Provider Identifier NDF_provider_name - Provider's full name NDF_cred - Provider credentials (e.g., \"M.D.\", \"D.O.\") NDF_provider_address - Practice location address NDF_provider_state - State practice NDF_specialty - Provider specialty description","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_national_downloadable_file.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query CMS National Downloadable File for Provider Information — phase0_national_downloadable_file","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage with minimal parameters npi_results <- phase0_national_downloadable_file(   npi_numbers = c(\"1234567890\", \"0987654321\"),   verbose = TRUE )  # Example with CSV output npi_with_save <- phase0_national_downloadable_file(   npi_numbers = c(\"1234567890\", \"0987654321\"),   batch_size = 2,   csv_save_path = \"output/cms_data\",   verbose = TRUE )  # Full example with all parameters npi_detailed <- phase0_national_downloadable_file(   npi_numbers = c(\"1234567890\", \"0987654321\", \"1122334455\"),   batch_size = 5,   url = \"https://data.cms.gov/provider-data/api/1/datastore/query/mj5m-pzi6/0\",   limit = 50,   sys_sleep = 1,   user_agent = \"my-research-project\",   verbose = TRUE,   csv_save_path = \"output/cms_data\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"function validates set NPI numbers, removes invalid missing entries, retrieves detailed clinician information valid NPIs. supports dataframe CSV file inputs ensures clean validated output clinician data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"","code":"phase0_retrieve_clinician_data(npi_data, verbose = TRUE)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"npi_data Either dataframe containing NPI numbers (must include column named npi) path CSV file NPI numbers. column npi must contain numeric character representations NPIs exactly 10 digits. verbose logical value controls whether detailed logging shown process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"tibble detailed clinician data valid NPI, including expanded metadata columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"function first validates input data ensure contains correctly formatted NPIs. Missing invalid NPIs removed. , valid NPI, function retrieves clinician data using provider package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_retrieve_clinician_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Phase 0: Retrieve Clinician Data by NPI — phase0_retrieve_clinician_data","text":"","code":"# Example 1: Validate and retrieve clinician data from a dataframe if (FALSE) { # \\dontrun{ npi_data_frame <- tibble::tibble(npi = c(\"1234567890\", \"1689603763\", \"invalid_npi\", NA)) clinician_data <- phase0_retrieve_clinician_data(npi_data_frame, verbose = TRUE) print(clinician_data) } # }  # Example 2: Validate and retrieve clinician data from a CSV file if (FALSE) { # \\dontrun{ npi_csv_path <- \"npi_data.csv\" npi_data_frame <- tibble::tibble(npi = c(\"1234567890\", \"1689603763\", \"invalid_npi\")) readr::write_csv(npi_data_frame, npi_csv_path) clinician_data <- phase0_retrieve_clinician_data(npi_csv_path, verbose = TRUE) print(clinician_data) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch NPI Search Function — phase0_search_batch_npi","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"function performs batch search names using NPI registry API returns flattened dataframe relevant results. Optionally, saves results CSV file. Extensive logging provided track step, including function start, input validation, NPI query status, data transformation, file saving.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"","code":"phase0_search_batch_npi(df, limit = 5, write_csv_path = NULL)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"df data frame columns first last, representing first last names query NPI registry. limit Number results per query. Defaults 5. write_csv_path File path saving results CSV. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"dataframe containing flattened NPI results, including columns queried first last names. results found, empty tibble returned.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"additional details NPPES API data dictionary, see: https://www.cms.gov/regulations--guidance/administrative-simplification/nationalprovidentstand/downloads/data_dissemination_file-code_values.pdf.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_batch_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Batch NPI Search Function — phase0_search_batch_npi","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Basic batch NPI search with default limit name_df <- data.frame(   first = c(\"Tyler\", \"Matthew\"),   last = c(\"Muffly\", \"Smith\"),   stringsAsFactors = FALSE ) results <- phase0_search_batch_npi(df = name_df) print(results)  # Example 2: Batch NPI search with increased limit results <- phase0_search_batch_npi(df = name_df, limit = 10) print(results)  # Example 3: Batch NPI search with CSV output results <- phase0_search_batch_npi(   df = name_df,   limit = 5,   write_csv_path = \"npi_search_results.csv\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_npi_by_number.html","id":null,"dir":"Reference","previous_headings":"","what":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","title":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","text":"function searches clinician data based list provided NPI numbers, processes data, saves search results specified chunk sizes CSV files. Logging occurs every step, function built work ---box example input, default chunking, logging console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_npi_by_number.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","text":"","code":"phase0_search_npi_by_number(   npi_dataframe,   records_per_chunk = 10,   save_directory = NULL )  phase0_search_npi_by_number(   npi_dataframe,   records_per_chunk = 10,   save_directory = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_npi_by_number.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","text":"npi_dataframe data frame column 'npi' containing NPI numbers search. records_per_chunk Integer. Specifies number records save per CSV file chunk. Default 10. save_directory Character. Directory save chunked results. Default NULL, defaults current working directory.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_npi_by_number.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","text":"combined data frame NPI search results combined data frame NPI search results. Returns empty data frame valid NPIs found.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_search_npi_by_number.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search and Process NPI Numbers with Detailed Logging — phase0_search_npi_by_number","text":"","code":"# Example 1: Basic usage with default settings npi_list <- data.frame(npi = c(\"1447273792\", \"1023030814\")) clinician_info <- phase0_search_npi_by_number(   npi_dataframe = npi_list ) #> INFO [2025-03-11 06:46:04] Function phase0_search_npi_by_number called with inputs: records_per_chunk = 10, save_directory = /Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference #> INFO [2025-03-11 06:46:04] Filtered NPI data. Number of valid NPIs: 2 #> INFO [2025-03-11 06:46:04] Fetching data for NPI: 1447273792 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:06] Successfully retrieved and flattened data for NPI: 1447273792 #> INFO [2025-03-11 06:46:06] Fetching data for NPI: 1023030814 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:08] Successfully retrieved and flattened data for NPI: 1023030814 #> INFO [2025-03-11 06:46:08] Data retrieved for 2 NPIs #> INFO [2025-03-11 06:46:08] Combined all clinician information. Total rows: 6, Total columns: 16 #> INFO [2025-03-11 06:46:08] Saved chunk of clinician information to /Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference/npi_results_20250311064608_chunk_1.csv #> INFO [2025-03-11 06:46:08] Function phase0_search_npi_by_number completed successfully.  # Example 2: Custom chunk size and directory temp_dir <- tempdir() clinician_info <- phase0_search_npi_by_number(   npi_dataframe = npi_list,   records_per_chunk = 5,   save_directory = temp_dir ) #> INFO [2025-03-11 06:46:08] Function phase0_search_npi_by_number called with inputs: records_per_chunk = 5, save_directory = /var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//Rtmp38Kqc6 #> INFO [2025-03-11 06:46:08] Filtered NPI data. Number of valid NPIs: 2 #> INFO [2025-03-11 06:46:08] Fetching data for NPI: 1447273792 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:09] Successfully retrieved and flattened data for NPI: 1447273792 #> INFO [2025-03-11 06:46:09] Fetching data for NPI: 1023030814 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:11] Successfully retrieved and flattened data for NPI: 1023030814 #> INFO [2025-03-11 06:46:11] Data retrieved for 2 NPIs #> INFO [2025-03-11 06:46:11] Combined all clinician information. Total rows: 6, Total columns: 16 #> INFO [2025-03-11 06:46:11] Saved chunk of clinician information to /var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//Rtmp38Kqc6/npi_results_20250311064611_chunk_1.csv #> INFO [2025-03-11 06:46:11] Saved chunk of clinician information to /var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//Rtmp38Kqc6/npi_results_20250311064611_chunk_6.csv #> INFO [2025-03-11 06:46:11] Function phase0_search_npi_by_number completed successfully.  # Example 3: Handle larger dataset with logging npi_large <- data.frame(   npi = c(\"1447273792\", \"1023030814\", \"1750380234\") ) clinician_info <- phase0_search_npi_by_number(   npi_dataframe = npi_large,   records_per_chunk = 2,   save_directory = \"npi_results\" ) #> INFO [2025-03-11 06:46:11] Function phase0_search_npi_by_number called with inputs: records_per_chunk = 2, save_directory = npi_results #> INFO [2025-03-11 06:46:11] Filtered NPI data. Number of valid NPIs: 3 #> INFO [2025-03-11 06:46:11] Fetching data for NPI: 1447273792 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:13] Successfully retrieved and flattened data for NPI: 1447273792 #> INFO [2025-03-11 06:46:13] Fetching data for NPI: 1023030814 #> 10 records requested #> Requesting records 0-10... #> INFO [2025-03-11 06:46:14] Successfully retrieved and flattened data for NPI: 1023030814 #> INFO [2025-03-11 06:46:14] Fetching data for NPI: 1750380234 #> 10 records requested #> Requesting records 0-10... #> ERROR [2025-03-11 06:46:16] Error retrieving data for NPI 1750380234: `df` must be an npi_results S3 object, not tbl_df. #> INFO [2025-03-11 06:46:16] Data retrieved for 2 NPIs #> INFO [2025-03-11 06:46:16] Combined all clinician information. Total rows: 6, Total columns: 16 #> INFO [2025-03-11 06:46:16] Saved chunk of clinician information to npi_results/npi_results_20250311064616_chunk_1.csv #> INFO [2025-03-11 06:46:16] Saved chunk of clinician information to npi_results/npi_results_20250311064616_chunk_3.csv #> INFO [2025-03-11 06:46:16] Saved chunk of clinician information to npi_results/npi_results_20250311064616_chunk_5.csv #> INFO [2025-03-11 06:46:16] Function phase0_search_npi_by_number completed successfully. # Example 1: Basic usage with default chunk size and current directory npi_list <- data.frame(npi = c(\"1234567890\", \"1098765432\", \"1987654321\")) phase0_search_npi_by_number(npi_dataframe = npi_list) #> INFO [2025-03-11 06:46:16] Function phase0_search_npi_by_number called with inputs: records_per_chunk = 10, save_directory = /Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference #> INFO [2025-03-11 06:46:16] Filtered NPI data. Number of valid NPIs: 3 #> INFO [2025-03-11 06:46:16] Fetching data for NPI: 1234567890 #> 10 records requested #> Requesting records 0-10... #> ERROR [2025-03-11 06:46:18] Error retrieving data for NPI 1234567890: `df` must be an npi_results S3 object, not tbl_df. #> INFO [2025-03-11 06:46:18] Fetching data for NPI: 1098765432 #> 10 records requested #> Requesting records 0-10... #> ERROR [2025-03-11 06:46:20] Error retrieving data for NPI 1098765432: `df` must be an npi_results S3 object, not tbl_df. #> INFO [2025-03-11 06:46:20] Fetching data for NPI: 1987654321 #> 10 records requested #> Requesting records 0-10... #> ERROR [2025-03-11 06:46:21] Error retrieving data for NPI 1987654321: `df` must be an npi_results S3 object, not tbl_df. #> INFO [2025-03-11 06:46:21] Data retrieved for 0 NPIs #> INFO [2025-03-11 06:46:21] Combined all clinician information. Total rows: 0, Total columns: 0 #> INFO [2025-03-11 06:46:21] No clinician information to save. #> INFO [2025-03-11 06:46:21] Function phase0_search_npi_by_number completed successfully. #> Null data.table (0 rows and 0 cols)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_split_calls_to_lab_assistants_and_save_by_priority.html","id":null,"dir":"Reference","previous_headings":"","what":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","title":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","text":"function splits input dataset based specified column (e.g., insurance specialty), assigns lab assistants group, saves subset separate CSV files. logs inputs, outputs, transformations console using logger package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_split_calls_to_lab_assistants_and_save_by_priority.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","text":"","code":"phase0_split_calls_to_lab_assistants_and_save_by_priority(   input_data_or_path,   output_folder = getwd(),   assistant_names,   seed_value = 1978,   complete_file_prefix = \"complete_version_\",   split_file_prefix = \"\",   recursive_create = TRUE,   split_column = \"insurance\",   priority_values = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_split_calls_to_lab_assistants_and_save_by_priority.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","text":"input_data_or_path dataframe valid file path input data (supports CSV, RDS, XLS/XLSX). output_folder Directory output files saved. Default current working directory. assistant_names character vector lab assistant names assigning cases. seed_value integer seed reproducibility. Default 1978. complete_file_prefix Prefix complete output file. Default \"complete_version_\". split_file_prefix Prefix split output files. Default empty string. recursive_create Logical indicating directories created recursively. Default TRUE. split_column column used split data (e.g., \"insurance\"). Default \"insurance\". priority_values character vector priority values within split_column ensure priority assignment. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_split_calls_to_lab_assistants_and_save_by_priority.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","text":"modified dataset processing. Data transformations logged console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase0_split_calls_to_lab_assistants_and_save_by_priority.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Phase 0: Split and Save Data by Lab Assistants with Priority — phase0_split_calls_to_lab_assistants_and_save_by_priority","text":"","code":"# Example 1: Basic usage with a dataframe sample_dataset <- data.frame(   insurance = c(\"Medicare\", \"Medicaid\", \"BCBS\"),   NPI = c(\"1234567890\", \"9876543210\", \"1122334455\"),   phone_number = c(\"123-456-7890\", \"098-765-4321\", \"555-555-5555\") ) assistants <- c(\"Alice\", \"Bob\", \"Charlie\") phase0_split_calls_to_lab_assistants_and_save_by_priority(   input_data_or_path = sample_dataset,   output_folder = \"output/\",   assistant_names = assistants ) #> INFO [2025-03-11 06:46:22] Starting phase0_split_calls_to_lab_assistants_and_save_by_priority() with inputs: #> INFO [2025-03-11 06:46:22] Output folder: output/ #> INFO [2025-03-11 06:46:22] Assistant names: Alice, Bob, Charlie #> INFO [2025-03-11 06:46:22] Seed value: 1978 #> INFO [2025-03-11 06:46:22] Split column: insurance #> INFO [2025-03-11 06:46:22] Priority values: None #> INFO [2025-03-11 06:46:22] Loaded input data with 3 rows and 3 columns. #> INFO [2025-03-11 06:46:22] Validated presence of split column: insurance. #> INFO [2025-03-11 06:46:22] Assigned lab assistants to rows in the dataset. #> Error in dplyr::mutate(., combined_info = paste0(row_number, \", \", row_number,     \", \", combined_info, \", Grouping: \", !!rlang::sym(split_column))): ℹ In argument: `combined_info = paste0(...)`. #> Caused by error: #> ! object 'combined_info' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase2_sample_surgeons.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","title":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","text":"function expands dataset physicians assigning multiple insurance types physician retaining original columns. logs inputs, outputs, data transformations, file paths console using logger package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase2_sample_surgeons.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","text":"","code":"phase2_sample_surgeons(   data,   insurance_types = c(\"Medicare\", \"Medicaid\", \"BCBS\"),   ensure_unique_phone_numbers = TRUE,   output_csv_path = NULL,   seed = 1978,   specialty_column = \"specialty_primary\",   npi_column = \"NPI\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase2_sample_surgeons.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","text":"data dataframe containing physician information. Must include columns specialty, NPI, phone number. insurance_types character vector insurance types assign physician. Default c(\"Medicare\", \"Medicaid\", \"BCBS\"). ensure_unique_phone_numbers Logical indicating whether ensure unique phone numbers among physicians expansion. Default TRUE. output_csv_path Directory save output CSV file. NULL, file saved. Default NULL. seed integer set random seed reproducibility. Default 1978. specialty_column name column representing specialty. Default \"specialty_primary\". npi_column name column representing NPI. Default \"NPI\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase2_sample_surgeons.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","text":"dataframe expanded rows, including one physician-insurance combination.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/phase2_sample_surgeons.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample Spine Surgeons Across Insurance Scenarios (Detailed Logging) — phase2_sample_surgeons","text":"","code":"# Example 1: Basic usage with default parameters physicians <- data.frame(   specialty_primary = c(\"Orthopedics\", \"Cardiology\"),   NPI = c(\"1234567890\", \"9876543210\"),   phone_number = c(\"123-456-7890\", \"098-765-4321\") ) expanded_data <- phase2_sample_surgeons(   data = physicians ) #> INFO [2025-03-11 06:46:22] Starting phase2_sample_surgeons() with inputs: #> INFO [2025-03-11 06:46:22] Insurance types: Medicare #> INFO [2025-03-11 06:46:22] Insurance types: Medicaid #> INFO [2025-03-11 06:46:22] Insurance types: BCBS #> INFO [2025-03-11 06:46:22] Ensure unique phone numbers: TRUE #> INFO [2025-03-11 06:46:22]  #> INFO [2025-03-11 06:46:22] Seed: 1978 #> INFO [2025-03-11 06:46:22] Specialty column: specialty_primary #> INFO [2025-03-11 06:46:22] NPI column: NPI #> INFO [2025-03-11 06:46:22] Seed set to 1978. #> INFO [2025-03-11 06:46:22] Ensuring unique phone numbers in the dataset. #> INFO [2025-03-11 06:46:22] Number of rows after ensuring unique phone numbers: 2. #> INFO [2025-03-11 06:46:22] Expanding physicians by insurance types... #> INFO [2025-03-11 06:46:22] Expanded data includes insurance types. Total rows: 6. #> INFO [2025-03-11 06:46:22] Completed phase2_sample_surgeons(). Returning expanded data. print(expanded_data) #> # A tibble: 6 × 4 #>   specialty_primary NPI        phone_number insurance #>   <chr>             <chr>      <chr>        <chr>     #> 1 Orthopedics       1234567890 123-456-7890 Medicare  #> 2 Orthopedics       1234567890 123-456-7890 Medicaid  #> 3 Orthopedics       1234567890 123-456-7890 BCBS      #> 4 Cardiology        9876543210 098-765-4321 Medicare  #> 5 Cardiology        9876543210 098-765-4321 Medicaid  #> 6 Cardiology        9876543210 098-765-4321 BCBS       # Example 2: Custom insurance types and saving output expanded_data <- phase2_sample_surgeons(   data = physicians,   insurance_types = c(\"Private\", \"Public\"),   output_csv_path = \"output/\" ) #> INFO [2025-03-11 06:46:22] Starting phase2_sample_surgeons() with inputs: #> INFO [2025-03-11 06:46:22] Insurance types: Private #> INFO [2025-03-11 06:46:22] Insurance types: Public #> INFO [2025-03-11 06:46:22] Ensure unique phone numbers: TRUE #> INFO [2025-03-11 06:46:22] Output CSV path: output/ #> INFO [2025-03-11 06:46:22] Seed: 1978 #> INFO [2025-03-11 06:46:22] Specialty column: specialty_primary #> INFO [2025-03-11 06:46:22] NPI column: NPI #> INFO [2025-03-11 06:46:22] Seed set to 1978. #> INFO [2025-03-11 06:46:22] Ensuring unique phone numbers in the dataset. #> INFO [2025-03-11 06:46:22] Number of rows after ensuring unique phone numbers: 2. #> INFO [2025-03-11 06:46:22] Expanding physicians by insurance types... #> INFO [2025-03-11 06:46:22] Expanded data includes insurance types. Total rows: 4. #> INFO [2025-03-11 06:46:22] Saved expanded data to: output//phase2_sample_surgeons_by_insurance_2025-03-11_06-46-22.csv. #> INFO [2025-03-11 06:46:22] Completed phase2_sample_surgeons(). Returning expanded data. print(expanded_data) #> # A tibble: 4 × 4 #>   specialty_primary NPI        phone_number insurance #>   <chr>             <chr>      <chr>        <chr>     #> 1 Orthopedics       1234567890 123-456-7890 Private   #> 2 Orthopedics       1234567890 123-456-7890 Public    #> 3 Cardiology        9876543210 098-765-4321 Private   #> 4 Cardiology        9876543210 098-765-4321 Public     # Example 3: Ensuring unique phone numbers and using a custom seed physicians <- data.frame(   specialty_primary = c(\"Orthopedics\", \"Cardiology\", \"Orthopedics\"),   NPI = c(\"1234567890\", \"9876543210\", \"1234567890\"),   phone_number = c(\"123-456-7890\", \"098-765-4321\", \"123-456-7890\") ) expanded_data <- phase2_sample_surgeons(   data = physicians,   ensure_unique_phone_numbers = TRUE,   seed = 42 ) #> INFO [2025-03-11 06:46:22] Starting phase2_sample_surgeons() with inputs: #> INFO [2025-03-11 06:46:22] Insurance types: Medicare #> INFO [2025-03-11 06:46:22] Insurance types: Medicaid #> INFO [2025-03-11 06:46:22] Insurance types: BCBS #> INFO [2025-03-11 06:46:22] Ensure unique phone numbers: TRUE #> INFO [2025-03-11 06:46:22]  #> INFO [2025-03-11 06:46:22] Seed: 42 #> INFO [2025-03-11 06:46:22] Specialty column: specialty_primary #> INFO [2025-03-11 06:46:22] NPI column: NPI #> INFO [2025-03-11 06:46:22] Seed set to 42. #> INFO [2025-03-11 06:46:22] Ensuring unique phone numbers in the dataset. #> INFO [2025-03-11 06:46:22] Number of rows after ensuring unique phone numbers: 2. #> INFO [2025-03-11 06:46:22] Expanding physicians by insurance types... #> INFO [2025-03-11 06:46:22] Expanded data includes insurance types. Total rows: 6. #> INFO [2025-03-11 06:46:22] Completed phase2_sample_surgeons(). Returning expanded data. print(expanded_data) #> # A tibble: 6 × 4 #>   specialty_primary NPI        phone_number insurance #>   <chr>             <chr>      <chr>        <chr>     #> 1 Orthopedics       1234567890 123-456-7890 Medicare  #> 2 Orthopedics       1234567890 123-456-7890 Medicaid  #> 3 Orthopedics       1234567890 123-456-7890 BCBS      #> 4 Cardiology        9876543210 098-765-4321 Medicare  #> 5 Cardiology        9876543210 098-765-4321 Medicaid  #> 6 Cardiology        9876543210 098-765-4321 BCBS"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate and Summarize Physician Age — physician_age","title":"Calculate and Summarize Physician Age — physician_age","text":"function calculates median age, well 25th 75th percentiles (Interquartile Range, IQR) specified age column data frame. returns sentence summarizing statistics.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate and Summarize Physician Age — physician_age","text":"","code":"physician_age(df, age_column)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate and Summarize Physician Age — physician_age","text":"df data frame containing age data. age_column character string representing name column df contains age data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate and Summarize Physician Age — physician_age","text":"character string summarizing median age IQR specified age column dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate and Summarize Physician Age — physician_age","text":"function calculates median, 25th percentile (Q1), 75th percentile (Q3) age data, rounding results two decimal places median one decimal place percentiles. constructs summary sentence describing statistics.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physician_age.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate and Summarize Physician Age — physician_age","text":"","code":"# Example 1: Basic usage with a small dataset df <- data.frame(age = c(30, 40, 50, 60, 35, 45, 55, 65)) summary_sentence <- physician_age(df, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 47.5 (IQR 25th percentile 38.8 to 75th percentile 56.2).\"  # Example 2: Handling missing data df_with_na <- data.frame(age = c(30, 40, NA, 60, 35, NA, 55, 65)) summary_sentence <- physician_age(df_with_na, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 47.5 (IQR 25th percentile 36.2 to 75th percentile 58.8).\"  # Example 3: Different age distribution df_large <- data.frame(age = c(rep(30, 70), rep(40, 30), rep(50, 20), rep(60, 10))) summary_sentence <- physician_age(df_large, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 30 (IQR 25th percentile 30 to 75th percentile 40).\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Board-Certified OBGYN Subspecialists — physicians","title":"Board-Certified OBGYN Subspecialists — physicians","text":"dataset contains information board-certified obstetrician-gynecologist (OBGYN) subspecialists United States, including National Provider Identifier (NPI), names, subspecialties, geographic coordinates.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Board-Certified OBGYN Subspecialists — physicians","text":"","code":"physicians"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Board-Certified OBGYN Subspecialists — physicians","text":"tibble 4,659 rows 5 columns: NPI numeric value representing National Provider Identifier physician. name character string containing name physician. subspecialty character string describing physician's OBGYN subspecialty. lat latitude physician's primary practice location. long longitude physician's primary practice location.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Board-Certified OBGYN Subspecialists — physicians","text":"data manually compiled demonstration purposes. National Provider Identifiers (NPI) subspecialties based publicly available data sources.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Board-Certified OBGYN Subspecialists — physicians","text":"dataset provides details physicians specialized OBGYN subspecialties Female Pelvic Medicine, Maternal-Fetal Medicine, Gynecologic Oncology, . data includes geographic information mapping spatial analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/physicians.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Board-Certified OBGYN Subspecialists — physicians","text":"","code":"# Load the physicians dataset data(physicians)  # View the structure of the dataset str(physicians) #> tibble [4,659 × 5] (S3: tbl_df/tbl/data.frame) #>  $ NPI         : num [1:4659] 1.92e+09 1.75e+09 1.55e+09 1.77e+09 1.76e+09 ... #>  $ name        : chr [1:4659] \"Katherine Boyd\" \"Thomas Byrne\" \"Bobby Garcia\" \"Peter McGovern\" ... #>  $ subspecialty: chr [1:4659] \"Female Pelvic Medicine and Reconstructive Surgery\" \"Maternal-Fetal Medicine\" \"Female Pelvic Medicine and Reconstructive Surgery\" \"Reproductive Endocrinology and Infertility\" ... #>  $ lat         : num [1:4659] 42.6 35.2 40.8 40.9 40.8 ... #>  $ long        : num [1:4659] -82.9 -101.9 -73.9 -73.9 -73.9 ...  # Summary of subspecialties summary(physicians$subspecialty) #>    Length     Class      Mode  #>      4659 character character   # Basic mapping using ggplot2 library(ggplot2) ggplot(physicians, aes(x = long, y = lat, color = subspecialty)) +   geom_point() +   ggtitle(\"Locations of Board-Certified OBGYN Subspecialists\") +   theme_minimal() #> Warning: Removed 171 rows containing missing values or values outside the scale range #> (`geom_point()`)."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_and_save_emmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"function computes estimated marginal means (EMMs) model object, creates plot EMMs confidence intervals, saves plot specified directory. includes robust error handling, detailed logging, default behaviors work box.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_and_save_emmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"","code":"plot_and_save_emmeans(   model_object,   specs,   variable_of_interest,   color_by,   output_dir = \"Ari/Figures\",   y_min = NULL,   y_max = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_and_save_emmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"model_object fitted model object EMMs computed. can generalized linear model (GLM), linear model, suitable models. specs character string formula specifying predictor variable(s) EMMs computed. example, treatment groups, scenarios, demographic variables. variable_of_interest character string specifying variable plotted x-axis. Typically, specs parameter. color_by character string specifying variable used color points error bars. categorical variable gender, insurance type, academic affiliation. output_dir character string specifying directory plot saved. Defaults \"Ari/Figures\". y_min Minimum value y-axis. Defaults NULL, means calculated automatically data. y_max Maximum value y-axis. Defaults NULL, means calculated automatically data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_and_save_emmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"list containing estimated marginal means data (data) ggplot object (plot).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_and_save_emmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"function logs data transformation, inputs, outputs, output files saved. uses emmeans package compute EMMs creates plot using ggplot2. function ensures logging key steps assist debugging auditing.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"function creates choropleth map visualizing physician counts region, options grouping states custom regions. handles merging state counts regional data, calculating centroids text labels, generating plots using ggplot2. Logging included provide insights execution flow data transformations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"","code":"plot_region_counts(state_counts, region_df, region_col, save_path = NULL)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"state_counts dataframe containing state-level physician counts. Must columns state_code (lowercase state names) total_available (numerical counts). region_df dataframe mapping states regions. Must include State (state names lowercase) column region grouping. region_col string specifying column region_df use grouping states regions (e.g., \"Region\"). save_path Optional. string specifying file path save plot image. NULL (default), plot displayed onscreen saved.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"ggplot object representing choropleth map.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"function merges state_counts region_df state name, aggregates physician counts region, maps counts onto choropleth. Labels representing counts added region centroids.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/plot_region_counts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"","code":"# Example 1: Plot and save a map grouped by ENT_BOG regions ent_bog_regions <- data.frame(   State = c(\"alabama\", \"alaska\", \"arizona\"),   ENT_BOG_Region = c(\"Region 4\", \"Region 9\", \"Region 9\") ) state_counts <- data.frame(   state_code = c(\"alabama\", \"alaska\", \"arizona\"),   total_available = c(10, 15, 8) ) plot_region_counts(   state_counts = state_counts,   region_df = ent_bog_regions,   region_col = \"ENT_BOG_Region\",   save_path = \"ent_bog_region_map.png\" ) #> INFO [2025-03-11 06:46:24] Function `plot_region_counts` called with inputs: #> INFO [2025-03-11 06:46:24] state_counts dataframe with columns: state_code, total_available #> INFO [2025-03-11 06:46:24] region_df dataframe with columns: State, ENT_BOG_Region #> INFO [2025-03-11 06:46:24] Region column for grouping: ENT_BOG_Region #> INFO [2025-03-11 06:46:24] Save path specified: ent_bog_region_map.png #> INFO [2025-03-11 06:46:24] Transforming state codes in `state_counts` and `region_df` to lowercase. #> INFO [2025-03-11 06:46:24] Merging `state_counts` with `region_df` based on the State and `ENT_BOG_Region` columns. #> INFO [2025-03-11 06:46:24] Completed merge and aggregation. Resulting `state_counts_with_regions` has 2 rows. #> INFO [2025-03-11 06:46:24] Retrieving US state map data and converting region names to lowercase. #> INFO [2025-03-11 06:46:24] Merging state map data with `region_df` and `state_counts_with_regions`. #> INFO [2025-03-11 06:46:24] Map data merged. `map_data_with_regions` now has 15537 rows. #> INFO [2025-03-11 06:46:24] Calculating centroids for each region to position labels. #> INFO [2025-03-11 06:46:24] Centroids calculated for each region. `region_centroids` has 3 rows. #> INFO [2025-03-11 06:46:24] Creating the choropleth map with regions as fill and centroid labels. #> INFO [2025-03-11 06:46:24] Plot creation complete. #> INFO [2025-03-11 06:46:24] Saving the plot to specified path: ent_bog_region_map.png #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_text()`). #> INFO [2025-03-11 06:46:25] Plot successfully saved at ent_bog_region_map.png #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_text()`).  #> INFO [2025-03-11 06:46:25] Function `plot_region_counts` completed execution. #> INFO [2025-03-11 06:46:25] Outputs: Displayed or saved choropleth map with regional physician counts.  # Example 2: Plot ACOG Districts without saving ACOG_Districts_sf <- data.frame(   State = c(\"california\", \"nevada\", \"utah\"),   ACOG_District = c(\"District IX\", \"District IX\", \"District VIII\") ) state_counts <- data.frame(   state_code = c(\"california\", \"nevada\", \"utah\"),   total_available = c(20, 12, 5) ) plot_region_counts(   state_counts = state_counts,   region_df = ACOG_Districts_sf,   region_col = \"ACOG_District\" ) #> INFO [2025-03-11 06:46:25] Function `plot_region_counts` called with inputs: #> INFO [2025-03-11 06:46:25] state_counts dataframe with columns: state_code, total_available #> INFO [2025-03-11 06:46:25] region_df dataframe with columns: State, ACOG_District #> INFO [2025-03-11 06:46:25] Region column for grouping: ACOG_District #> INFO [2025-03-11 06:46:25] No save path specified; plot will only display on screen. #> INFO [2025-03-11 06:46:25] Transforming state codes in `state_counts` and `region_df` to lowercase. #> INFO [2025-03-11 06:46:25] Merging `state_counts` with `region_df` based on the State and `ACOG_District` columns. #> INFO [2025-03-11 06:46:25] Completed merge and aggregation. Resulting `state_counts_with_regions` has 2 rows. #> INFO [2025-03-11 06:46:25] Retrieving US state map data and converting region names to lowercase. #> INFO [2025-03-11 06:46:25] Merging state map data with `region_df` and `state_counts_with_regions`. #> INFO [2025-03-11 06:46:25] Map data merged. `map_data_with_regions` now has 15537 rows. #> INFO [2025-03-11 06:46:25] Calculating centroids for each region to position labels. #> INFO [2025-03-11 06:46:25] Centroids calculated for each region. `region_centroids` has 3 rows. #> INFO [2025-03-11 06:46:25] Creating the choropleth map with regions as fill and centroid labels. #> INFO [2025-03-11 06:46:25] Plot creation complete. #> INFO [2025-03-11 06:46:25] No save path provided, displaying plot on screen. #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_text()`).  #> INFO [2025-03-11 06:46:25] Function `plot_region_counts` completed execution. #> INFO [2025-03-11 06:46:25] Outputs: Displayed or saved choropleth map with regional physician counts.  # Example 3: Plot custom regions and save output custom_regions <- data.frame(   State = c(\"texas\", \"new york\", \"florida\"),   Custom_Region = c(\"South\", \"Northeast\", \"South\") ) state_counts <- data.frame(   state_code = c(\"texas\", \"new york\", \"florida\"),   total_available = c(25, 17, 13) ) plot_region_counts(   state_counts = state_counts,   region_df = custom_regions,   region_col = \"Custom_Region\",   save_path = \"custom_region_map.png\" ) #> INFO [2025-03-11 06:46:25] Function `plot_region_counts` called with inputs: #> INFO [2025-03-11 06:46:25] state_counts dataframe with columns: state_code, total_available #> INFO [2025-03-11 06:46:25] region_df dataframe with columns: State, Custom_Region #> INFO [2025-03-11 06:46:25] Region column for grouping: Custom_Region #> INFO [2025-03-11 06:46:25] Save path specified: custom_region_map.png #> INFO [2025-03-11 06:46:25] Transforming state codes in `state_counts` and `region_df` to lowercase. #> INFO [2025-03-11 06:46:25] Merging `state_counts` with `region_df` based on the State and `Custom_Region` columns. #> INFO [2025-03-11 06:46:25] Completed merge and aggregation. Resulting `state_counts_with_regions` has 2 rows. #> INFO [2025-03-11 06:46:25] Retrieving US state map data and converting region names to lowercase. #> INFO [2025-03-11 06:46:25] Merging state map data with `region_df` and `state_counts_with_regions`. #> INFO [2025-03-11 06:46:25] Map data merged. `map_data_with_regions` now has 15537 rows. #> INFO [2025-03-11 06:46:25] Calculating centroids for each region to position labels. #> INFO [2025-03-11 06:46:25] Centroids calculated for each region. `region_centroids` has 3 rows. #> INFO [2025-03-11 06:46:25] Creating the choropleth map with regions as fill and centroid labels. #> INFO [2025-03-11 06:46:25] Plot creation complete. #> INFO [2025-03-11 06:46:25] Saving the plot to specified path: custom_region_map.png #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_text()`). #> INFO [2025-03-11 06:46:26] Plot successfully saved at custom_region_map.png #> Warning: Removed 1 row containing missing values or values outside the scale range #> (`geom_text()`).  #> INFO [2025-03-11 06:46:26] Function `plot_region_counts` completed execution. #> INFO [2025-03-11 06:46:26] Outputs: Displayed or saved choropleth map with regional physician counts."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"function reads dataset, fits Poisson regression model predict waiting times (business_days_until_appointment) based specified grouping variable, prints incidence rate ratios (IRR) along confidence intervals p-values. function returns fitted Poisson model.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"","code":"poisson_wait_time_stats(data_dir, file_name, group_var = \"insurance\")"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"data_dir character string specifying directory data file located. file_name character string specifying name RDS file read. group_var character string specifying grouping variable use model (e.g., \"insurance\" \"scenario\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"fitted Poisson regression model object (glm).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"function fits Poisson regression model log link function. model estimates incidence rate ratios (IRRs) level specified grouping variable relative reference level (first level dataset). function prints IRRs, confidence intervals, p-values comparison. fitted model returned use.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/poisson_wait_time_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage: poisson_model <- poisson_wait_time_stats(data_dir = \"your_data_directory\",                                          file_name = \"Phase_2.rds\",                                          group_var = \"scenario\") } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/prepare_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"function prepares dataset excluding specified columns predictor variables. logs process, including inputs outputs.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/prepare_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"","code":"prepare_dataset(df, target_variable, excluded_columns)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/prepare_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"df data frame containing dataset. target_variable string representing name target variable. excluded_columns vector strings representing names columns exclude predictors.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/prepare_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"vector strings representing names predictor variables.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Process and Save Isochrones — process_and_save_isochrones","title":"Process and Save Isochrones — process_and_save_isochrones","text":"function takes input file locations, retrieves isochrones location, saves shapefiles. processes data chunks 25 rows time prevent data loss case errors.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process and Save Isochrones — process_and_save_isochrones","text":"","code":"process_and_save_isochrones(input_file, chunk_size = 25)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process and Save Isochrones — process_and_save_isochrones","text":"input_file data frame containing location data columns \"lat\" \"long.\" input file represent geographic coordinates isochrones calculated. chunk_size number rows process chunk. Default 25.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process and Save Isochrones — process_and_save_isochrones","text":"sf (simple features) data frame containing isochrone polygons.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process and Save Isochrones — process_and_save_isochrones","text":"function uses hereR package calculate isochrones based provided geographic coordinates. retrieves isochrones location input file, processes data chunks minimize risk data loss case errors, saves isochrones shapefiles analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/process_and_save_isochrones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process and Save Isochrones — process_and_save_isochrones","text":"","code":"# Load the input file (e.g., from a CSV) input_file <- read_csv(\"data/locations.csv\") #> Error in read_csv(\"data/locations.csv\"): could not find function \"read_csv\"  # Process and save isochrones for the input file (chunk size set to 25) isochrones_data <- process_and_save_isochrones(input_file, chunk_size = 25) #> Error in process_and_save_isochrones(input_file, chunk_size = 25): could not find function \"process_and_save_isochrones\"  # Optionally, write the combined isochrones to a shapefile sf::st_write(isochrones_data, dsn = \"data/isochrones/isochrones_all_combined\",              layer = \"isochrones\", driver = \"ESRI Shapefile\", quiet = FALSE) #> Error: object 'isochrones_data' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/qualitycheck.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Quality Check Table — qualitycheck","title":"Save Quality Check Table — qualitycheck","text":"function takes data frame containing 'npi' 'name' columns creates quality check table. table includes count observations 'npi' 'name' combination count greater 2. resulting table saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/qualitycheck.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Quality Check Table — qualitycheck","text":"","code":"qualitycheck(df, filepath)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/qualitycheck.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Quality Check Table — qualitycheck","text":"df data frame containing columns 'npi' 'name'. filepath path CSV file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/qualitycheck.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save Quality Check Table — qualitycheck","text":"Prints message console indicating CSV file saved successfully.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/race_drive_time_generate_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"function generates summary sentence indicating level access gynecologic oncologists specified race drive time. can run individual races races dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/race_drive_time_generate_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"","code":"race_drive_time_generate_summary_sentence(   tabulated_data,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/race_drive_time_generate_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"tabulated_data data frame containing data analyze. Must include columns Driving Time (minutes), Year, total_female_026, columns race proportions like White_prop, Black_prop, etc. driving_time_minutes numeric value specifying driving time minutes filter data. Default 180. race character string specifying race generate summary sentence. Supported values \"White\", \"Black\", \"American Indian/Alaska Native\", \"Asian\", \"Native Hawaiian Pacific Islander\", \"\" generate sentences supported races. Default \"American Indian/Alaska Native\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/race_drive_time_generate_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"character string containing summary sentence, list summary sentences race = \"\".","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/race_drive_time_generate_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"","code":"# Example usage summary_sentence <- race_drive_time_generate_summary_sentence(   tabulated_data = tabulated_all_years_numeric,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" ) #> Function race_drive_time_generate_summary_sentence called with inputs: #> Driving Time (minutes): 180 #> Race: American Indian/Alaska Native #> Race column used for analysis: AIAN_prop #> Error: object 'tabulated_all_years_numeric' not found print(summary_sentence) #> Error: object 'summary_sentence' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":null,"dir":"Reference","previous_headings":"","what":"U.S. Census Bureau Regions and Divisions — regions_df","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"dataset provides region division information U.S. states, based U.S. Census Bureau classifications.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"","code":"regions_df"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"data frame 51 rows 4 variables: State name U.S. state (e.g., \"California\"). State.Code two-letter postal abbreviation state (e.g., \"CA\"). Region region state (e.g., \"West\"). Division division state (e.g., \"Pacific\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"U.S. Census Bureau","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"dataset can used group U.S. states region division analysis. Region column categorizes states Northeast, Midwest, South, West. Division column provides finer granularity within region.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/regions_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"U.S. Census Bureau Regions and Divisions — regions_df","text":"","code":"# Load the dataset data(regions_df)  # View the first few rows head(regions_df) #>        State State.Code Region           Division #> 1     Alaska         AK   West            Pacific #> 2    Alabama         AL  South East South Central #> 3   Arkansas         AR  South West South Central #> 4    Arizona         AZ   West           Mountain #> 5 California         CA   West            Pacific #> 6   Colorado         CO   West           Mountain  # Count states by region table(regions_df$Region) #>  #>   Midwest Northeast     South      West  #>        12         9        17        13   # Subset states in the Pacific division subset(regions_df, Division == \"Pacific\") #>         State State.Code Region Division #> 1      Alaska         AK   West  Pacific #> 5  California         CA   West  Pacific #> 12     Hawaii         HI   West  Pacific #> 38     Oregon         OR   West  Pacific #> 48 Washington         WA   West  Pacific"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_constant_vars.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Constant Variables from a Data Frame — remove_constant_vars","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"function takes data frame returns new data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_constant_vars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"","code":"remove_constant_vars(data_frame)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_constant_vars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"data_frame data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_constant_vars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_constant_vars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"","code":"if (FALSE) { # \\dontrun{ new_data <- remove_constant_vars(data_frame) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_near_zero_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"function takes data frame returns new data frame near-zero variance variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_near_zero_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"","code":"remove_near_zero_var(data_frame, freqCut = 19, uniqueCut = 10)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_near_zero_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"data_frame data frame near-zero variance variables removed. freqCut ratio common value second common value. Defaults 19. uniqueCut percentage distinct values number total samples. Defaults 10.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_near_zero_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"data frame near-zero variance variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/remove_near_zero_var.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"","code":"if (FALSE) { # \\dontrun{ new_data <- remove_near_zero_var(data_frame) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/rename_columns_by_substring.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename columns based on substring matches — rename_columns_by_substring","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"function searches column names data frame specified substrings renames first matching column new name provided user. logs detailed information operation, including columns found renaming actions taken. multiple columns match substring, first renamed, warning issued.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/rename_columns_by_substring.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"","code":"rename_columns_by_substring(data, target_strings, new_names)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/rename_columns_by_substring.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"data data frame whose columns need renaming. target_strings vector substrings search within column names. new_names vector new names corresponding target strings.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/rename_columns_by_substring.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"data frame renamed columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/rename_columns_by_substring.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"","code":"# Example 1: Simple renaming of a single column df1 <- data.frame(   doctor_info = 1:5,   patient_contact_data = 6:10 ) df1 <- rename_columns_by_substring(df1,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician_info\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 1 columns matching 'doctor': doctor_info #>  #>  #> --- Column renaming complete. Updated column names:  physician_info, patient_contact_data --- print(df1)  # Should show 'physician_info' instead of 'doctor_info' #>   physician_info patient_contact_data #> 1              1                    6 #> 2              2                    7 #> 3              3                    8 #> 4              4                    9 #> 5              5                   10  # Example 2: Renaming multiple columns with a single target string df2 <- data.frame(   doctor_notes = 1:5,   doctor_info = 6:10,   patient_data = 11:15 ) df2 <- rename_columns_by_substring(df2,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 2 columns matching 'doctor': doctor_notes, doctor_info #> Warning: Multiple columns match 'doctor'. Only the first (doctor_notes) will be renamed to 'physician'. #>  #>  #> --- Column renaming complete. Updated column names:  physician, doctor_info, patient_data --- print(df2)  # Should show 'physician' instead of 'doctor_info' and 'doctor_notes' #>   physician doctor_info patient_data #> 1         1           6           11 #> 2         2           7           12 #> 3         3           8           13 #> 4         4           9           14 #> 5         5          10           15  # Example 3: Renaming with a warning for multiple matches df3 <- data.frame(   doctor_details = 1:5,   doctor_notes = 6:10,   doctor_data = 11:15 ) df3 <- rename_columns_by_substring(df3,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 3 columns matching 'doctor': doctor_details, doctor_notes, doctor_data #> Warning: Multiple columns match 'doctor'. Only the first (doctor_details) will be renamed to 'physician'. #>  #>  #> --- Column renaming complete. Updated column names:  physician, doctor_notes, doctor_data --- print(df3)  # Should show 'physician' as the first match and issue a warning #>   physician doctor_notes doctor_data #> 1         1            6          11 #> 2         2            7          12 #> 3         3            8          13 #> 4         4            9          14 #> 5         5           10          15"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/result_section_pairwise_difference.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","title":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","text":"function performs pairwise Kruskal-Wallis tests numeric variable across specified x-axis groups, analyzes directionality trends, identifies significance trends, generates interpretive sentences.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/result_section_pairwise_difference.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","text":"","code":"result_section_pairwise_difference(data, numeric_var, x_var)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/result_section_pairwise_difference.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","text":"data tibble containing least columns specified x_var numeric_var. numeric_var string specifying name numeric variable analyze. x_var string specifying name grouping (x-axis) variable analyze trends across.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/result_section_pairwise_difference.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","text":"list containing pairwise_trends, trend_summary, interpretation_sentences.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/result_section_pairwise_difference.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze Pairwise Trends Programmatically — result_section_pairwise_difference","text":"","code":"# Example 1: Basic analysis with three scenarios example_data <- tibble::tibble(   scenario = rep(c(\"Group_A\", \"Group_B\", \"Group_C\"), each = 30),   measurement = c(rnorm(30, mean = 10), rnorm(30, mean = 15), rnorm(30, mean = 20)) ) result <- result_section_pairwise_difference(   data = example_data, numeric_var = \"measurement\", x_var = \"scenario\" ) #> INFO [2025-03-11 06:46:28] Starting result_section_pairwise_difference... #> INFO [2025-03-11 06:46:28] Input tibble dimensions: 90 rows, 2 columns. #> INFO [2025-03-11 06:46:28] Analyzing numeric variable: measurement, grouped by: scenario. #> INFO [2025-03-11 06:46:28] Step 1: Performing pairwise Kruskal-Wallis tests... #> INFO [2025-03-11 06:46:28] Unique groups identified in scenario: 3 #> INFO [2025-03-11 06:46:28] Comparing groups: Group_A vs Group_B #> INFO [2025-03-11 06:46:28] Comparing groups: Group_A vs Group_C #> INFO [2025-03-11 06:46:28] Comparing groups: Group_B vs Group_C #> INFO [2025-03-11 06:46:28] Pairwise comparisons completed. Total comparisons: 3. #> INFO [2025-03-11 06:46:28] Step 2: Analyzing directionality trends... #> INFO [2025-03-11 06:46:28] Directionality analysis completed successfully. #> INFO [2025-03-11 06:46:28] Step 3: Analyzing significance trends... #> INFO [2025-03-11 06:46:28] Significance analysis completed successfully. #> INFO [2025-03-11 06:46:28] Step 4: Combining directionality and significance trends... #> INFO [2025-03-11 06:46:28] Trends combined successfully. #> INFO [2025-03-11 06:46:28] Calculating medians and percentiles for each group in scenario... #> INFO [2025-03-11 06:46:28] Medians and percentiles calculated successfully. #> INFO [2025-03-11 06:46:28] Generating interpretive sentences... #> INFO [2025-03-11 06:46:28] Interpretive sentences generated successfully. #> INFO [2025-03-11 06:46:28] Pairwise trend analysis completed successfully. #> INFO [2025-03-11 06:46:28] Returning pairwise trends, trend summary, and interpretation sentences. print(result$pairwise_trends) #> # A tibble: 3 × 5 #>   Group1  Group2  Direction  p_value p_value_formatted #>   <chr>   <chr>   <chr>        <dbl> <chr>             #> 1 Group_A Group_B Lower     3.51e-11 p<0.01            #> 2 Group_A Group_C Lower     2.87e-11 p<0.01            #> 3 Group_B Group_C Lower     2.87e-11 p<0.01            print(result$trend_summary) #> # A tibble: 3 × 5 #>   Group1  Group2  Higher Lower Significant #>   <chr>   <chr>    <int> <int>       <int> #> 1 Group_A Group_B      0     1           1 #> 2 Group_A Group_C      0     1           1 #> 3 Group_B Group_C      0     1           1 print(result$interpretation_sentences) #> Payments for Group_A (median: $9.8996; IQR [$9.6005 - $11.0700]) are lower than payments for Group_B (median: $15.1479; IQR [$14.2304 - $15.6084], p<0.01). #> Payments for Group_A (median: $9.8996; IQR [$9.6005 - $11.0700]) are lower than payments for Group_C (median: $20.2969; IQR [$19.6703 - $20.7564], p<0.01). #> Payments for Group_B (median: $15.1479; IQR [$14.2304 - $15.6084]) are lower than payments for Group_C (median: $20.2969; IQR [$19.6703 - $20.7564], p<0.01)."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_analysis_household_income.html","id":null,"dir":"Reference","previous_headings":"","what":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","title":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","text":"function analyzes distribution physicians across household income quartiles based ZIP code data within U.S. state. logs steps process, including data retrieval, processing, calculation income quartiles.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_analysis_household_income.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","text":"","code":"results_section_analysis_household_income(   year = 2022,   physician_information_with_zip )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_analysis_household_income.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","text":"year Integer specifying ACS survey year household income data, defaults 2022. physician_information_with_zip String specifying file path physician information dataset, containing least \"ID\" \"zip\" columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_analysis_household_income.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","text":"tibble summarizing distribution physicians across household income quartiles state, including overall summary U.S. highest income quartile.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_analysis_household_income.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analyze Physician Distribution by Household Income Quartile with Logging — results_section_analysis_household_income","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Standard analysis for ACS year 2022 physician_income_analysis <- results_section_analysis_household_income(   year = 2022,   physician_information_with_zip = \"path/to/physician_data.rds\" ) print(physician_income_analysis)  # Example 2: Specifying an earlier ACS year for historical analysis physician_income_analysis <- results_section_analysis_household_income(   year = 2020,   physician_information_with_zip = \"path/to/physician_data.rds\" ) head(physician_income_analysis) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_exclusions.html","id":null,"dir":"Reference","previous_headings":"","what":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","title":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","text":"function provides summary exclusion criteria applied mystery caller study data. identifies count percentage physicians excluded based predefined criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_exclusions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","text":"","code":"results_section_exclusions(   call_data,   exclusion_col = \"reason_for_exclusions\",   able_to_contact_value = \"Able to contact\",   group_var = NULL )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_exclusions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","text":"call_data data frame containing mystery caller data, including exclusion criteria call status columns. exclusion_col string specifying column contains exclusion reasons (e.g., reason_for_exclusions). able_to_contact_value string representing value exclusion column indicates successful contact. Default \"Able contact\". group_var optional string specifying grouping variable (e.g., specialty). provided, exclusions summarized within group. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_exclusions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","text":"formatted summary string describing exclusions counts percentages.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_exclusions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Exclusion Criteria Summary for Mystery Caller Study — results_section_exclusions","text":"","code":"# Example 1: Basic exclusion summary call_data <- data.frame(   physician_id = 1:100,   reason_for_exclusions = sample(c(     \"Able to contact\", \"Went to voicemail\",     \"Not accepting new patients\"   ), 100, replace = TRUE) ) summary <- results_section_exclusions(   call_data = call_data,   exclusion_col = \"reason_for_exclusions\" ) #> INFO [2025-03-11 06:46:28] Starting exclusions summary calculation. #> INFO [2025-03-11 06:46:28]  #> INFO [2025-03-11 06:46:28] Generated exclusion summary: Of the total 100 phone calls made, 36 were successfully connected, and 64 were excluded. print(summary) #> [1] \"Of the total 100 phone calls made, 36 were successfully connected, and 64 were excluded.\"  # Example 2: Summary with grouping variable call_data$specialty <- sample(c(\"OBGYN\", \"Family Medicine\"), 100, replace = TRUE) grouped_summary <- results_section_exclusions(   call_data = call_data,   exclusion_col = \"reason_for_exclusions\",   group_var = \"specialty\" ) #> INFO [2025-03-11 06:46:28] Starting exclusions summary calculation. #> INFO [2025-03-11 06:46:28] Input details - Total rows in call_data: 100, exclusion_col: reason_for_exclusions, able_to_contact_value: Able to contact, group_var: specialty #> INFO [2025-03-11 06:46:28] Exclusion summary by group created. #> INFO [2025-03-11 06:46:28] Generated exclusion summary: Of the total 100 phone calls made, 71 were successfully connected, and 29 were excluded. #> INFO [2025-03-11 06:46:28] Generated exclusion summary: Of the total 100 phone calls made, 65 were successfully connected, and 35 were excluded. print(grouped_summary) #> [1] \"Of the total 100 phone calls made, 71 were successfully connected, and 29 were excluded.\" #> [2] \"Of the total 100 phone calls made, 65 were successfully connected, and 35 were excluded.\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_fit_mixed_model_with_logging.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","title":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","text":"function fits mixed-effects model analyze association appointment wait times insurance type, random effects state physician ID. logs model creation diagnostics step.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_fit_mixed_model_with_logging.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","text":"","code":"results_section_fit_mixed_model_with_logging(   wait_time_data,   outcome_var,   random_effects,   fixed_effects,   verbose = FALSE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_fit_mixed_model_with_logging.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","text":"wait_time_data data frame columns including wait_time, insurance_type, state, physician_id, demographic factors. outcome_var string specifying outcome variable, typically wait_time. random_effects character vector variables include random effects (e.g., state, physician_id). fixed_effects character vector fixed effect variables include model (e.g., insurance_type, scenario). verbose Logical. TRUE, prints detailed model diagnostics.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_fit_mixed_model_with_logging.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","text":"list containing fitted mixed-effects model summaries.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_fit_mixed_model_with_logging.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Mixed Effects Model for Mystery Caller Study with Logging — results_section_fit_mixed_model_with_logging","text":"","code":"# Example usage here"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_generate_exclusion_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Exclusion Summary — results_section_generate_exclusion_summary","title":"Generate Exclusion Summary — results_section_generate_exclusion_summary","text":"function generates summary sentence janitor::tabyl output includes counts percentages exclusions. uses tidyverse approach data transformations logger package logging inputs, transformations, outputs console.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_generate_exclusion_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Exclusion Summary — results_section_generate_exclusion_summary","text":"","code":"results_section_generate_exclusion_summary(   exclusion_summary_table,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_generate_exclusion_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Exclusion Summary — results_section_generate_exclusion_summary","text":"exclusion_summary_table dataframe created janitor::tabyl, containing columns n, percent, reason_for_exclusions. dataframe summarizes exclusions respective counts percentages. verbose Logical. TRUE, logs detailed information process, including input validation, filtering, sentence construction. Defaults TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_generate_exclusion_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Exclusion Summary — results_section_generate_exclusion_summary","text":"dynamically ordered summary sentence exclusions character string.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_generate_exclusion_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Exclusion Summary — results_section_generate_exclusion_summary","text":"","code":"# Example 1: Basic usage with detailed logging library(janitor) #>  #> Attaching package: ‘janitor’ #> The following objects are masked from ‘package:stats’: #>  #>     chisq.test, fisher.test exclusion_summary_table <- janitor::tabyl(df_filtered$reason_for_exclusions) %>%   dplyr::rename(reason_for_exclusions = `df_filtered$reason_for_exclusions`) %>%   dplyr::mutate(percent = paste0(round(percent * 100, 0), \"%\")) %>%   dplyr::arrange(dplyr::desc(n)) #> Error: object 'df_filtered' not found  exclusion_sentence <- results_section_generate_exclusion_summary(   exclusion_summary_table = exclusion_summary_table,   verbose = TRUE ) #> INFO [2025-03-11 06:46:30] Starting results_section_generate_exclusion_summary function. #> INFO [2025-03-11 06:46:30] Validating input table... #> Error: object 'exclusion_summary_table' not found print(exclusion_sentence) #> Error: object 'exclusion_sentence' not found  # Example 2: Suppressing verbose logging exclusion_summary_table <- janitor::tabyl(df_filtered$reason_for_exclusions) %>%   dplyr::rename(reason_for_exclusions = `df_filtered$reason_for_exclusions`) %>%   dplyr::mutate(percent = paste0(round(percent * 100, 0), \"%\")) %>%   dplyr::arrange(dplyr::desc(n)) #> Error: object 'df_filtered' not found  exclusion_sentence <- results_section_generate_exclusion_summary(   exclusion_summary_table = exclusion_summary_table,   verbose = FALSE ) #> Error: object 'exclusion_summary_table' not found print(exclusion_sentence) #> Error: object 'exclusion_sentence' not found  # Example 3: Handling a small exclusion table small_table <- data.frame(   reason_for_exclusions = c(\"Reason A\", \"Reason B\", \"Reason C\"),   n = c(10, 5, 2),   percent = c(\"50%\", \"25%\", \"10%\") )  exclusion_sentence <- results_section_generate_exclusion_summary(   exclusion_summary_table = small_table,   verbose = TRUE ) #> INFO [2025-03-11 06:46:30] Starting results_section_generate_exclusion_summary function. #> INFO [2025-03-11 06:46:30] Validating input table... #> INFO [2025-03-11 06:46:30] Input table dimensions: 3 rows, 3 columns. #> INFO [2025-03-11 06:46:30] Input column names: reason_for_exclusions, n, percent #> INFO [2025-03-11 06:46:30] Filtering out 'Able to contact' and arranging by count... #> INFO [2025-03-11 06:46:30] Filtered exclusions: 3 rows after filtering. #> INFO [2025-03-11 06:46:30] Generating dynamic sentence fragments... #> INFO [2025-03-11 06:46:30] Generated sentence fragments: 10 (50%) reason a, 5 (25%) reason b, and 2 (10%) reason c #> INFO [2025-03-11 06:46:30] Completed results_section_generate_exclusion_summary function. #> INFO [2025-03-11 06:46:30] Generated exclusion summary sentence: Of the excluded calls, 10 (50%) reason a, 5 (25%) reason b, and 2 (10%) reason c. print(exclusion_sentence) #> Of the excluded calls, 10 (50%) reason a, 5 (25%) reason b, and 2 (10%) reason c."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_medicaid_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"function generates summary sentence results section based significant predictor variables Medicaid acceptance rates. logs process, performs error checking, includes default behavior robust execution.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_medicaid_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"","code":"results_section_medicaid_summary(   significant_predictors,   medicaid_acceptance_rate,   accepted_medicaid_count,   total_medicaid_physicians_count )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_medicaid_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"significant_predictors data frame containing significant predictor variables, directions, formatted p-values. columns: \"Variable\", \"Direction\", \"Formatted_P_Value\". medicaid_acceptance_rate numeric value representing Medicaid acceptance rate (percentage). accepted_medicaid_count integer representing count physicians accepting Medicaid. total_medicaid_physicians_count integer representing total count physicians considered Medicaid.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_medicaid_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"character string representing summary sentence.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_medicaid_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"","code":"# Example 2: Using larger physician counts and different acceptance rate significant_vars <- data.frame(   Variable = c(\"Experience\", \"Training Level\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.05\", \"0.01\") ) results_section_medicaid_summary(   significant_vars,   medicaid_acceptance_rate = 35.2,   accepted_medicaid_count = 500,   total_medicaid_physicians_count = 1200 ) #> Starting to create the summary sentence... #> Validating inputs... #> Inputs validated successfully. #> Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #>  Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 )  #> Constructing summary sentence... #> Error in gender_info$value: $ operator is invalid for atomic vectors  # Example 3: Using a single significant predictor and a high acceptance rate significant_vars <- data.frame(   Variable = c(\"Age\"),   Direction = c(\"positively associated\"),   Formatted_P_Value = c(\"<0.001\") ) results_section_medicaid_summary(significant_vars, 60.0, 750, 1250) #> Starting to create the summary sentence... #> Validating inputs... #> Inputs validated successfully. #> Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #>  Age positively associated (p = <0.001 )  #> Constructing summary sentence... #> Error in gender_info$value: $ operator is invalid for atomic vectors"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_physician_by_household_income.html","id":null,"dir":"Reference","previous_headings":"","what":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","title":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","text":"function retrieves ACS data household income ZIP code level, classifies physicians income quartiles within state, returns summary distribution physicians across income quartiles state well overall summary U.S. function also logs process, including inputs, outputs, step analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_physician_by_household_income.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","text":"","code":"results_section_physician_by_household_income(   year = 2022,   physician_information_with_zip = \"Phase_2.rds\" )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_physician_by_household_income.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","text":"year Integer. year ACS survey. Defaults latest ACS 5-year survey available (e.g., 2022). physician_information_with_zip String. file path physician information dataset. dataset must contain \"ID\" \"zip\" columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_physician_by_household_income.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","text":"tibble summarizing physician distribution household income quartile across state. tibble contains columns: - state: State \"US\" U.S. summary. - income_quartile: income quartile (Q1 - Q4). - income_range: income range quartile. - physicians_in_quartile: number physicians quartile. - total_physicians: total number physicians state. - percent_in_quartile: percentage physicians quartile.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_physician_by_household_income.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Physician Distribution by Household Income Quartile — results_section_physician_by_household_income","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Default behavior with specified ACS year and physician data file physician_summary <- results_section_physician_by_household_income(   year = 2022,   physician_information_with_zip = \"path/to/Phase_2.rds\" ) print(physician_summary)  # Example 2: Running with a different ACS year and a different file path physician_summary_2020 <- results_section_physician_by_household_income(   year = 2020,   physician_information_with_zip = \"path/to/Phase_2_2020.rds\" ) print(physician_summary_2020)  # Example 3: Checking the structure of the output print(head(physician_summary)) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"Calculate Interpret Overdispersion Metrics Poisson Model","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"","code":"results_section_poisson_overdispersion_testing(poisson_model)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"poisson_model fitted Poisson model object (e.g., created stats::glm using family = poisson). function expects valid Poisson regression model.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"list two components: metrics named vector containing chi-squared statistic, ratio, residual degrees freedom, p-value. interpretation character string providing interpretation overdispersion results.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"Overdispersion occurs variance greater mean count data, violating assumptions Poisson regression. function helps assess whether overdispersion present model offers guidance addressing detected. interpretation categorizes overdispersion four levels: significant overdispersion: model's fit adequate. Slight overdispersion: May require adjustments. Mild overdispersion: Negative Binomial model beneficial. Significant overdispersion: Consider using Negative Binomial model adding random effects.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_poisson_overdispersion_testing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate and Interpret Overdispersion Metrics for a Poisson Model — results_section_poisson_overdispersion_testing","text":"","code":"# Example 1: Basic Poisson Model with InsectSprays dataset poisson_model <- stats::glm(count ~ spray, data = datasets::InsectSprays, family = poisson) overdispersion_result <- overdisp_fun(poisson_model) #> Error in overdisp_fun(poisson_model): could not find function \"overdisp_fun\"  # Print metrics # print(overdispersion_result$metrics)  # Example 2: Poisson Model with simulated count data set.seed(123) data_sim <- data.frame(   y = rpois(100, lambda = 3),   x = runif(100) ) poisson_model_sim <- stats::glm(y ~ x, data = data_sim, family = poisson) overdispersion_result_sim <- overdisp_fun(poisson_model_sim) #> Error in overdisp_fun(poisson_model_sim): could not find function \"overdisp_fun\"  # Print metrics # print(overdispersion_result_sim$metrics)  # Print interpretation # print(overdispersion_result_sim$interpretation)  # Example 3: Model with significant overdispersion data_overdisp <- data.frame(   y = c(rpois(50, lambda = 5), rpois(50, lambda = 20)),   x = c(rep(0, 50), rep(1, 50)) ) poisson_model_overdisp <- stats::glm(y ~ x, data = data_overdisp, family = poisson) overdispersion_result_overdisp <- overdisp_fun(poisson_model_overdisp) #> Error in overdisp_fun(poisson_model_overdisp): could not find function \"overdisp_fun\"  # Print metrics # print(overdispersion_result_overdisp$metrics)  # Print interpretation # print(overdispersion_result_overdisp$interpretation)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_summarize_common_provider_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"function calculates returns summary sentence describing common gender, specialty, training, academic affiliation provided dataset. filters missing values column determining common value, calculates proportion common value relative total non-missing values column.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_summarize_common_provider_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"","code":"results_section_summarize_common_provider_info(   provider_info,   gender_col,   specialty_col,   training_col,   academic_affiliation_col )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_summarize_common_provider_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"provider_info data frame containing provider information. gender_col Name column representing gender. specialty_col Name column representing specialty. training_col Name column representing training credentials. academic_affiliation_col Name column representing academic affiliation.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_summarize_common_provider_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"character string summarizing common gender, specialty, training, academic affiliation along respective proportions.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_summarize_common_provider_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"","code":"# Example usage with specified columns provider_info <- data.frame(   gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", \"Cardiology\", \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\") ) summarize_common_provider_info(   provider_info,   gender_col = \"gender\",   specialty_col = \"specialty\",   training_col = \"Provider.Credential.Text\",   academic_affiliation_col = \"academic_affiliation\" ) #> Error in summarize_common_provider_info(provider_info, gender_col = \"gender\",     specialty_col = \"specialty\", training_col = \"Provider.Credential.Text\",     academic_affiliation_col = \"academic_affiliation\"): could not find function \"summarize_common_provider_info\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_median_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"function calculates median wait time, 25th percentile (Q1), 75th percentile (Q3) business_days_until_appointment column overall grouped specified variable. logs step console includes error handling ensure robustness.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_median_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"","code":"results_section_wait_time_median_stats(   appointment_data,   wait_time_col = \"business_days_until_appointment\",   group_var = NULL,   round_digits = 1 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_median_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"appointment_data data frame containing appointment wait time data. wait_time_col Character string; name column representing wait time business days. group_var Character string; name column group grouped statistics (e.g., \"Subspecialty\"). NULL, overall statistics calculated. round_digits Integer; number decimal places round Q1 Q3 values.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_median_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"list containing two tibbles: stat_summary tibble calculated statistics median wait time, Q1, Q3, Group. sentence_summary tibble summary sentences group.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_median_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"","code":"# Example usage results <- results_section_wait_time_median_stats(   appointment_data = df,   wait_time_col = \"business_days_until_appointment\",   group_var = \"Subspecialty\",   round_digits = 1 ) #> INFO [2025-03-11 06:46:32] Starting wait time statistics calculation. #> INFO [2025-03-11 06:46:32]  #> INFO [2025-03-11 06:46:32] Error: Input must be a data frame. #> Error in results_section_wait_time_median_stats(appointment_data = df,     wait_time_col = \"business_days_until_appointment\", group_var = \"Subspecialty\",     round_digits = 1): Error: Input must be a data frame. results$stat_summary # Contains statistical data #> Error: object 'results' not found results$sentence_summary # Contains summary sentences #> Error: object 'results' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_poisson_estimates.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","title":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","text":"function calculates estimated wait time business days 95% confidence interval using Poisson regression business_days_until_appointment column, overall grouped. logs step console includes error handling.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_poisson_estimates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","text":"","code":"results_section_wait_time_poisson_estimates(   appointment_data,   wait_time_col = \"business_days_until_appointment\",   group_var = NULL,   round_digits = 1 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_poisson_estimates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","text":"appointment_data data frame containing appointment wait time data. wait_time_col Character string; name column representing wait time business days. group_var Character string; name column group grouped estimates. round_digits Integer; number decimal places round estimates.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_poisson_estimates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","text":"list containing two tibbles: stat_summary tibble estimated wait times, 95% CI, summary sentences group. sentence_summary tibble summary sentences group.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/results_section_wait_time_poisson_estimates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Wait Time Estimates with 95% CI Using Poisson Regression — results_section_wait_time_poisson_estimates","text":"","code":"# Example usage results <- results_section_wait_time_poisson_estimates(   appointment_data = df,   wait_time_col = \"business_days_until_appointment\",   group_var = \"Subspecialty\",   round_digits = 1 ) #> Error in log_function_start(appointment_data, wait_time_col, group_var,     round_digits): could not find function \"log_function_start\" results$stat_summary # Contains statistical data #> Error: object 'results' not found results$sentence_summary # Contains summary sentences #> Error: object 'results' not found"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Clinician Data — retrieve_clinician_data","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"Retrieves clinician data valid NPI dataframe CSV file. function ensures valid NPI numbers processed retrieves detailed clinician information.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"","code":"retrieve_clinician_data(npi_numbers)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"input_data Either dataframe containing NPI numbers (must include column named npi) path CSV file NPI numbers.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"tibble clinician data valid NPI.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"function checks input data validity removes invalid NPI numbers. CSV file path provided, file read, data processed.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/retrieve_clinician_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"","code":"# Example 1: Provide a dataframe with NPI numbers sample_data <- tibble::tibble(npi = c(1689603763, 1234567890)) clinician_data <- retrieve_clinician_data(sample_data) #> Error in purrr::map(npi_numbers, fetch_clinician_data): ℹ In index: 1. #> ℹ With name: npi. #> Caused by error in `h()`: #> ! `glue` failed in `formatter_glue` on: #>  #>  'glue' chr [1:2] \"Fetching data for NPI: 1689603763\" ... #>  #> Raw error message: #>  #> All unnamed arguments must be length 1 #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces. print(clinician_data) #> Error: object 'clinician_data' not found  # Example 2: Provide a CSV file path write.csv(sample_data, \"npi_data.csv\", row.names = FALSE) clinician_data <- retrieve_clinician_data(\"npi_data.csv\") #> INFO [2024-12-14 20:38:43] Fetching data for NPI: npi_data.csv #> 10 records requested #> Requesting records 0-10... #> ERROR [2024-12-14 20:38:44] Error retrieving data for NPI npi_data.csv:  #> Field: number #> NPI must be 10 digits #> INFO [2024-12-14 20:38:44] Data retrieved for 0 NPIs print(clinician_data) #> list()"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Save a ggplot Object to a File with Customizable Parameters — save_plot","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"save_plot function saves ggplot object specified file path customizable parameters width, height, resolution, . also handles directory creation, file format determination, logs saving process using logger package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"","code":"save_plot(   plot,   file_name,   save_directory,   plot_width = 8,   plot_height = 6,   resolution_dpi = 300,   size_units = \"in\",   scale_factor = 1,   file_format = NULL,   background_color = \"white\",   verbose_output = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"plot ggplot object saved. file_name character string specifying name file (including extension) save plot . save_directory character string specifying directory plot saved. plot_width Numeric value specifying width plot. Defaults 8. plot_height Numeric value specifying height plot. Defaults 6. resolution_dpi Numeric value specifying resolution dots per inch (DPI). Defaults 300. size_units Character string specifying units width height. Defaults \"\" (inches). scale_factor Numeric value specifying scaling factor plot. Defaults 1. file_format character string specifying file format save plot (e.g., \"png\", \"pdf\"). NULL, format inferred file extension file_name. Defaults NULL. background_color Character string specifying background color plot. Defaults \"white\". verbose_output Logical value indicating whether print messages console upon successful saving. Defaults TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"function return value. performs side effect saving plot file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"function checks necessary packages (logger, ggplot2, tools) loads required. ensures save directory exists, creating necessary. file format determined based file extension explicitly provided. function logs step process handles errors gracefully, providing informative messages.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save a ggplot Object to a File with Customizable Parameters — save_plot","text":"","code":"if (FALSE) { # \\dontrun{ # Load required packages library(ggplot2) library(tyler) # Assuming save_plot is part of the 'tyler' package  # Create a sample ggplot sample_plot <- ggplot(mtcars, aes(mpg, wt)) +   geom_point()  # Save the plot with default parameters save_plot(   plot = sample_plot,   file_name = \"my_plot.png\",   save_directory = \"plots\" )  # Save the plot with custom parameters save_plot(   plot = sample_plot,   file_name = \"my_plot.pdf\",   save_directory = \"plots\",   plot_width = 10,   plot_height = 8,   resolution_dpi = 600,   background_color = \"transparent\",   verbose_output = FALSE ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_quality_check_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Quality Check Table — save_quality_check_table","title":"Save Quality Check Table — save_quality_check_table","text":"function takes data frame containing 'npi' 'name' columns creates quality check table. table includes count observations 'npi' 'name' combination count greater 2. resulting table saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_quality_check_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Quality Check Table — save_quality_check_table","text":"","code":"save_quality_check_table(df, filepath)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_quality_check_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Quality Check Table — save_quality_check_table","text":"df data frame containing columns 'npi' 'name'. filepath path CSV file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/save_quality_check_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save Quality Check Table — save_quality_check_table","text":"Prints message console indicating CSV file saved successfully.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape Physicians' Data — scrape_physicians_data","title":"Scrape Physicians' Data — scrape_physicians_data","text":"function scrapes data physicians within specified ID range, excluding wrong IDs.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape Physicians' Data — scrape_physicians_data","text":"","code":"scrape_physicians_data(startID, endID)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape Physicians' Data — scrape_physicians_data","text":"startID starting ID scraping. endID ending ID scraping.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape Physicians' Data — scrape_physicians_data","text":"dataframe containing scraped physicians' data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape Physicians' Data — scrape_physicians_data","text":"","code":"# Call the function scrape_result <- scrape_physicians_data(startID = 9045999, endID = 9046000) #> Error in scrape_physicians_data(startID = 9045999, endID = 9046000): could not find function \"scrape_physicians_data\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data_with_tor.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"function scrapes data physicians within specified ID range, excluding wrong IDs.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data_with_tor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"","code":"scrape_physicians_data_with_tor(startID, endID, torPort)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data_with_tor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"startID starting ID scraping. endID ending ID scraping. torPort port number Tor SOCKS proxy.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data_with_tor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"dataframe containing scraped physicians' data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/scrape_physicians_data_with_tor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"","code":"# Call the function scrape_result <- scrape_physicians_data_with_tor(startID = 9045999, endID = 9046000, torPort = 9150) #> Error in scrape_physicians_data_with_tor(startID = 9045999, endID = 9046000,     torPort = 9150): could not find function \"scrape_physicians_data_with_tor\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NPI Numbers for Given Names — search_npi","title":"Search NPI Numbers for Given Names — search_npi","text":"function searches NPI (National Provider Identifier) numbers based given first last names. can accept input data form dataframe, CSV file, RDS file columns named 'first' 'last' representing first names last names, respectively.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NPI Numbers for Given Names — search_npi","text":"","code":"search_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NPI Numbers for Given Names — search_npi","text":"input_data dataframe, file path CSV, RDS file containing first last names.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NPI Numbers for Given Names — search_npi","text":"dataframe containing NPI numbers provided names match specified taxonomies.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search NPI Numbers for Given Names — search_npi","text":"","code":"# Input as a dataframe input_df <- data.frame(   first = c(\"John\", \"Jane\", \"Alice\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") ) npi_results <- search_npi(input_df) #> Error in validate_wildcard_rules(x): x must be a character vector with length 1  # Input as a CSV file input_csv <- \"path/to/input.csv\" npi_results <- search_npi(input_csv) #> Error: 'path/to/input.csv' does not exist in current working directory ('/Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference')."},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi_by_taxonomy.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","title":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","text":"function performs comprehensive search NPPES registry using taxonomy codes, retrieving matching providers across states. handles pagination API limits breaking searches state name-prefix combinations.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi_by_taxonomy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","text":"","code":"search_npi_by_taxonomy(   taxonomy_codes,   states = NULL,   limit_per_code = 200,   enumeration_type = NULL,   write_csv_path = NULL,   verbose = TRUE,   batch_delay = 1 )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi_by_taxonomy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","text":"taxonomy_codes Character vector taxonomy codes search. states Character vector two-letter state codes search. NULL, searches states. Default NULL. limit_per_code Integer. Maximum results per request (max 200). Default 200. enumeration_type Character. Filter provider type: \"NPI-1\" individuals, \"NPI-2\" organizations, NULL . Default NULL. write_csv_path Character. Optional file path save results CSV. Default NULL. verbose Logical. TRUE, prints detailed progress messages. Default TRUE. batch_delay Numeric. Delay seconds API calls. Default 1.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi_by_taxonomy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","text":"tibble containing flattened NPI results provider information.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/search_npi_by_taxonomy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Batch NPI Search by Taxonomy Function — search_npi_by_taxonomy","text":"","code":"if (FALSE) { # \\dontrun{ # Search for all OBGYNs across all states all_obgyns <- search_npi_by_taxonomy(   taxonomy_codes = \"207V00000X\",   states = NULL,   limit_per_code = 200,   enumeration_type = \"NPI-1\",   write_csv_path = \"all_obgyn_providers.csv\",   batch_delay = 1 )  # Search specific states state_obgyns <- search_npi_by_taxonomy(   taxonomy_codes = \"207V00000X\",   states = c(\"CA\", \"NY\", \"TX\"),   limit_per_code = 200,   enumeration_type = \"NPI-1\",   write_csv_path = \"state_obgyn_providers.csv\" )  # Search multiple taxonomies specialists <- search_npi_by_taxonomy(   taxonomy_codes = c(\"207V00000X\", \"207VX0201X\"),   states = \"CA\",   limit_per_code = 200,   enumeration_type = \"NPI-1\",   write_csv_path = \"ca_specialists.csv\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/split_and_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Split data into multiple parts and save each part as separate Excel files — split_and_save","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"function splits data based provided lab assistant names saves part separate Excel file. allows arrangement calls insurance type prioritize Medicaid first two days Blue Cross/Blue Shield last two days.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/split_and_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"","code":"split_and_save(   data_or_path,   output_directory,   lab_assistant_names,   seed = 1978,   complete_file_prefix = \"complete_non_split_version_\",   split_file_prefix = \"\",   recursive_create = TRUE,   insurance_order = c(\"Medicaid\", \"Blue Cross/Blue Shield\") )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/split_and_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"data_or_path Either dataframe containing input data path input data file (RDS, CSV, XLS/XLSX). output_directory Directory output Excel files saved. lab_assistant_names Vector lab assistant names name output files. seed Seed value randomization (default 1978). complete_file_prefix Prefix complete output file name (default \"complete_non_split_version_\"). split_file_prefix Prefix split output file name (default empty). recursive_create Logical indicating directories created recursively (default TRUE). insurance_order Vector insurance types ordered priority call scheduling (default c(\"Medicaid\", \"Blue Cross/Blue Shield\")).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/split_and_save.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"","code":"if (FALSE) { # \\dontrun{ library(tyler) input_data <- readr::read_csv(\"/path/to/your/input/file.csv\") output_directory <- \"/path/to/your/output/directory\" lab_assistant_names <- c(\"Label1\", \"Label2\", \"Label3\") insurance_order <- c(\"Medicaid\", \"Blue Cross/Blue Shield\") split_and_save(data_or_path = input_data, output_directory, lab_assistant_names, insurance_order = insurance_order) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/states_where_physicians_were_NOT_contacted.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"function summarizes demographic details identifying states physicians successfully contacted included.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/states_where_physicians_were_NOT_contacted.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"","code":"states_where_physicians_were_NOT_contacted(filtered_data, all_states = NULL)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/states_where_physicians_were_NOT_contacted.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"filtered_data data frame containing filtered data contacted physicians. all_states character vector possible states including Washington, DC. provided, default set states used.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/states_where_physicians_were_NOT_contacted.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"character string summarizing inclusion exclusion states.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/states_where_physicians_were_NOT_contacted.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"","code":"# Example with provided all_states filtered_data <- data.frame(state = c(\"California\", \"New York\", \"Texas\")) all_states <- c(   \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",   \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\",   \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\",   \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\",   \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\",   \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\",   \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\",   \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",   \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\",   \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",   \"District of Columbia\" ) states_where_physicians_were_NOT_contacted(filtered_data, all_states) #> [1] \"Physicians were successfully contacted in 3 states including the District of Columbia. The excluded states include Alabama, Alaska, Arizona, Arkansas, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming and District of Columbia.\"  # Example with default all_states filtered_data <- data.frame(state = c(\"California\", \"New York\", \"Texas\", \"Nevada\")) states_where_physicians_were_NOT_contacted(filtered_data) #> [1] \"Physicians were successfully contacted in 4 states including the District of Columbia. The excluded states include Alabama, Alaska, Arizona, Arkansas, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming and District of Columbia.\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/table1_by_insurance.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","title":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","text":"function creates descriptive Table 1 physician demographics separate columns insurance type (MEDICAID, BCBS, Medicare).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/table1_by_insurance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","text":"","code":"table1_by_insurance(   physician_data,   output_file = NULL,   control_settings = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/table1_by_insurance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","text":"physician_data data frame containing physician demographic data insurance information. Must include 'insurance' column values \"MEDICAID\", \"BCBS\", \"Medicare\". output_file Character string specifying path output file. NULL (default), file created. control_settings List tableby.control settings customize table. NULL (default), standard settings applied. verbose Logical. TRUE (default), generate detailed logging statements.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/table1_by_insurance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","text":"arsenal tableby object containing formatted Table 1 columns insurance type.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/table1_by_insurance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Table 1 for Physician Demographics by Insurance Acceptance — table1_by_insurance","text":"","code":"# Example 1: Create a basic Table 1 with default settings library(readr)  # Read sample data file_path <- \"Table_1_all_insurances_included.csv\" physician_data <- readr::read_csv(file_path) #> Error: 'Table_1_all_insurances_included.csv' does not exist in current working directory ('/Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference').  # Generate Table 1 table1_result <- table1_by_insurance(   physician_data = physician_data,   output_file = NULL,   control_settings = NULL,   verbose = TRUE ) #> Error in table1_by_insurance(physician_data = physician_data, output_file = NULL,     control_settings = NULL, verbose = TRUE): could not find function \"table1_by_insurance\"  # Print the results summary(table1_result) #> Error: object 'table1_result' not found  # Example 2: Create Table 1 with custom control settings and save to file custom_controls <- arsenal::tableby.control(   test = TRUE,   numeric.test = \"kwt\",   cat.test = \"chisq\",   numeric.stats = c(\"N\", \"mean\", \"sd\"),   cat.stats = c(\"countpct\"),   digits = 2 )  table1_custom <- table1_by_insurance(   physician_data = physician_data,   output_file = \"physician_demographics_by_insurance.docx\",   control_settings = custom_controls,   verbose = TRUE ) #> Error in table1_by_insurance(physician_data = physician_data, output_file = \"physician_demographics_by_insurance.docx\",     control_settings = custom_controls, verbose = TRUE): could not find function \"table1_by_insurance\"  # Example 3: Create Table 1 with non-default control settings # and examine physician characteristics by insurance acceptance mycontrols <- arsenal::tableby.control(   test = TRUE,   numeric.test = \"kwt\",   cat.test = \"chisq\",   numeric.stats = c(\"N\", \"medianq1q3\"),   cat.stats = c(\"countpct\"),   stats.labels = list(     medianq1q3 = \"Median (Q1, Q3)\",     N = \"n\"   ),   digits = 1,   digits.p = 2,   digits.pct = 1,   cat.simplify = FALSE )  table1_by_insurance <- table1_by_insurance(   physician_data = physician_data,   output_file = \"physician_by_insurance_table1.html\",   control_settings = mycontrols,   verbose = TRUE ) #> Error in table1_by_insurance(physician_data = physician_data, output_file = \"physician_by_insurance_table1.html\",     control_settings = mycontrols, verbose = TRUE): could not find function \"table1_by_insurance\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":null,"dir":"Reference","previous_headings":"","what":"Taxonomy Codes for Healthcare Providers — taxonomy","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"dataset contains taxonomy codes healthcare providers, including classifications specializations, defined National Uniform Claim Committee (NUCC). particularly useful mapping provider types respective NUCC taxonomy codes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"","code":"taxonomy"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"data frame 862 rows 3 variables: Code NUCC taxonomy code healthcare providers (e.g., \"101Y00000X\"). Classification general classification provider (e.g., \"Counselor\"). Specialization specific specialization within classification (e.g., \"Addiction (Substance Use Disorder)\"). May NA applicable.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"NUCC taxonomy codes, version 23.0: https://www.nucc.org/images/stories/PDF/taxonomy_23_0.pdf","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"dataset includes taxonomy codes across various provider types, limited Obstetricians Gynecologists. NUCC taxonomy widely used healthcare credentialing, claims processing, provider directories. dataset can assist linking provider specialties taxonomy codes research administrative purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/taxonomy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Taxonomy Codes for Healthcare Providers — taxonomy","text":"","code":"# Load the taxonomy dataset data(taxonomy)  # View the first few rows head(taxonomy) #> # A tibble: 6 × 3 #>   Code       Classification Specialization                     #>   <chr>      <chr>          <chr>                              #> 1 101Y00000X Counselor      NA                                 #> 2 101YA0400X Counselor      Addiction (Substance Use Disorder) #> 3 101YM0800X Counselor      Mental Health                      #> 4 101YP1600X Counselor      Pastoral                           #> 5 101YP2500X Counselor      Professional                       #> 6 101YS0200X Counselor      School                              # Filter for Obstetricians and Gynecologists obgyn_taxonomy <- taxonomy %>%   dplyr::filter(grepl(\"Obstetrics|Gynecology\", Classification, ignore.case = TRUE))  # Count the number of unique classifications dplyr::count(taxonomy, Classification) #> # A tibble: 241 × 2 #>    Classification                         n #>    <chr>                              <int> #>  1 Acupuncturist                          1 #>  2 Adult Companion                        1 #>  3 Advanced Practice Dental Therapist     1 #>  4 Advanced Practice Midwife              1 #>  5 Air Carrier                            1 #>  6 Allergy & Immunology                   3 #>  7 Alzheimer Center (Dementia Center)     1 #>  8 Ambulance                              4 #>  9 Anaplastologist                        1 #> 10 Anesthesiologist Assistant             1 #> # ℹ 231 more rows"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/test_and_process_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Test and Process Isochrones — test_and_process_isochrones","title":"Test and Process Isochrones — test_and_process_isochrones","text":"function tests processes isochrones location input file. identifies reports errors encountered isochrone retrieval process.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/test_and_process_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test and Process Isochrones — test_and_process_isochrones","text":"","code":"test_and_process_isochrones(input_file)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/test_and_process_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test and Process Isochrones — test_and_process_isochrones","text":"input_file data frame containing location data columns \"lat\" \"long.\" input file represent geographic coordinates isochrones calculated.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/test_and_process_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test and Process Isochrones — test_and_process_isochrones","text":"Prints messages indicating errors, , isochrone retrieval.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/test_and_process_isochrones.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test and Process Isochrones — test_and_process_isochrones","text":"function uses hereR package calculate isochrones based provided geographic coordinates. retrieves isochrones location input file, identifies errors retrieval process, reports errors. function designed used input data meets specific requirements, including valid latitude longitude values.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/tm_write2pdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Arsenal Table to PDF — tm_write2pdf","title":"Save Arsenal Table to PDF — tm_write2pdf","text":"function saves Arsenal table PDF.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/tm_write2pdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Arsenal Table to PDF — tm_write2pdf","text":"","code":"tm_write2pdf(object, filename)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/tm_write2pdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Arsenal Table to PDF — tm_write2pdf","text":"object Arsenal table object. filename file path saving PDF.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/tyler-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care — tyler-package","title":"tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care — tyler-package","text":"'tyler' package provides collection functions designed facilitate mystery caller studies, often used evaluating patient access healthcare. includes tools searching processing National Provider Identifier (NPI) numbers based names analyzing demographic data associated NPIs. package simplifies handling NPI data creation informative tables analysis reporting.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/uninsured_rate_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Uninsured Rate Summary by State and Gender — uninsured_rate_summary","title":"Uninsured Rate Summary by State and Gender — uninsured_rate_summary","text":"dataset provides uninsured rates males females across different states, calculated using data American Community Survey (ACS).","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/uninsured_rate_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uninsured Rate Summary by State and Gender — uninsured_rate_summary","text":"","code":"uninsured_rate_summary"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/uninsured_rate_summary.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Uninsured Rate Summary by State and Gender — uninsured_rate_summary","text":"tibble 52 rows 9 variables: State Name state uninsured_female Number uninsured females uninsured_male Number uninsured males total_male Total male population total_female Total female population uninsured_male_rate Proportion uninsured males uninsured_female_rate Proportion uninsured females uninsured_male_female Total number uninsured individuals (male female) total_male_female Total population (male female)","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/uninsured_rate_summary.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Uninsured Rate Summary by State and Gender — uninsured_rate_summary","text":"American Community Survey (ACS)","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":null,"dir":"Reference","previous_headings":"","what":"U.S. Regions and Divisions — us_census_bureau_regions_df","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"dataset provides comprehensive region division information U.S. states. includes regional groupings based U.S. Census Bureau's classifications supports wide range analyses involving geographic, economic, demographic data. dataset provides region division information U.S. states.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"","code":"us_census_bureau_regions_df  us_census_bureau_regions_df"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"data frame 51 rows 4 variables: State name U.S. state (e.g., \"California\", \"Texas\"). State Code two-letter postal abbreviation state (e.g., \"CA\", \"TX\"). Region region state (e.g., \"West\", \"South\"). Division division state (e.g., \"Pacific\", \"Mountain\"). data frame 51 rows 4 variables: State name U.S. state (e.g., \"California\"). State.Code two-letter postal abbreviation state. Region region state (e.g., \"West\"). Division division state (e.g., \"Pacific\").","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"U.S. Census Bureau. \"Regions Divisions United States.\" Available https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"Regions classified Northeast, South, Midwest, West. Divisions granular groupings within regions, Pacific, Mountain, East North Central. dataset aligns U.S. Census Bureau standards, making suitable demographic studies.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"U.S. Census Bureau. \"Geographic Terms Concepts - Census Divisions Regions.\" Retrieved https://www.census.gov/programs-surveys/geography/guidance/geo-areas/reg-div.html.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/us_census_bureau_regions_df.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"U.S. Regions and Divisions — us_census_bureau_regions_df","text":"","code":"# Load the dataset data(regions_df)  # View the unique regions unique(regions_df$Region) #> [1] \"West\"      \"South\"     \"Northeast\" \"Midwest\"    # Count states per division table(regions_df$Division) #>  #> East North Central East South Central    Middle Atlantic           Mountain  #>                  5                  4                  4                  8  #>        New England            Pacific     South Atlantic West North Central  #>                  6                  5                  8                  7  #> West South Central  #>                  4   # Filter for states in the Pacific division subset(regions_df, Division == \"Pacific\") #>         State State.Code Region Division #> 1      Alaska         AK   West  Pacific #> 5  California         CA   West  Pacific #> 12     Hawaii         HI   West  Pacific #> 38     Oregon         OR   West  Pacific #> 48 Washington         WA   West  Pacific data(us_census_bureau_regions_df) head(us_census_bureau_regions_df) #>        State State.Code Region           Division #> 1     Alaska         AK   West            Pacific #> 2    Alabama         AL  South East South Central #> 3   Arkansas         AR  South West South Central #> 4    Arizona         AZ   West           Mountain #> 5 California         CA   West            Pacific #> 6   Colorado         CO   West           Mountain"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_remove_invalid_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"function reads CSV file containing NPI numbers, validates format using npi package, removes rows missing invalid NPIs.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_remove_invalid_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"","code":"validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_remove_invalid_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"input_data Either dataframe containing NPI numbers path CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_remove_invalid_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"dataframe containing valid NPI numbers.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_remove_invalid_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"","code":"# Example usage: # input_data <- \"~/path/to/your/NPI/file.csv\" # valid_df <- validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"function validates set NPI numbers, removes invalid missing entries, retrieves detailed clinician information valid NPIs. supports dataframe CSV file inputs ensures clean validated output clinician data.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"","code":"validate_and_retrieve_clinician_data(npi_data)"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"npi_data Either dataframe containing NPI numbers (must include column named npi) path CSV file NPI numbers. column npi must contain numeric character representations NPIs exactly 10 digits.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"tibble detailed clinician data valid NPI, including expanded metadata columns.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"function first validates input data ensure contains correctly formatted NPIs. Missing invalid NPIs removed. , valid NPI, function retrieves clinician data using provider package.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/validate_and_retrieve_clinician_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate and Retrieve Clinician Data by NPI — validate_and_retrieve_clinician_data","text":"","code":"# Example 1: Validate and retrieve clinician data from a dataframe if (FALSE) { # \\dontrun{ npi_data_frame <- tibble::tibble(npi = c(\"1234567890\", \"1689603763\", \"invalid_npi\", NA)) clinician_data <- validate_and_retrieve_clinician_data(npi_data_frame) print(clinician_data) } # }  # Example 2: Validate and retrieve clinician data from a CSV file if (FALSE) { # \\dontrun{ npi_csv_path <- \"npi_data.csv\" npi_data_frame <- tibble::tibble(npi = c(\"1234567890\", \"1689603763\", \"invalid_npi\")) readr::write_csv(npi_data_frame, npi_csv_path) clinician_data <- validate_and_retrieve_clinician_data(npi_csv_path) print(clinician_data) } # }  # Example 3: Handle a large dataset of NPIs if (FALSE) { # \\dontrun{ large_npi_data <- tibble::tibble(npi = sprintf(\"%010d\", sample(1e9, 1000))) clinician_data <- validate_and_retrieve_clinician_data(large_npi_data) print(dim(clinician_data)) } # }"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NPI Numbers for Given Names — vc","title":"Search NPI Numbers for Given Names — vc","text":"function searches NPI (National Provider Identifier) numbers based given first last names. can accept input data form dataframe, CSV file, RDS file columns named 'first' 'last' representing first names last names, respectively.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NPI Numbers for Given Names — vc","text":"","code":"vc"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Search NPI Numbers for Given Names — vc","text":"object class character length 97.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NPI Numbers for Given Names — vc","text":"input_data dataframe, file path CSV, RDS file containing first last names.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NPI Numbers for Given Names — vc","text":"dataframe containing NPI numbers provided names match specified taxonomies.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/vc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search NPI Numbers for Given Names — vc","text":"","code":"# Input as a dataframe input_df <- data.frame(   first = c(\"John\", \"Jane\", \"Alice\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") ) npi_results <- search_npi(input_df) #> Error in search_npi(input_df): could not find function \"search_npi\"  # Input as a CSV file input_csv <- \"path/to/input.csv\" npi_results <- search_npi(input_csv) #> Error in search_npi(input_csv): could not find function \"search_npi\""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/women_by_zip.html","id":null,"dir":"Reference","previous_headings":"","what":"Total Women by ZIP Code — women_by_zip","title":"Total Women by ZIP Code — women_by_zip","text":"dataset containing total number women ZIP Code Tabulation Area (ZCTA) United States based American Community Survey (ACS) 5-Year Estimates.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/women_by_zip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Total Women by ZIP Code — women_by_zip","text":"","code":"women_by_zip"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/women_by_zip.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Total Women by ZIP Code — women_by_zip","text":"data frame two columns: zip_code ZIP Code Tabulation Area (ZCTA). total_women estimated total number women ZCTA.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/women_by_zip.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Total Women by ZIP Code — women_by_zip","text":"U.S. Census Bureau, ACS 5-Year Estimates, 2021.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/women_by_zip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Total Women by ZIP Code — women_by_zip","text":"","code":"data(women_by_zip) head(women_by_zip) #> # A tibble: 6 × 4 #>   zip_code NAME        variable   total_women #>   <chr>    <chr>       <chr>            <dbl> #> 1 00601    ZCTA5 00601 B01001_026        8497 #> 2 00602    ZCTA5 00602 B01001_026       19237 #> 3 00603    ZCTA5 00603 B01001_026       25262 #> 4 00606    ZCTA5 00606 B01001_026        2867 #> 5 00610    ZCTA5 00610 B01001_026       13225 #> 6 00611    ZCTA5 00611 B01001_026         648"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"function writes data frame CSV file specified output directory. includes detailed logging inform user function's progress potential issues. function writes data frame CSV file specified output directory. includes detailed logging inform user function's progress potential issues.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"","code":"write_output_csv(   df,   filename,   output_dir = \"ortho_sports_med/Figures\",   verbose = TRUE )  write_output_csv(   df,   filename,   output_dir = \"ortho_sports_med/Figures\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"df data frame written CSV file. filename string specifying name output file (.csv extension). output_dir string specifying directory CSV file saved. Default \"ortho_sports_med/Figures\". verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"NULL. function saves CSV file specified location. NULL. function saves CSV file specified location.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"Directory Creation: specified output directory exist, function attempts create . Logging: Verbose logging informs user progress potential errors writing process. Error Handling: function handles errors invalid input types directory creation failures.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"key-features-of-the-function--1","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"Directory Creation: specified output directory exist, function attempts create . Logging: Verbose logging informs user progress potential errors writing process. Error Handling: function handles errors invalid input types directory creation failures.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/write_output_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"","code":"# Example 1: Save a data frame to the default directory with detailed logging df <- data.frame(Name = c(\"John\", \"Jane\"), Age = c(30, 25)) write_output_csv(df, \"output.csv\") #> File successfully saved to: ortho_sports_med/Figures/output.csv   # Example 2: Save a data frame to a custom directory without logging df <- data.frame(Product = c(\"Apple\", \"Banana\"), Price = c(1.2, 0.5)) write_output_csv(df, \"output.csv\", output_dir = \"custom/directory\", verbose = FALSE)  # Example 3: Save a large data frame with detailed logging df_large <- data.frame(ID = 1:1000, Value = rnorm(1000)) write_output_csv(df_large, \"large_output.csv\", output_dir = \"data/outputs\", verbose = TRUE) #> File successfully saved to: data/outputs/large_output.csv   # Example 1: Save a data frame to the default directory with detailed logging df <- data.frame(Name = c(\"John\", \"Jane\"), Age = c(30, 25)) write_output_csv(df, \"output.csv\") #> File successfully saved to: ortho_sports_med/Figures/output.csv   # Example 2: Save a data frame to a custom directory without logging df <- data.frame(Product = c(\"Apple\", \"Banana\"), Price = c(1.2, 0.5)) write_output_csv(df, \"output.csv\", output_dir = \"custom/directory\", verbose = FALSE)  # Example 3: Save a large data frame with detailed logging df_large <- data.frame(ID = 1:1000, Value = rnorm(1000)) write_output_csv(df_large, \"large_output.csv\", output_dir = \"data/outputs\", verbose = TRUE) #> File successfully saved to: data/outputs/large_output.csv"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":null,"dir":"Reference","previous_headings":"","what":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"dataset provides Rural-Urban Commuting Area (RUCA) codes related rural/urban classifications U.S. ZIP codes, including information primary secondary RUCA designations AHRQ rural vs. urban status.","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"","code":"zip_to_ruca"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"tibble 35,648 rows 10 variables: zip ZIP code (character) USPS_ZIP_PREF_CITY USPS preferred city name ZIP code state_name Full state territory name AHRQ_rural_vs_urban Rural/urban classification according AHRQ standards (e.g., \"Urban\", \"Large rural town\", \"Small rural town\") primary_ruca_desc Description primary RUCA classification primary_ruca Numeric code primary RUCA classification (1-10) secondary_ruca Numeric code secondary RUCA classification (1-10) secondary_ruca_desc Description secondary RUCA classification tract_fips Census tract FIPS code res_ratio Residential ratio within ZIP code area","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"USDA Economic Research Service AHRQ rural-urban classifications https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes/","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"Rural-Urban Commuting Area (RUCA) codes census tract-based classification scheme utilizes Bureau Census urbanized area urban cluster definitions work commuting information characterize nation's census tracts regarding rural urban status relationships. dataset particularly useful : Healthcare access delivery studies Rural health research Geographic analysis healthcare disparities Policy planning rural urban healthcare services","code":""},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/reference/zip_to_ruca.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rural-Urban Commuting Area (RUCA) Codes by ZIP Code — zip_to_ruca","text":"","code":"# Load the RUCA dataset data(zip_to_ruca)  # View distribution of AHRQ rural/urban classifications dplyr::count(zip_to_ruca, AHRQ_rural_vs_urban) #> # A tibble: 5 × 2 #>   AHRQ_rural_vs_urban     n #>   <chr>               <int> #> 1 Isolated rural       5488 #> 2 Large rural town     3914 #> 3 Small rural town     2426 #> 4 Urban               15741 #> 5 NA                   8079  # Find all ZIP codes classified as \"Large rural town\" large_rural <- zip_to_ruca %>%   dplyr::filter(AHRQ_rural_vs_urban == \"Large rural town\")  # Calculate average residential ratio by primary RUCA code zip_to_ruca %>%   dplyr::group_by(primary_ruca, primary_ruca_desc) %>%   dplyr::summarise(     avg_res_ratio = mean(res_ratio, na.rm = TRUE),     n = n()   ) #> `summarise()` has grouped output by 'primary_ruca'. You can override using the #> `.groups` argument. #> # A tibble: 11 × 4 #> # Groups:   primary_ruca [11] #>    primary_ruca primary_ruca_desc             avg_res_ratio     n #>           <dbl> <chr>                                 <dbl> <int> #>  1            1 Urban area                            0.400 10203 #>  2            2 Large rural town                      0.473  4943 #>  3            3 Small rural town                      0.547   595 #>  4            4 Isolated rural area                   0.455  1228 #>  5            5 Urban commuting area                  0.519  2239 #>  6            6 Small rural commuting area            0.556   447 #>  7            7 Large rural commuting area            0.475   892 #>  8            8 Isolated rural commuting area         0.573  1094 #>  9            9 Urban cluster                         0.534   440 #> 10           10 Remote rural area                     0.581  5488 #> 11           NA NA                                  NaN      8079"},{"path":"https://mufflyt.github.io/tyler/mysteryshopper/news/index.html","id":"tyler-0009000","dir":"Changelog","previous_headings":"","what":"tyler 0.0.0.9000","title":"tyler 0.0.0.9000","text":"November 1, 2024 * narrowed focus package include mystery caller study functions. mapping isochrone functions kept separately. November 1, 2024 started better naming conventions named part project used: “nppes_” may helpful processing NPI file using duck database “phase0_” - Able gather data clean (genderize, standardize phone numbers, geocode) “clean_phase_1” - Sets confirmation calls. “phase2_” - Sets REDCap database inputs, quality control. “results_section_” - Creates prose results section. “figure_” - plotting data results section. “map_” - mapping data dot map honeycomb plot region. November 2, 2024 * continue struggle issues getting pkgdown website post github. looks like related use exploratory::statecode() function may public exportable @export? * hard time getting phase1 vignette post.","code":""}]
