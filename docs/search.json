[{"path":"https://mufflyt.github.io/tyler/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Tyler Muffly Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Gathering Drive Time Isochrones","text":"create_isochrones_for_dataframe function powerful tool allows calculate isochrones given location using hereR package. Isochrones represent areas can reached specific point within certain travel time distance. visual representations valuable various applications, location analysis, logistics, transportation planning. guide, walk use create_isochrones function.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"geodesic-versus-drive-time-for-patient-travel","dir":"Articles","previous_headings":"","what":"Geodesic versus Drive-Time for Patient Travel","title":"Gathering Drive Time Isochrones","text":"methods calculating patient travel distance hospitals can vary significantly. paper aims provide overview different methods characteristics. primary factor influencing travel distance calculations choice distance measure, specifically, whether ’s driving distance straight-line distance. distinction significant impact results.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"straight-line-distance","dir":"Articles","previous_headings":"Geodesic versus Drive-Time for Patient Travel","what":"Straight-Line Distance","title":"Gathering Drive Time Isochrones","text":"common practice AHRQ calculate shortest “straight-line” distance (geodetic great circle distance) patient location point care (e.g., hospital emergency department). method favored can easily computed using statistical software like SAS®. Agency Healthcare Research Quality (AHRQ): AHRQ employs equation convert straight-line distance drive time. equation includes various parameters like baseline distance, census division dummy variables, urban/rural location dummy variables, error terms. AHRQ utilizes ggmap package geocode addresses hospitals. AHRQ also considers alternative metric, driving distance driving times. can obtained various mapping software Google Maps, MapQuest, OpenStreetMaps, ArcGIS Network Analyst. AHRQ uses patient location geographic centroid patient’s zip code. (https://hcup-us.ahrq.gov/reports/methods/MS2021-02-Distance--Hospital.jsp) March Dimes Maternity Care Deserts: organization also uses drive time metric calculating travel distance. Reference ESRI Methodology: ESRI methodology creating drive-time areas, certain limitations travel times distances. Reference. Limitations: “must granted network analysis privilege use Create Drive-Time Areas.”, “Travel times exceed 9 hours (540 minutes) walking 5 hours (300 minutes) travel times.”, * “Travel distances exceed 27 miles (43.45 kilometers) walking 300 miles (482.8 kilometers) travel distances.” Veteran’s Administration: Veteran’s Administration utilizes drive time calculations. Reference Department Transportation: Department Transportation provides tools distance calculations. Reference","code":"`Di=αBi+Ciβ+ LiΥ + εi`   Where:  - i indexes patients - Di : driving distance - Bi : baseline distance - Ci : vector of census division dummy variables - Li : vector of urban/rural location dummy variables - α : coefficient for baseline distance - β : vector of coefficients for census division dummy variables - Υ : vector of coefficients for urban/rural location dummy variables - εi : mean-zero error term"},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"potential-references-comparing-drive-time-vs--geodesic","dir":"Articles","previous_headings":"","what":"Potential References comparing Drive Time vs. Geodesic","title":"Gathering Drive Time Isochrones","text":"Lidsky , Sun Z, Nussbaum DP, Adam MA, Speicher PJ, Blazer DG. “Going extra mile: improved survival pancreatic cancer patients traveling high-volume centers.” Annals Surgery. 2017;266(2):333–8. Bliss RL, Katz JN, Wright EA, Losina E. “Estimating proximity care: straight line zipcode centroid distances acceptable measures?” Medical Care. 2012;50(1):99–106. isprs-archives-XLVIII-4-W7-2023-53-2023.pdf comprehensive overview highlights diversity methods used calculate patient travel distance hospitals potential impact healthcare outcomes.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"prerequisites","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic","what":"Prerequisites","title":"Gathering Drive Time Isochrones","text":"start using create_isochrones function, make sure completed following steps: API Key: need API key. don’t one, can obtain Developer Portal. Environment Variable: Set API key environment variable named HERE_API_KEY. essential secure access services. Load tyler package: Ensure load tyler package, contains create_isochrones function.","code":"library(tyler) ## Warning: replacing previous import 'maps::map' by 'purrr::map' when loading ## 'tyler'"},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"usage","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic","what":"Usage","title":"Gathering Drive Time Isochrones","text":"Now prerequisites place, let’s explore use create_isochrones_for_dataframe function. use API calculate optimal routes directions various modes transportation, including driving, walking, cycling, public transit. provides detailed turn--turn instructions, estimated travel times, route alternatives. simpler using OSRM server running AWS cloud, cost minimal.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"input-parameters","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic > Usage","what":"Input Parameters","title":"Gathering Drive Time Isochrones","text":"create_isochrones function accepts following parameters: * location: sf object representing location isolines calculated. Need separate lat long columns. * range: numeric vector time ranges seconds. time ranges determine extent isolines. * posix_time: POSIXct object representing date time calculation. default set “2023-10-20 08:00:00”. chose date Influenza season people see physicians first appointment day 0800. may need split geometry column separate lat long columns using code: join postmastr file postmastr_clinician_data.csv geocoded results file geocoded_data_to_match_house_number. API allow pass master ID number API data washed geocoding. postmastr package allows parse addresses clinician_data can match addresses together based : state, house number, zip code. done exploratory.io read back Gathering data.R.","code":"geocoded_data1 <- geocoded_data %>%         dplyr::mutate(lat = sf::st_coordinates(.)[, \"Y\"],                long = sf::st_coordinates(.)[, \"X\"])  readr::write_csv(geocoded_data1, \"/NPPES_November_filtered_data_for_geocoding_geocoded_addresses_not_sf.csv\") inner_join(`geocoded_data_to_match_house_number`, by = join_by(   `postmastr.pm.state` == `here.state_code`,    `postmastr.pm.zip` == `here.postal_code`,    `postmastr.pm.house` == `here.house_number`)) inner_join_postmastr_clinician_data <- readr::read_csv(\"data/inner_join_postmastr_clinician_data.csv\")  %>%   st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326) %>%   dplyr::mutate(geometry = location)    create_isochrones_for_dataframe(inner_join_postmastr_clinician_data_sf, range = c(30*60, 60*60, 120*60, 180*60))"},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"example","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic > Usage","what":"Example","title":"Gathering Drive Time Isochrones","text":"added code needed read RDS, xlsx, xls, csv files. can also read sf files. needed column called ‘location’ another called geometry. ’s example use create_isochrones_for_dataframe function:","code":"input_file <- \"data/inner_join_postmastr_clinician_data.csv\" isochrones_data <- create_isochrones_for_dataframe(input_file, breaks = c(30*60, 60*60, 120*60, 180*60))  > isochrones_data [1] 1 splay setup instructions: To create isochrones for a specific point(s) use the following code: tryLocationMemo(location = location, range = c(1800, 3600, 7200, 10800)) Setting up the hereR access... Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 2.8 Kb Isoline successfully produced for range: 1800 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 3.1 Kb Isoline successfully produced for range: 3600 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 4.8 Kb Isoline successfully produced for range: 7200 seconds Sending 1 request(s) with unlimited RPS to: 'https://isoline.router.hereapi.com/v8/isolines?...' Received 1 response(s) with total size: 6.9 Kb Isoline successfully produced for range: 10800 seconds"},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"output","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic > Usage","what":"Output","title":"Gathering Drive Time Isochrones","text":"function returns list isolines different time ranges. isoline represented sf object, making easy visualize analyze. create_isochrones function wrapped memoise nice job caching data. note, none columns feed function come side going API. Therefore, hoping 1:1 relationship rows isochrones. may need mark column different time feed API pseudo-identifier.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"conclusion","dir":"Articles","previous_headings":"Potential References comparing Drive Time vs. Geodesic","what":"Conclusion","title":"Gathering Drive Time Isochrones","text":"create_isochrones function simplifies process calculating isolines location-based analysis. Whether ’re exploring accessibility, optimizing routes, conducting spatial analysis, isochrones provide valuable insights travel times distances. tyler package create_isochrones function, can streamline location-based workflows make informed decisions.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/create_isochrones.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Gathering Drive Time Isochrones","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Geocoding","text":"geocode function designed help geocode datasets containing addresses change lattitude longitude.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"installation","dir":"Articles","previous_headings":"Overview > Step 1","what":"Installation","title":"Geocoding","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":"library(tyler)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"understanding-geocoding","dir":"Articles","previous_headings":"Example Usage","what":"Understanding Geocoding","title":"Geocoding","text":"Certainly! Geocoding, process converting addresses place names geographic coordinates (latitude longitude), advantages disadvantages. ’s overview pluses minuses geocoding: Pluses Geocoding: Location Accuracy: Geocoding provides precise location information, allowing pinpoint addresses places map high accuracy. crucial various applications mapping, navigation, location-based services. Spatial Analysis: Geocoded data enables spatial analysis, allowing perform tasks like proximity analysis, spatial clustering, spatial interpolation. ’s invaluable geographic information systems (GIS) geographic research. Geographic Visualization: Geocoded data can visualized maps, making easier understand communicate spatial patterns trends. particularly useful data presentation decision-making. Routing Navigation: Geocoding essential navigation systems, delivery route optimization, location-based apps provide directions estimated travel times. Minuses Geocoding: Data Quality Issues: Geocoding accuracy heavily relies quality underlying address data. Inaccurate outdated address information can lead geocoding errors. Costs: Geocoding services software often come associated costs, particularly large-scale geocoding operations. costs can include data licensing fees usage charges. Complexity: Advanced geocoding tasks, reverse geocoding (converting coordinates addresses) batch geocoding, can technically complex may require expertise specialized tools. summary, geocoding offers numerous benefits terms location accuracy, spatial analysis, visualization, navigation. However, also comes challenges related data quality, costs, complexity. Careful consideration factors essential using geocoding various applications. geocode Zip codes several issues limitations associated geocoding solely based zip codes: Lack Precision: Zip codes designed cover group addresses area, specific points. Therefore, geocoding based solely zip code provides approximation location, often center centroid zip code area. lack precision can problematic applications require accurate coordinates. Zip Code Boundaries: Zip code boundaries can irregular may align natural administrative boundaries. means geocoding based zip codes can result coordinates reflect actual geography area, leading inaccuracies. Zip Code Changes: Zip code boundaries assignments can change time due population growth, urban development, administrative reasons. Geocoding based outdated zip code data can lead incorrect locations. Large Zip Codes: zip codes cover vast geographic areas, especially rural regions. Geocoding center large zip code areas can highly inaccurate specific locations within area. Overlapping Zip Codes: cases, zip codes may overlap one another. Geocoding based solely zip code may distinguish overlapping areas, leading ambiguity. Urban Density: densely populated urban areas, zip codes can small densely packed addresses. Geocoding solely zip code may still result lack precision trying identify particular location within zip code.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Prepare Your Data","title":"Geocoding","text":"can provide data either dataframe CSV file argument input_data. hood ggmap::geocode accessing Google API. ggmap::geocode program AHRQ uses. data CSV file, pass file path input_data parameter. Sometimes data needs separate lat long columns. code can : match geocoded_data orginal dataframe? can use postmastr package allows addresses dataframes parsed house number, street, state, etc. individual parts can join together. seems janky af. postmast’s functionality rests order operations must followed ensure correct parsing: prep postal code state city unit house number ranged house number fractional house number house suffix street directionals street suffix street name reconstruct","code":"output_data <- geocode_unique_addresses(     file_path =\"address_for_geocoding.csv\",     google_maps_api_key = \"123\",     output_file_path = \"data/geocoded_unique_addresses.csv\") geocoded_data1 <- geocoded_data %>%         dplyr::mutate(lat = sf::st_coordinates(.)[, \"Y\"],                long = sf::st_coordinates(.)[, \"X\"]) # install.packages(\"remotes\")       # remotes::install_github(\"slu-openGIS/postmastr\")       library(postmastr)       abc <- read_csv(csv_file)       abc %>% pm_identify(var = \"address\") -> sushi2       sushi2_min <- pm_prep(sushi2, var = \"address\", type = \"street\")       pm_postal_all(sushi2_min)       sushi2_min <- pm_postal_parse(sushi2_min)       moDict <- pm_dictionary(locale = \"us\", type = \"state\", case = c(\"title\", \"upper\"))       moDict       pm_state_all(sushi2_min, dictionary = moDict) #Checks to make sure that all states have matches       sushi2_min <- pm_state_parse(sushi2_min, dictionary = moDict)       pm_house_all(sushi2_min)       sushi2_min <- pm_house_parse(sushi2_min)       sushi2_min <- pm_streetDir_parse(sushi2_min)       sushi2_min <- pm_streetSuf_parse(sushi2_min)       sushi2_min <- pm_street_parse(sushi2_min, ordinal = TRUE, drop = TRUE)       sushi2_parsed <- pm_replace(sushi2_min, source = sushi2)       readr::write_csv(sushi2_parsed, \"~/Dropbox (Personal)/workforce/Master_References/NPPES/NPPES_November_filtered_data_address_parsed.csv\")"},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Geocoding","text":"validate_and_remove_invalid_npi function handy tool cleaning validating datasets NPI numbers. following steps outlined vignette, can ensure data contains valid NPIs analysis processing.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/geocode.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Geocoding","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Data from the US Census Bureau for Isochrones","text":"vignette demonstrates usage get_census_data function, designed retrieve Census data states’ block groups. leverages censusapi package query U.S. Census Bureau’s API collect demographic information specified state FIPS codes. ’ll get population block group using censusapi library relies heavily vignette.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"centers-for-medicare-and-medicaid-services-doctors-and-clinicians-downloadable-file","dir":"Articles","previous_headings":"Introduction","what":"Centers for Medicare and Medicaid Services Doctors and Clinicians Downloadable file","title":"Getting Data from the US Census Bureau for Isochrones","text":"Downloadable File housed CMS Medicare Compare (aka Physician Compare site): CMS Medicare Compare. downloaded full data set file left join runs risk date give data update monthly. data dictionary file: CMS Medicare Compare Data Dictionar. Doctors Clinicians national downloadable file organized individual clinician level; line unique clinicianenrollment record-group-address (NPI-Ind_enrl_ID-Org_PAC_ID-adrs_id) level. Clinicians multiple Medicare enrollment records /single enrollments linking multiple practice locations listed multiple lines.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"state-federal-information-processing-standards-codes","dir":"Articles","previous_headings":"Introduction","what":"State Federal Information Processing Standards Codes","title":"Getting Data from the US Census Bureau for Isochrones","text":"function retrieves Census data using censusapi states’ block groups looping specified list state FIPS codes. brings back data females “B01001_01, 26, 33:49E”. FIPS codes, Federal Information Processing Standards codes, standardized set codes used uniquely identify geographic areas United States. codes assigned various administrative geographical entities, states, counties, cities, . used block groups analysis. GEOID block groups United States can constructed using following format: STATECOUNTYTRACTBLOCK_GROUP. Specifically: * STATE 2-digit code state. * COUNTY 3-digit code county. * TRACT 6-digit code census tract. * BLOCK_GROUP 1-digit code block group within tract.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"block-group-as-the-unit-of-measurement","dir":"Articles","previous_headings":"Introduction","what":"Block Group as the Unit of Measurement","title":"Getting Data from the US Census Bureau for Isochrones","text":"United States Census Bureau’s geographic hierarchy, “block group” smaller detailed geographic unit used collecting reporting demographic statistical data. Block groups subdivisions census tracts typically designed contain 600 3,000 people, although can vary depending population density area. Census block group borders defined based visible easily identifiable features roads, rivers, streets, natural boundaries like mountains parks. Census Bureau aims create block group boundaries follow features make easily distinguishable. Block groups used primary units collecting detailed demographic socioeconomic data decennial census American Community Survey (ACS). Census enumerators visit households within block group collect information population, housing, employment, income, education, . densely populated urban area, block group might represent city block small neighborhood within larger city. example, block group cover city blocks downtown Manhattan, New York City.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"data-from-the-us-census-bureau","dir":"Articles","previous_headings":"","what":"Data From the US Census Bureau","title":"Getting Data from the US Census Bureau for Isochrones","text":"variables part dataset obtained U.S. Census Bureau’s American Community Survey (ACS). U.S. Census Bureau’s American Community Survey (ACS) ongoing nationwide survey conducted United States Census Bureau. designed collect provide detailed demographic, social, economic, housing information American population. key features aspects ACS: Continuous Survey: ACS conducted continuously throughout year, providing updated current data. Unlike decennial census, occurs every ten years, ACS conducted annually, allowing frequent timely information. Sampling: ACS uses sample-based approach collect data representative subset U.S. population. sample includes households individuals 50 states, District Columbia, Puerto Rico. Questionnaire: Respondents asked complete detailed questionnaire covers wide range topics, including demographics (age, sex, race, etc.), housing characteristics, education, employment, income, health insurance, . Geographic Coverage: ACS provides data various geographic levels, including national, state, county, city, town, even census tract block group. allows detailed analysis communities regions. Data Release: ACS releases data various forms, including one-year estimates, three-year estimates, five-year estimates. One-year estimates available areas larger populations, three-year five-year estimates designed smaller areas subpopulations. five-year estimates provide reliable data small geographic areas specific demographic groups. Accessibility: ACS data publicly accessible can accessed Census Bureau’s website, data.census.gov, data dissemination platforms. Researchers, policymakers, businesses, general public use ACS data various purposes, including policy development, market research, community planning. Importance: ACS critical tool understanding changing demographics socio-economic characteristics U.S. population. used congressional apportionment, resource allocation, grant distribution, various research purposes. Privacy Confidentiality: Census Bureau takes privacy confidentiality seriously. Personal information collected ACS questionnaire protected law, responses aggregated ensure individual respondents identified. Census Long Form Replacement: ACS introduced replace long-form questionnaire part decennial census. long-form collected detailed demographic housing information, ACS continues provide valuable data ongoing basis. represent demographic information block groups within various states. ’s explanation variable: name: variable represents name label block group. total_females: represents total number females block group. female_21_yo: variable represents number females aged 21 years older block group. female_22_to_24_years: represents number females aged 22 24 years block group. female_25_to_29_years: variable represents number females aged 25 29 years block group. female_30_to_34_years: represents number females aged 30 34 years block group. etc. Eventually data matched onto Block Groups. block group shapefile 2021 ACS via National Historical Geographic Information System (NHGIS). calculate many people live within outside drive time isochrones, ’ll need identify percent Census block group lies within isochrones.","code":"name = NAME,     total_females = B01001_026E,     female_21_yo = B01001_033E,     female_22_to_24_years = B01001_034E,     female_25_to_29_years = B01001_035E,     female_30_to_34_years = B01001_036E,     female_35_to_39_years = B01001_037E,     female_40_to_44_years = B01001_038E,     female_45_to_49_years = B01001_039E,     female_50_to_54_years = B01001_040E,     female_55_to_59_years = B01001_041E,     female_60_to_61_years = B01001_042E,     female_62_to_64_years = B01001_043E,     female_65_to_66_years = B01001_044E,     female_67_to_69_years = B01001_045E,     female_70_to_74_years = B01001_046E,     female_75_to_79_years = B01001_047E,     female_80_to_84_years = B01001_048E,     female_85_years_and_older = B01001_049E,     fips_state = state"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"function-description","dir":"Articles","previous_headings":"","what":"Function Description","title":"Getting Data from the US Census Bureau for Isochrones","text":"get_census_data function retrieves Census data states’ block groups. ’s brief description parameters: us_fips: vector state FIPS (Federal Information Processing Standards) codes. code uniquely identifies U.S. state. example, Colorado represented FIPS code 08. resulting data combined single dataframe analysis.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"installation","dir":"Articles","previous_headings":"Function Description > Step 1","what":"Installation","title":"Getting Data from the US Census Bureau for Isochrones","text":"using tyler::get_census_data function, need install load required packages. can running following code: lists contain metadata general variables variables related race ethnicity.","code":"# Install and load the necessary packages install.packages(\"censusapi\") library(censusapi) library(dplyr) library(tyler)"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"all-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"All Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B01001\") %>%        readr::write_csv(\"data/acs_vars.csv\")        # This code cleans it up a bit acs_vars <- acs_vars %>%   dplyr::select(-predicateType, -group, -limit, -predicateOnly) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"!!Male:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"Annotation of Margin of Error\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Annotation of Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(label, fixed(\"Margin of Error!!\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Annotation of Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::mutate(label = stringr::str_remove(label, regex(\"^Estimate!!Total:!!Female:!!\", ignore_case = TRUE))) %>%   dplyr::filter(!stringr::str_detect(name, fixed(\"EA\")) & !str_detect(label, fixed(\"!!Male:\"))) %>%   dplyr::mutate(numbers = purrr::map_chr(str_extract_all(label, \"^[:digit:]+\"), ~ ifelse(length(.) == 0, NA_character_, paste(.x, collapse = \"\")))) %>%   dplyr::mutate(numbers = as.numeric(numbers)) %>%   dplyr::mutate(numbers = tidyr::replace_na(numbers, 0)) %>%   dplyr::mutate(numbers = as.numeric(numbers)) %>%   dplyr::arrange(numbers)    > acs_vars           name                     label    concept numbers 1  B01001_026E Estimate!!Total:!!Female: SEX BY AGE       0 2  B01001_027E             Under 5 years SEX BY AGE       0 3  B01001_001E          Estimate!!Total: SEX BY AGE       0 4  B01001_028E              5 to 9 years SEX BY AGE       5 5  B01001_029E            10 to 14 years SEX BY AGE      10"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"race-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"Race Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_race_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B02001\") %>%       readr::write_csv(\"data/acs_race_vars.csv\")  #output: > acs_race_vars # A tibble: 40 × 7    name         label                                                      concept predicateType group limit predicateOnly    <chr>        <chr>                                                      <chr>   <chr>         <chr> <dbl> <lgl>          1 B02001_010EA Annotation of Estimate!!Total:!!Two or more races:!!Two r… RACE    string        B020…     0 TRUE           2 B02001_010MA Annotation of Margin of Error!!Total:!!Two or more races:… RACE    string        B020…     0 TRUE           3 B02001_001EA Annotation of Estimate!!Total:                             RACE    string        B020…     0 TRUE           4 B02001_001MA Annotation of Margin of Error!!Total:                      RACE    string        B020…     0 TRUE           5 B02001_004EA Annotation of Estimate!!Total:!!American Indian and Alask… RACE    string        B020…     0 TRUE           6 B02001_004MA Annotation of Margin of Error!!Total:!!American Indian an… RACE    string        B020…     0 TRUE           7 B02001_005EA Annotation of Estimate!!Total:!!Asian alone                RACE    string        B020…     0 TRUE           8 B02001_005MA Annotation of Margin of Error!!Total:!!Asian alone         RACE    string        B020…     0 TRUE           9 B02001_002EA Annotation of Estimate!!Total:!!White alone                RACE    string        B020…     0 TRUE          10 B02001_002MA Annotation of Margin of Error!!Total:!!White alone         RACE    string        B020…     0 TRUE"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"race-and-ethnicity-variables","dir":"Articles","previous_headings":"Function Description > Step 1 > Installation","what":"Race and Ethnicity Variables","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"acs_raceeth_vars <- censusapi::listCensusMetadata(name = \"acs/acs5\",        vintage = 2019, group = \"B03002\") %>%       readr::write_csv(\"data/acs_raceeth_vars.csv\")"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Function Description","what":"Step 2: Prepare Your Data","title":"Getting Data from the US Census Bureau for Isochrones","text":"Define vector state FIPS codes. example, can use tigris package obtain FIPS codes U.S. states:","code":"us_fips_list <- tigris::fips_codes %>%     dplyr::select(state_code, state_name) %>%     dplyr::distinct(state_code, .keep_all = TRUE) %>%     filter(state_code < 56) %>%                         #state_codes over 56 are territories     dplyr::select(state_code) %>%     dplyr::pull()                                                # All US State FIPS Codes us_fips_list <- c(\"01\", \"02\", \"04\", \"05\", \"06\", \"08\", \"09\", \"10\", \"11\", \"12\", \"13\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\", \"21\", \"22\", \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\", \"33\", \"34\", \"35\", \"36\", \"37\", \"38\", \"39\", \"40\", \"41\", \"42\", \"44\", \"45\", \"46\", \"47\", \"48\", \"49\", \"50\", \"51\", \"53\", \"54\", \"55\")"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"step-3-gather-the-data-from-the-us-census-bureau-api","dir":"Articles","previous_headings":"Function Description","what":"Step 3: Gather the Data from the US Census Bureau API","title":"Getting Data from the US Census Bureau for Isochrones","text":"Call get_census_data function us_fips_list vector. example:","code":"all_census_data <- get_census_data(us_fips = us_fips_list)  ########################################################################## # Get Census data by block group in relevant states # Construct: for=block group:*&in=state:01&in=county:*&in=tract:* ###########################################################################  #output GOES HERE!!!!"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"step-4-the-function-will-retrieve-census-data-for-all-specified-states-and-combine-it-into-a-single-dataframe-which-you-can-use-for-further-analysis-","dir":"Articles","previous_headings":"Function Description","what":"Step 4: The function will retrieve Census data for all specified states and combine it into a single dataframe, which you can use for further analysis.","title":"Getting Data from the US Census Bureau for Isochrones","text":"","code":"demographics_bg <- acs_block_group %>%   rename(     name = NAME,     total_females = B01001_026E,     female_21_yo = B01001_033E,     female_22_to_24_years = B01001_034E,     female_25_to_29_years = B01001_035E,     female_30_to_34_years = B01001_036E,     female_35_to_39_years = B01001_037E,     female_40_to_44_years = B01001_038E,     female_45_to_49_years = B01001_039E,     female_50_to_54_years = B01001_040E,     female_55_to_59_years = B01001_041E,     female_60_to_61_years = B01001_042E,     female_62_to_64_years = B01001_043E,     female_65_to_66_years = B01001_044E,     female_67_to_69_years = B01001_045E,     female_70_to_74_years = B01001_046E,     female_75_to_79_years = B01001_047E,     female_80_to_84_years = B01001_048E,     female_85_years_and_older = B01001_049E,     fips_state = state   ) %>%   mutate(     fips_county = paste0(fips_state, county),     fips_tract = paste0(fips_state, county, tract),     fips_block_group = paste0(       fips_state,       county,       str_pad(tract, width = 6, pad = \"0\"),       block_group     )   ) %>%   mutate(     population = female_21_yo + female_22_to_24_years + female_25_to_29_years +       female_30_to_34_years + female_35_to_39_years + female_40_to_44_years +       female_45_to_49_years +       female_50_to_54_years +       female_55_to_59_years +       female_60_to_61_years +       female_62_to_64_years +       female_65_to_66_years +       female_67_to_69_years +       female_70_to_74_years +       female_75_to_79_years +       female_80_to_84_years +       female_85_years_and_older   ) %>% #total of reproductive age women   arrange(fips_state) %>%   select(     fips_block_group,     fips_state,     fips_county,     fips_tract,     name,     population,     everything()   ) %>%   select(-starts_with(\"B\"),          -contains(\"universe\"),          -county,          -tract,          -block_group)  colnames(demographics_bg)  demographics_bg <- demographics_bg %>% arrange(fips_block_group) readr::write.csv(demographics_bg, \"data/acs-block-group-demographics.csv\", na = \"\", row.names = F) readr::write_rds(demographics_bg, \"data/acs-block-group-demographics.rds\")"},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"step-5-join-the-data-to-the-block-groups","dir":"Articles","previous_headings":"Function Description","what":"Step 5: Join the Data to the Block Groups","title":"Getting Data from the US Census Bureau for Isochrones","text":"code starts loading block group shapefile using sf::st_read() function. shapefile path replaced actual file path. left join performed “demographics_bg” dataset “bg_shape” dataset using dplyr::left_join(). join based matching “fips_block_group” column “demographics_bg” “GEOID” column “bg_shape”.","code":"# Load the block group shapefile using sf::st_read() function # Replace \"/data/shp/block_group/\" with the actual file path to the shapefile bg_shape <- sf::st_read(/data/shp/block_group/\") %>%      # Remove leading zeros from the GEOID column using stringr::str_remove()   # This is a common step to ensure GEOIDs are consistent      dplyr::mutate(GEOID = stringr::str_remove(GEOID, regex(\"^0\", ignore_case = TRUE))) %>%      # Select only the GEOID and geometry columns from the shapefile   dplyr::select(GEOID, geometry)   # Write the block group shapefile with selected columns to a CSV file # This will create a CSV file with GEOID and geometry information bg_shape %>%   readr::write_csv(\"bg_shape_with_geometry.csv\")   # Convert the \"fips_block_group\" column in the \"demographics_bg\" dataset to character # This is done to ensure compatibility for joining with the GEOID column in the shapefile demographics_bg$fips_block_group <- as.character(demographics_bg$fips_block_group)   # Perform a left join between the demographics dataset and the block group shapefile # Join the datasets using the \"fips_block_group\" column from demographics_bg # and the \"GEOID\" column from bg_shape  geometry <- dplyr::left_join(x = demographics_bg,            y = bg_shape,            by = c(\"fips_block_group\" = \"GEOID\")) # Write the resulting dataset with geometry information to a CSV file # This will create a CSV file containing demographic data and geometry information readr::write_csv(geometry, \"block_groups_with_geometry.csv\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"usage-tips","dir":"Articles","previous_headings":"","what":"Usage Tips","title":"Getting Data from the US Census Bureau for Isochrones","text":"Ensure valid Census API key access data. Replace \"your_census_api_key_here\" actual API key function call. included one second pause function loop mindful rate limiting API usage policies making multiple requests Census Bureau’s API.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Getting Data from the US Census Bureau for Isochrones","text":"get_census_data function simplifies process obtaining Census data states’ block groups.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/get_census_data.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Getting Data from the US Census Bureau for Isochrones","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"tyler::taxonomy data tyler::search_by_taxonomy function R package offers convenient efficient way query NPI Database healthcare providers based taxonomy descriptions. vignette provides comprehensive guide effectively utilize function, explores various capabilities, offers illustrative use cases.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"installation","dir":"Articles","previous_headings":"Overview","what":"Installation","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"understanding-taxonomy-descriptions","dir":"Articles","previous_headings":"Example Usage","what":"Understanding Taxonomy Descriptions","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"Taxonomy descriptions, derived National Physician Taxonomy Codes NPPES (National Plan Provider Enumeration System) database, fundamental components United States healthcare system. play pivotal role identification categorization healthcare providers various purposes, including billing, insurance, regulatory compliance. particular, tyler::taxonomy data frame contains NUCC taxonomy codes utilized NPPES data files. Taxonomy Code comprises unique ten-character identifier aids identification healthcare provider types areas expertise. Notably, OBGYN taxonomy codes sourced Version 23.1 dated July 1, 2023. Taxonomy codes can obtained National Uniform Claim Committee (NUCC) website . can employ codes pinpoint specific taxonomy descriptions search. instance, interested finding taxonomy codes include string \"GYN\" can use code facilitate search search_by_taxonomy function.","code":"obgyn_taxonomy <- tyler::taxonomy %>%    dplyr::filter(str_detect(`Classification`, fixed(\"GYN\", ignore_case = TRUE))) %>%    dplyr::select(Code, Specialization) Code       Specialization                                       <chr>      <chr>                                              1 207V00000X NA                                                 2 207VC0300X Complex Family Planning                            3 207VC0200X Critical Care Medicine                             4 207VF0040X Female Pelvic Medicine and Reconstructive Surgery  5 207VX0201X Gynecologic Oncology                               6 207VG0400X Gynecology                                         7 207VH0002X Hospice and Palliative Medicine                    8 207VM0101X Maternal & Fetal Medicine                          9 207VB0002X Obesity Medicine                                  10 207VX0000X Obstetrics                                        11 207VE0102X Reproductive Endocrinology"},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"search-by-taxonomy-description","dir":"Articles","previous_headings":"Example Usage","what":"Search by Taxonomy Description","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function excels searching NPI Database healthcare providers based taxonomy descriptions. functionality proves invaluable verifying external data regarding subspecialist provider counts filling gaps providers may board-certified actively practicing (board-eligible). data can seamlessly integrated databases, enhancing utility. internal use, can refer \"Exploratory/Workforce/subspecialists_only\". One significant advantage search results include National Provider Identifier (NPI).","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"example-usage-1","dir":"Articles","previous_headings":"Example Usage","what":"Example Usage","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"illustrative example, employ search_by_taxonomy function identify healthcare providers specializing “Hospice Palliative Medicine” based taxonomy descriptions. resulting output dataframe containing information physicians either MD qualification, practicing United States individuals, self-identifying taxonomy “Hospice Palliative Medicine.” can easily view resulting data frame user-friendly format. often need merge rows search_taxonomy data get_clinicians data. steps needed make structure names similar get_clinicians data.","code":"# Search for providers based on taxonomy descriptions taxonomy_descriptions <- c(\"Hospice and Palliative Medicine\")  data <- search_by_taxonomy(taxonomy_to_search = taxonomy_descriptions) #> 1200 records requested #> Requesting records 0-200... #> Requesting records 200-400... #> Requesting records 400-600... #> Requesting records 600-800... #> Requesting records 800-1000... #> Requesting records 1000-1200... > data           npi basic_first_name basic_last_name basic_sole_proprietor basic_gender basic_enumeration_date 1  1437277092         MARIETTA   ABALOS-GALITO                   YES            F             2007-03-26 2  1629034905          ANTHONY        ABBRUZZI                    NO            M             2006-04-25 3  1093806697            AYMAN     ABDEL HALIM                    NO            M             2006-09-27 # View the resulting data frame head(data) all_taxonomy_search_data <- data %>%    distinct(npi, .keep_all = TRUE) %>%      # Keep only the OBGYN subspecialist taxonomy descriptions.     filter(taxonomies_desc %in% c(\"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\", \"Obstetrics & Gynecology, Gynecologic Oncology\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\", \"Obstetrics & Gynecology, Reproductive Endocrinology\")) %>%      # Extract the first five of the zip code.     mutate(addresses_postal_code = str_sub(addresses_postal_code,1 ,5)) %>%   mutate(basic_enumeration_date = ymd(basic_enumeration_date)) %>%      # Pull the year out of the enumeration full data.     mutate(basic_enumeration_date_year = year(basic_enumeration_date), .after = ifelse(\"basic_enumeration_date\" %in% names(.), \"basic_enumeration_date\", last_col())) %>%   mutate(basic_middle_name = str_sub(basic_middle_name,1 ,1)) %>%   mutate(across(c(basic_first_name, basic_last_name, basic_middle_name), .fns = ~str_remove_all(., \"[[\\\\p{P}][\\\\p{S}]]\"))) %>%      # Get data ready to add these taxonomy rows to the `search_npi`/GOBA data set.     rename(NPI = npi, first_name = basic_first_name, last_name = basic_last_name, middle_name = basic_middle_name, GenderPhysicianCompare = basic_gender, sub1 = taxonomies_desc, city = addresses_city, state = addresses_state, name.x = full_name, `Zip CodePhysicianCompare` = addresses_postal_code) %>%   mutate(GenderPhysicianCompare = recode(GenderPhysicianCompare, \"F\" = \"Female\", \"M\" = \"Male\", type_convert = TRUE)) %>%      # Show the subspecialty from goba.     mutate(sub1 = recode(sub1, \"Obstetrics & Gynecology, Female Pelvic Medicine and Reconstructive Surgery\" = \"FPM\", \"Obstetrics & Gynecology, Gynecologic Oncology\" = \"ONC\", \"Obstetrics & Gynecology, Maternal & Fetal Medicine\" = \"MFM\", \"Obstetrics & Gynecology, Reproductive Endocrinology\" = \"REI\", type_convert = TRUE))"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"parameters","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Parameters","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"taxonomy_to_search: character vector contain desired taxonomy description(s) used search criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"output","dir":"Articles","previous_headings":"Example Usage > Function Details","what":"Output","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"function returns data frame filtered include NPI data matching specified taxonomy description(s).","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"search_by_taxonomy function stands wrapper exploring NPI Database taxonomy descriptions. empowers users identify healthcare providers precise specializations, rendering resource healthcare-related research -depth analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/my-vignette.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Searching the NPI Database Starting with Taxonomy Codes","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Search and Process NPI Numbers","text":"search_and_process_npi function tool working datasets containing National Provider Identifier (NPI) numbers. search_and_process_npi wrapper fantastic npi package. Thank authors maintainers npi package. NPI numbers provide standardized way identify track healthcare providers, including physicians, across United States. Government agencies, Centers Medicare & Medicaid Services (CMS), use NPI-based data plan allocate healthcare resources, including provider reimbursements, medical services, workforce distribution. search_and_process_npi allows search NPIs based first last names clinicians start many mystery caller projects getting names patient-facing directory physicians. Getting NPI number unlock multiple demographics physicians (gender, medical school type, address) function like `retrieve_clinician_data``.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"why-use-apis-for-healthcare-data","dir":"Articles","previous_headings":"Overview","what":"Why use APIs for healthcare data?","title":"Search and Process NPI Numbers","text":"Accessing APIs retrieve healthcare data can offer several advantages downloading joining data multiple sources: Real-Time Data: APIs often provide access real-time near-real-time data. Downloading static data files may result using outdated information, APIs can provide latest data becomes available. Data Integrity: APIs typically offer structured validated data. access data via API, can confident quality consistency. contrast, downloading joining data various sources may introduce data integrity issues, missing mismatched records. Efficiency: APIs allow request specific subsets data, reducing amount data transferred processed. can improve efficiency reduce processing time, especially dealing large datasets. Downloading joining entire datasets can time-consuming resource-intensive. Reduced Storage Requirements: Storing large datasets locally can costly terms storage space. Accessing data APIs means don’t need maintain local copy entire dataset, saving storage costs reducing risk data redundancy. Scalability: APIs designed handle high volume requests. Security Privacy: Healthcare data often contains sensitive information, APIs can provide better control data access authentication. Data Source Aggregation: APIs can provide centralized point access data multiple sources. Data Governance: APIs often come documentation usage policies, can help ensure compliance data governance privacy regulations. provides transparency data usage, making easier adhere legal ethical standards. Version Control: APIs versioned, allowing users specify version API want use. ensures backward compatibility provides level stability accessing data. downloading joining data files, version control can challenging. Reduced Maintenance: APIs maintained updated data providers. using APIs, rely provider manage data updates, ensuring always access latest information.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Search and Process NPI Numbers","text":"can use search_and_process_npi function, make sure tyler package installed. can install using following command:","code":"# install.packages(\"tyler\") library(tyler)"},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"example-usage","dir":"Articles","previous_headings":"","what":"Example Usage","title":"Search and Process NPI Numbers","text":"National Provider Identifier Search defaults find individual people (individuals enumeration_type = “ind”), physicians (“MD”, “”) United States listed NPPES.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"step-1-load-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 1: Load Your Data","title":"Search and Process NPI Numbers","text":"can provide data either dataframe specify path CSV, XLS, XLSX file containing data. dataframe must column named first another named last surname. acog_presidents example data set can use case.","code":"# Toy example using a dataframe data_df <- data.frame(   first = c(\"John\", \"Jane\", \"Bob\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") )  # Example using a CSV file input_file <- \"acog_presidents.csv\"  # Note the file must have a column named \"first\" and a column named \"last\"."},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"step-2-call-the-search_and_process_npi-function","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Call the search_and_process_npi Function","title":"Search and Process NPI Numbers","text":"Now, let’s use search_and_process_npi function search NPI numbers based first last names data. cast WIDE net matches specialties.","code":"# Example using a CSV file output_result <- search_and_process_npi(input_file = input_file)"},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"step-3-customize-your-search","dir":"Articles","previous_headings":"Example Usage","what":"Step 3: Customize Your Search","title":"Search and Process NPI Numbers","text":"can customize NPI search specifying parameters enumeration_type, limit, country_code, filter_credentials. Magic numbers :) take long time run. Best run overnight 2,000 searches. acog_president dataframe take 10 minutes. ’s can : worried message API accessed. just means match NAMES.","code":"# Example with custom search parameters result_df <- search_and_process_npi(   input_data = input_file,   enumeration_type = \"ind\",               # Search for individual NPIs   limit = 10,                             # Set the search limit to 10 results per name pair   country_code = \"US\",                    # Filter for NPIs in the United States   filter_credentials = c(\"MD\", \"DO\")      # Filter for specific credentials ) ERROR : `df` must be an npi_results S3 object, not tbl_df."},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"step-4-access-the-results","dir":"Articles","previous_headings":"Example Usage","what":"Step 4: Access the Results","title":"Search and Process NPI Numbers","text":"function return data frame containing processed NPI search results. can access data frame analysis.going lot duplicates need clean thoughtfully.","code":"# Access the result data frame result_df  > output_result               npi basic_first_name basic_last_name basic_middle_name basic_credential basic_sole_proprietor basic_gender     1: 1053601807             RYAN       SCHLUETER            JEWELL               DO                   YES            M     2: 1184186256            LAURA          MARTIN         ELIZABETH               DO                    NO            F     3: 1063703494           LAUREN          BISHOP            ALICIA             M.D.                    NO            F     4: 1740800705           LAUREN          BISHOP         ELISABETH             M.D.                    NO            F             basic_enumeration_date basic_last_updated basic_certification_date basic_status taxonomies_code     1:             2011-04-13         2021-09-30               2021-09-30            A      207VM0101X     2:             2019-04-05         2023-03-16               2023-03-16            A      207P00000X     3:             2011-04-19         2023-03-16               2023-03-16            A      207VE0102X     4:             2020-04-20         2023-07-03               2023-07-03            A      207Q00000X                           taxonomies_taxonomy_group                                     taxonomies_desc taxonomies_state     1: 193400000X - Single Specialty Group  Obstetrics & Gynecology, Maternal & Fetal Medicine               GA     2:                                                                      Emergency Medicine               MS     3:                                     Obstetrics & Gynecology, Reproductive Endocrinology               NY     4:                                                                         Family Medicine               TX                 taxonomies_license taxonomies_primary basic_name_prefix basic_name_suffix     1:              80379               TRUE              <NA>              <NA>     2:              29372               TRUE               Dr.              <NA>     3:          302927-01               TRUE               Dr.              <NA>     4:              U5076               TRUE              <NA>              <NA>"},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"step-5-analyze-the-results","dir":"Articles","previous_headings":"Example Usage","what":"Step 5: Analyze the Results","title":"Search and Process NPI Numbers","text":"can now analyze NPI search results needed specific use case. result_df data frame contains information NPIs match search criteria. NPI numbers directly NPPES need run validate_and_remove_invalid_npi. One key step cleaning data filtering taxonomies. can changed different applications various subspecialties. Note people tricky list taxonomy specialty “Specialist” something else super vague. create code fix well, shown . Finally helpful join results called processed_result input_file called acog_presidents code can used .","code":"# Remove selected columns from the 'output_result' dataframe processed_result <- output_result %>%   dplyr::select(     -basic_middle_name,      -basic_certification_date,      -basic_name_prefix,      -basic_name_suffix,      -taxonomies_taxonomy_group,      -taxonomies_license,      -taxonomies_primary   ) %>%    mutate(across(c(basic_first_name, basic_last_name, basic_credential),        .fns = ~str_remove_all(., \"[[\\\\p{P}][\\\\p{S}]]\"))) %>%   mutate(basic_credential = str_to_upper(basic_credential)) %>%   filter(str_detect(basic_credential, \"MD|DO\")) %>%   mutate(basic_credential = str_sub(basic_credential,1 ,2)) %>%   filter(basic_credential %in% c(\"DO\", \"MD\")) %>%   filter(str_detect(taxonomies_desc, fixed(\"Gyn\", ignore_case=TRUE))) %>%   distinct(npi, .keep_all = TRUE)  > processed_result %>% head(5)           npi basic_first_name basic_last_name basic_credential basic_sole_proprietor basic_gender basic_enumeration_date 1: 1053601807             RYAN       SCHLUETER               DO                   YES            M             2011-04-13 2: 1063703494           LAUREN          BISHOP               MD                    NO            F             2011-04-19 3: 1376862383            JAMIE     SZCZEPANSKI               MD                    NO            F             2010-06-01 4: 1457676405          JESSICA         SHIELDS               DO                    NO            F             2010-04-01 5: 1366752107         CAROLINA          SUELDO               MD                    NO            F             2010-10-14    basic_last_updated basic_status taxonomies_code                                     taxonomies_desc taxonomies_state 1:         2021-09-30            A      207VM0101X  Obstetrics & Gynecology, Maternal & Fetal Medicine               GA 2:         2023-03-16            A      207VE0102X Obstetrics & Gynecology, Reproductive Endocrinology               NY 3:         2020-07-23            A      207V00000X                             Obstetrics & Gynecology               NY 4:         2019-07-18            A      207V00000X                             Obstetrics & Gynecology               MA 5:         2019-02-25            A      207V00000X                             Obstetrics & Gynecology               FL # Filter out rows where 'taxonomies_desc' contains the substring \"Gyn\" (case-insensitive).  This can be changed for different applications: \"Ortho\", \"Rheum\", \"Otolary\", \"Heme\", \"Anesthesi\".    processed_result <- processed_result %>%   dplyr::filter(stringr::str_detect(taxonomies_desc, fixed(\"gyn\", ignore_case = TRUE)) |   stringr::str_detect(taxonomies_desc, fixed(\"specialist\", ignore_case = TRUE))) combined_acog_presidents <-    acog_presidents %>%   dplyr::left_join(`processed_result`, by = c(\"first\" = \"basic_first_name\",                                                \"last\" = \"basic_last_name\",                                                \"honorrific\" = \"basic_credential\"),                                                ignore.case=TRUE)"},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Search and Process NPI Numbers","text":"search_and_process_npi function simplifies task searching processing NPI numbers healthcare datasets.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/search_and_process_npi.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Search and Process NPI Numbers","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Gather Physician Data Starting With NPI Numbers","text":"validate_and_remove_invalid_npi function designed help process datasets containing National Provider Identifier (NPI) numbers search_by_taxonomy. validates format NPIs using npi package removes rows missing invalid NPIs. vignette guide usage.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"installation","dir":"Articles","previous_headings":"Overview > Step 1","what":"Installation","title":"Gather Physician Data Starting With NPI Numbers","text":"can harness power search_by_taxonomy function, essential ensure tyler package installed. can effortlessly install using following command:","code":"library(tyler)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"understanding-national-provider-identifier","dir":"Articles","previous_headings":"Example Usage","what":"Understanding National Provider Identifier","title":"Gather Physician Data Starting With NPI Numbers","text":"valid National Provider Identifier (NPI) number United States meet certain criteria considered legitimate. key characteristics make NPI number valid: Length: NPI number consists ten digits. shorter longer ten digits. Numeric Digits: characters NPI must numeric digits (0-9). letters, symbols, special characters allowed. Luhn Algorithm: Luhn algorithm commonly used validate credit card numbers, applied NPI numbers. NPIs supposedly checksummed using Luhn algorithm. summary, valid NPI number ten numeric digits additional characters. validate NPI numbers programmatically, can check length confirm contain numeric digits (0-9). However, ’s important note specific format validation rules NPI numbers defined National Plan Provider Enumeration System (NPPES).","code":""},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"step-2-prepare-your-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 2: Prepare Your Data","title":"Gather Physician Data Starting With NPI Numbers","text":"can provide data either dataframe CSV file argument input_data. data dataframe, simply pass input_data parameter. data CSV file, pass file path input_data parameter.","code":"# Example using a dataframe data_df <- data.frame(npi = c(\"1234567890\", \"9876543210\", \"invalid_npi\")) valid_df <- validate_and_remove_invalid_npi(input_data)  # Example using a CSV file input_data <- \"path/to/your/file.csv\" valid_df <- validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"step-3-data-validation","dir":"Articles","previous_headings":"Example Usage","what":"Step 3: Data Validation","title":"Gather Physician Data Starting With NPI Numbers","text":"function validate NPIs data. performs following checks: Removes rows missing NPIs. Removes rows empty NPIs. Ensures NPIs valid format (numeric 10 characters length). Invalid NPIs removed, new column named “npi_is_valid” added indicate NPI validity.","code":"# A tibble: 7,494 × 7    sub1  first_name last_name          npi state         city             npi_is_valid    <chr> <chr>      <chr>            <dbl> <chr>         <chr>            <lgl>         1 MFM   Ryan       Schlueter   1053601807 Georgia       Atlanta          TRUE          2 FPM   Laura      Martin      1528351640 Florida       Miramar          TRUE          3 REI   Lauren     Bishop      1063703494 New York      New York         TRUE          4 MFM   Jamie      Szczepanski 1376862383 New York      Buffalo          TRUE"},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"step-4-get-the-valid-data","dir":"Articles","previous_headings":"Example Usage","what":"Step 4: Get the Valid Data","title":"Gather Physician Data Starting With NPI Numbers","text":"function return dataframe containing valid NPI numbers.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"step-5-validating-npi-numbers-is-needed-before-searching-by-npi-number-in-the-cms-databases-","dir":"Articles","previous_headings":"Example Usage","what":"Step 5: Validating NPI numbers is needed before searching by NPI number in the CMS databases.","title":"Gather Physician Data Starting With NPI Numbers","text":"error can break results error handling beyond knowledge base now. use case use validate_and_remove_invalid_npi function searching physician demographics (medical school, etc) National Downloadable File CMS (https://data.cms.gov/provider-data/dataset/mj5m-pzi6). database update monthly know data fresh validate_and_remove_invalid_npi makes clean. Fresh clean! step can confidently feed NPI numbers provider::clinicians function without fear NPI number error. Specifically use case. see CSV get read provider::clinicians searched. output people results skips people results.","code":"df_updated <- NULL  retrieve_clinician_data <- function(input_data) {   library(provider)   library(dplyr)   library(purrr)   library(readr)   library(tidyr)   library(lubridate)   library(memoise)   library(zipcodeR)    # Load libraries   #remotes::install_github(\"andrewallenbruce/provider\")    if (is.data.frame(input_data)) {     # Input is a dataframe     df <- input_data   } else if (is.character(input_data)) {     # Input is a file path to a CSV     df <- readr::read_csv(input_data)   } else {     stop(\"Input must be a dataframe or a file path to a CSV.\")   }    # Clean the NPI numbers   df <- validate_and_remove_invalid_npi(df) # Function to retrieve clinician data for a single NPI   get_clinician_data <- function(npi) {     if (!is.numeric(npi) || nchar(npi) != 10) {       cat(\"Invalid NPI:\", npi, \"\\n\")       return(NULL)  # Skip this NPI     }      clinician_info <- provider::clinicians(npi = npi)     if (is.null(clinician_info)) {       cat(\"No results for NPI:\", npi, \"\\n\")     } else {       return(clinician_info)     }     Sys.sleep(1)   }    #df <- df %>% head(5) #test    # Loop through the \"npi\" column and get clinician data   df_updated <- df %>%     dplyr::mutate(row_number = row_number()) %>%     dplyr::mutate(clinician_data = purrr::map(npi, get_clinician_data)) %>%     tidyr::unnest(clinician_data, names_sep = \"_\") %>%     dplyr::distinct(npi, .keep_all = TRUE)    return(df_updated) } # Call the retrieve_clinician_data function with an NPI value input_data <- (\"~/Dropbox (Personal)/workforce/subspecialists_only.csv\") clinician_data <- retrieve_clinician_data(input_data)  Rows: 7498 Columns: 9                                                                                                                                         ── Column specification ────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: \",\" chr (8): sub1, first_name, last_name, state, name.x, city, GenderPhysicianCompare, Zip CodePhysicianCompare dbl (1): npi  ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ✖ No results for npi = 1063703494 No results for NPI: 1063703494  ✖ No results for npi = 1104052125 No results for NPI: 1104052125  ✖ No results for npi = 1972745586 No results for NPI: 1972745586  ✖ No results for npi = 1427386804 No results for NPI: 1427386804  ✖ No results for npi = 1942586581 No results for NPI: 1942586581"},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Gather Physician Data Starting With NPI Numbers","text":"validate_and_remove_invalid_npi function handy tool cleaning validating datasets NPI numbers. following steps outlined vignette, can ensure data contains valid NPIs analysis processing.","code":""},{"path":"https://mufflyt.github.io/tyler/articles/validate_and_remove_invalid_npi.html","id":"features-and-bugs","dir":"Articles","previous_headings":"","what":"Features and bugs","title":"Gather Physician Data Starting With NPI Numbers","text":"ideas features make name handling easier, find bug, best approach either report add !","code":""},{"path":"https://mufflyt.github.io/tyler/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tyler Muffly. Maintainer.","code":""},{"path":"https://mufflyt.github.io/tyler/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Muffly T (2024). tyler: Common Functions Mystery Caller Audit Studies Evaluating Patient Access Care. R package version 1.2.0, https://mufflyt.github.io/tyler/.","code":"@Manual{,   title = {tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care},   author = {Tyler Muffly},   year = {2024},   note = {R package version 1.2.0},   url = {https://mufflyt.github.io/tyler/}, }"},{"path":[]},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"and-pull-requests-httpsgithubcommufflyttylerpulls","dir":"","previous_headings":"","what":"and pull requests (https://github.com/mufflyt/tyler/pulls).","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"tyler package provides suite tools conducting mystery caller studies facilitating workforce distribution research obstetrics gynecology (OBGYN) professionals. streamlines process retrieving analyzing National Provider Identifier (NPI) data, demographic information, healthcare access data, also offering resources examining OBGYN residency programs. Key Features - Mystery Caller Studies: Tools analyze patient access healthcare searching processing NPI numbers based names criteria. - OBGYN Workforce Distribution: Functions datasets designed support workforce research, including detailed information OBGYN residency programs United States.","code":""},{"path":"https://mufflyt.github.io/tyler/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"can install development version tyler GitHub : See package vignette fuller introduction suggestions use tyler() function efficiently.","code":"# install.packages(\"devtools\") devtools::install_github(\"mufflyt/tyler\")"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tyler-package-data-overview","dir":"","previous_headings":"","what":"Tyler Package Data Overview","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"document describes key datasets included tyler package, focusing ACOG districts, physicians, taxonomy codes used healthcare provider identification.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"data-tyleracgme","dir":"","previous_headings":"","what":"DATA: tyler::acgme","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"dataset tyler::acgme provides detailed information OBGYN residency programs accredited Accreditation Council Graduate Medical Education (ACGME). dataset includes fields program name, location, accreditation date, program director, affiliated hospitals. valuable resource mapping analyzing OBGYN residencies across U.S. Dataframe OBGYN residency programs scraped https://apps.acgme.org/ads/Public/Programs/Search. Name, city, state, accreditation date, program director name, website, rotations, affiliated hospitals included. ‘tyler::acgme’ - dataframe every OBGYN residency ACGME web site. data can used map obgyn residencies, etc.","code":"obgyn_residencies <- tyler::acgme  # View the first few rows head(obgyn_residencies) # A tibble: 318 × 142    program_name       address zip   city  state sponsoring_instituti…¹ sponsoring_instituti…² phone original_accreditati…³    <chr>              <chr>   <chr> <chr> <chr> <chr>                  <chr>                  <chr> <chr>                   1 University of Ala… \"Unive… 35249 Birm… Alab… 010498                 University of Alabama… (205… September 01, 1949      2 USA Health Program \"Unive… 36604 Mobi… Alab… 010406                 USA Health             (251… August 01, 1960         3 University of Ari… \"Banne… 85006 Phoe… Ariz… 038179                 University of Arizona… (602… May 07, 1951      # Example: Filter for residency programs in California ca_residencies <- dplyr::filter(obgyn_residencies, state == \"CA\") print(ca_residencies)"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"data-tyleracog_districts","dir":"","previous_headings":"","what":"DATA: ‘tyler::ACOG_Districts’","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"American College Obstetricians Gynecologists (ACOG) professional organization representing obstetricians gynecologists United States. ACOG divides membership various geographical regions known “ACOG Districts.” single-state ACOG Districts (e.g., California, Texas, Florida) also need use US Census Bureau subdivisions. Subdivisions important census statistical purposes help organize categorize population data local level. dataset includes: - State names: Full name U.S. state. - ACOG Districts: corresponding ACOG district state. - Subregions: U.S. Census Bureau subregions help organize population data. - State abbreviations: official two-letter postal abbreviations state. dataset useful research geographic distribution OBGYN professionals affiliations ACOG districts.","code":"acog_districts <- tyler::ACOG_Districts head(tyler::ACOG_Districts) # A tibble: 52 × 4    State                ACOG_District Subregion     State_Abbreviations    <chr>                <chr>         <chr>         <chr>                1 Alabama              District VII  District VII  AL                   2 Alaska               District VIII District VIII AK                   3 Arizona              District VIII District VIII AZ                   4 Arkansas             District VII  District VII  AR                   5 California           District IX   District IX   CA                   6 Colorado             District VIII District VIII CO                   7 Connecticut          District I    District I    CT                   8 Delaware             District IV   District IV   DE                   9 District of Columbia District IV   District IV   DC                  10 Florida              District XII  District XII  FL"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"data-tylerphysicians","dir":"","previous_headings":"","what":"DATA: tyler::physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"dataset contains details OBGYN subspecialists, including names, specialties, geographic coordinates. Physicians.rds file, located tyler/inst/extdata, stores internal dataset. dataset includes: - NPI: National Provider Identifier, unique identifier healthcare providers U.S. - Name: Physician’s full name. - Subspecialty: Physician’s specific area expertise within OBGYN. - Latitude/Longitude: Geographic coordinates physician’s practice.","code":"tyler::physicians # A tibble: 4,659 × 5           NPI name                        subspecialty                                        lat   long         <dbl> <chr>                       <chr>                                             <dbl>  <dbl>  1 1922051358 Katherine Boyd              Female Pelvic Medicine and Reconstructive Surgery  42.6  -82.9  2 1750344388 Thomas Byrne                Maternal-Fetal Medicine                            35.2 -102.   3 1548520133 Bobby Garcia                Female Pelvic Medicine and Reconstructive Surgery  40.8  -73.9"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"data-tylertaxonomy","dir":"","previous_headings":"","what":"DATA: tyler::taxonomy","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Physician Taxonomy Codes NPPES (National Plan Provider Enumeration System) database essential components healthcare system United States. codes play crucial role identifying categorizing healthcare providers various purposes, including billing, insurance, regulatory compliance. tyler::taxonomy dataset dataframe containing NUCC taxonomy codes used NPPES data files. taxonomy code consists unique ten-character identifier helps classify healthcare providers type area expertise. dataset includes OBGYN taxonomy codes, Version 23.1 7/1/2023. can find details NUCC website.","code":""},{"path":"https://mufflyt.github.io/tyler/index.html","id":"example","dir":"","previous_headings":"DATA: tyler::taxonomy","what":"Example:","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"","code":"obgyn_taxonomy <- tyler::taxonomy %>%    filter(str_detect(Classification, fixed(\"GYN\", ignore_case = TRUE))) %>%    select(Code, Specialization)  obgyn_taxonomy # A tibble of OBGYN-related taxonomy codes    Code       Specialization                                       <chr>      <chr>                                              1 207V00000X Obstetrics & Gynecology                                                 2 207VC0300X Complex Family Planning                            3 207VC0200X Critical Care Medicine                             4 207VF0040X Female Pelvic Medicine and Reconstructive Surgery  5 207VX0201X Gynecologic Oncology                               6 207VG0400X Gynecology                                         7 207VH0002X Hospice and Palliative Medicine                    8 207VM0101X Maternal & Fetal Medicine                          9 207VB0002X Obesity Medicine                                  10 207VX0000X Obstetrics                                        11 207VE0102X Reproductive Endocrinology"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylersearch_by_taxonomy","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::search_by_taxonomy","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function searches NPI Database healthcare providers based taxonomy description. search_by_taxonomy function wrapper npi::npi_search accessing registry’s Version 2.1 API. Many thanks author maintainers npi package amazing work.helps confirm outside data subspecialist provider counts fill gaps providers board-certified practicing (board-eligible). data can matched databases. Please see Exploratory/workforce/subspecialists_only code . nice thing search results come NPI.","code":"# This will allow us to get subspecialty names and NPI numbers go_data <- search_by_taxonomy(\"Gynecologic Oncology\") fpmrs_data <- search_by_taxonomy(\"Female Pelvic Medicine and Reconstructive Surgery\") rei_data <- search_by_taxonomy(\"Reproductive Endocrinology\") mfm_data <- search_by_taxonomy(\"Maternal & Fetal Medicine\")  # Merge all data frames into one       all_taxonomy_search_data <- bind_rows(         go_data,         fpmrs_data,         rei_data,         mfm_data) %>%         dplyr::distinct(npi, .keep_all = TRUE)  dim(all_taxonomy_search_data) glimpse(all_taxonomy_search_data) # 1200 records requested # Requesting records 0-200... # Requesting records 200-400..."},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylersearch_and_process_npi","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::search_and_process_npi","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"National Provider Identifier Search: Search first names, last names, individuals enumeration_type = \"ind\", physicians (\"MD\", \"\") United States NPPES. NPI numbers provide standardized way identify track healthcare providers, including physicians, across United States. Government agencies, Centers Medicare & Medicaid Services (CMS), use NPI-based data plan allocate healthcare resources, including provider reimbursements, medical services, workforce distribution.","code":"search_and_process_npi <- function(input_file,                                    enumeration_type = \"ind\",                                    limit = 5L,                                    country_code = \"US\",                                    filter_credentials = c(\"MD\", \"DO\"))  input_file <- \"/Users/tylermuffly/Dropbox (Personal)/Nomogram/nomogram/data/nppes_search/Lo_R_Author.csv\" output_result <- search_and_process_npi(input_file)"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylervalidate_and_remove_invalid_npi","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::validate_and_remove_invalid_npi","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"cleans NPI numbers goes tyler::retrieve_clinician_data one incorrect NPI number inserted screws entire search. Saves find csv file.","code":"input_csv_path <- \"~/Dropbox (Personal)/workforce/subspecialists_only.csv\"  # Replace with the path to your CSV file valid_df <- validate_and_remove_invalid_npi(input_csv_path) Search result saved as: data/search_results_1053601807_20231119192903.csv                                 Search result saved as: data/search_results_1528351640_20231119192911.csv                                 ✖ No results for npi = 1063703494 No results for NPI: 1063703494"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylerretrieve_clinician_data","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::retrieve_clinician_data","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function retrieves clinician data based validated NPI numbers, using validate_and_remove_invalid_npi filter NPIs. logs successful searches, saves results CSV files, flags NPIs data found. results timestamped filenames, ensuring search history maintained. function vital efficiently gathering clinician data external sources storing future use. retrieve_clinician_data function retrieves clinician information Medicare Care Compare system. Previously, data accessed via Physician Compare, sunsetted December 2020. dataset can found CMS Provider Data. Physician Compare sunset December 1, 2020 replaced : https://www.medicare.gov/care-compare/?redirect=true&providerType=Physician. entire data set https://data.cms.gov/provider-data/dataset/mj5m-pzi6. cool library called provider super helpful accessing .","code":"# Call the retrieve_clinician_data function with an NPI value input_csv_path <- (\"~/Dropbox (Personal)/workforce/subspecialists_only.csv\") clinician_data <- tyler::retrieve_clinician_data(input_csv_path) ✖ No results for npi = 1093151441 NULL # A tibble: 3 × 17   npi     pac   enid  first last  gender school grad_year specialty facility_name pac_org members_org address_org city_org   <chr>   <chr> <chr> <chr> <chr> <fct>  <chr>      <int> <chr>     <chr>         <chr>         <int> <chr>       <chr>    1 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 25 MICHIGA… GRAND R… 2 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 4444 KALAM… KENTWOOD 3 119406… 3476… I202… JACL… DENE… Female NEW Y…      2013 OBSTETRI… SPECTRUM HEA… 458756…        1551 4069 LAKE … GRAND R… # ℹ 3 more variables: state_org <ord>, zip_org <chr>, phone_org <chr>"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylergenderize_physicians","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::genderize_physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"wrapper around gender package help fill gender physician names. requires csv column called first_name. lot gender data found via Physician Compare past.","code":"tyler::genderize_physicians <- function(input_csv)"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"searching-for-data-tylergeocode_unique_addresses","dir":"","previous_headings":"","what":"SEARCHING FOR DATA: tyler::geocode_unique_addresses","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Takes csv file addresses prints lat long separate columns. need google_maps_api_key. Geocoding process converting human-readable addresses place names geographic coordinates (latitude longitude) can used locate places map. Google Geocoding API service provided Google allows developers perform geocoding reverse geocoding, process converting coordinates back human-readable addresses.","code":"output_data <-      tyler::geocode_unique_addresses(file_path = \"/Users/tylermuffly/Dropbox (Personal)/Tannous/data/address_for_geocoding.csv\",      google_maps_api_key = \"????\",      output_file_path = \"/Users/tylermuffly/Dropbox (Personal)/Tannous/data/geocoded_unique_addresses.csv\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"creating-mystery-caller-dataphase-1-tylercity_state_sample_specialists","dir":"","previous_headings":"","what":"Creating Mystery Caller Data/Phase 1: tyler::city_state_sample_specialists","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script samples specialists based city state data performs analyses based geographic location.","code":"# Example: Sampling specialists based on city and state data <- data.frame(city = c(\"Denver\", \"Chicago\"), state = c(\"CO\", \"IL\"), specialists = c(5, 3)) city_state_sample_specialists(data)  # Example with stratified sampling city_state_sample_specialists(data, stratified = TRUE)"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"creating-mystery-caller-dataphase-1-tylercity_state_assign_scenarios","dir":"","previous_headings":"","what":"Creating Mystery Caller Data/Phase 1: tyler::city_state_assign_scenarios","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script assigns city state data different scenarios, mapping geographic information scenario datasets.","code":"# Example: Assigning city and state to scenarios data <- data.frame(city = c(\"Denver\", \"Chicago\"), state = c(\"CO\", \"IL\")) city_state_assign_scenarios(data)  # Example with multiple scenarios scenarios <- list(scenario1 = data, scenario2 = data) city_state_assign_scenarios(scenarios)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerpoisson_formula_maker","dir":"","previous_headings":"","what":"tyler::poisson_formula_maker","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"poisson_formula_maker function constructs formula Poisson regression model based specified predictor variables. typically used running Poisson models, fit_poisson_models, streamline model fitting count-based data like waiting times.","code":"# Example: Create a Poisson regression formula poisson_formula <- poisson_formula_maker(predictor_vars = c(\"age\", \"gender\", \"insurance\"))  # View the formula print(poisson_formula)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylergenerate_latex_equation","dir":"","previous_headings":"","what":"tyler::generate_latex_equation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates LaTeX-formatted equation input model formula. commonly used fitting statistical models like regression present final equation document report. function complements functions like tyler::linear_regression_summary_sentence tyler::logistic_regression, generate human-readable summaries, allowing text formula outputs.","code":"# Example: Generate LaTeX equation from a linear model model <- lm(mpg ~ cyl + disp, data = mtcars) latex_equation <- generate_latex_equation(model)  # View the LaTeX equation print(latex_equation)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylergenerate_overall_table","dir":"","previous_headings":"","what":"tyler::generate_overall_table","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary table demographics based Table 1 data. supports multiple file formats, including RDS, CSV, XLS, logs key step, inputs, data transformations, file paths. can select specific columns apply custom label translations. ensures output directory exists saves generated table PDF. Error handling ensures function validates data logs issues occur execution.","code":"# Example: Generating an overall table generate_overall_table(input_file_path = \"data/Table1.rds\", output_directory = \"output_tables\")  # Example with selected columns generate_overall_table(input_file_path = \"data/Table1.csv\", output_directory = \"output_tables\", selected_columns = c(\"age\", \"gender\"))  # Example with label translations label_translations <- list(age = \"Age (years)\", gender = \"Gender\") generate_overall_table(input_file_path = \"data/Table1.xlsx\", output_directory = \"output_tables\", label_translations = label_translations)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercount_unique_physicians","dir":"","previous_headings":"","what":"tyler::count_unique_physicians","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script counts number unique physicians dataset based unique identifier NPI.","code":"# Example: Counting unique physicians data <- data.frame(NPI = c(\"12345\", \"67890\", \"12345\", \"54321\")) count_unique_physicians(data)  # Example with a custom identifier column count_unique_physicians(data, id_column = \"NPI\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercalculate_descriptive_stats","dir":"","previous_headings":"","what":"tyler::calculate_descriptive_stats","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function computes descriptive statistics, means, medians, standard deviations datasets, covering categorical numerical variables. can customize calculate statistics selected columns. logs progress saves results structured format, making highly flexible analyzing datasets. function key quickly summarizing large datasets systematically.","code":"# Example: Calculating descriptive statistics data <- data.frame(age = c(23, 35, 40, 29, 50), gender = c(\"M\", \"F\", \"M\", \"F\", \"M\")) calculate_descriptive_stats(data)  # Example with selected columns calculate_descriptive_stats(data, selected_columns = c(\"age\"))"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercalc_percentages","dir":"","previous_headings":"","what":"tyler::calc_percentages","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates percentages given dataset. can calculate percentage category categorical variables optionally handle missing data.","code":"# Example: Calculating percentages data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")) calc_percentages(data)  # Example with missing data handling calc_percentages(data, handle_missing = TRUE)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercalculate_distribution","dir":"","previous_headings":"","what":"tyler::calculate_distribution","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates distribution values numerical categorical variables dataset.","code":"# Example: Calculating distribution data <- data.frame(value = c(1, 2, 2, 3, 3, 3, 4)) calculate_distribution(data)  # Example with categorical variables data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\")) calculate_distribution(data, variable_type = \"categorical\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercalculate_proportion","dir":"","previous_headings":"","what":"tyler::calculate_proportion","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script calculates proportion specific event category dataset.","code":"# Example: Calculating proportion data <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"B\")) calculate_proportion(data, event = \"A\")  # Example with a custom threshold calculate_proportion(data, event = \"B\", threshold = 0.2)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercheck_normality","dir":"","previous_headings":"","what":"tyler::check_normality","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script checks normality numeric dataset using statistical tests graphical methods QQ plots.","code":"# Example: Checking normality of a dataset data <- data.frame(value = c(10, 12, 15, 13, 14, 15, 18)) check_normality(data)  # Example with a specific significance level check_normality(data, significance_level = 0.05)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerpoisson_wait_time_stats","dir":"","previous_headings":"","what":"tyler::poisson_wait_time_stats","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function analyzes wait time statistics using Poisson model. designed model count-based data, like number appointments waiting times. complements poisson_formula_maker fit_poisson_models, helps understanding distribution determinants wait times.","code":"# Example: Calculate Poisson wait time statistics wait_time_stats <- poisson_wait_time_stats(df = appointments_data, target_variable = \"wait_time\")  # View the wait time statistics print(wait_time_stats)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_and_plot_interaction","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_forest_plot","dir":"","previous_headings":"","what":"tyler::create_forest_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_forest_plot function creates forest plot significant predictors, displaying coefficients confidence intervals. used fitting statistical models, Poisson logistic regression, visualize key predictors effects. function often paired model-fitting functions like fit_poisson_models fit_mixed_model_with_logging.","code":"# Example: Creating a forest plot for significant predictors df <- data.frame(predictor = c(\"age\", \"gender\", \"income\"), estimate = c(0.2, -0.5, 0.3), lower = c(0.1, -0.7, 0.2), upper = c(0.3, -0.3, 0.5)) create_forest_plot(df, \"target_variable\", significant_vars = df)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_insurance_by_insurance_scatter_plot","dir":"","previous_headings":"","what":"tyler::create_insurance_by_insurance_scatter_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_insurance_by_insurance_scatter_plot function creates scatter plot comparing waiting times two insurance types. complements data cleaning processing functions prepare appointment data visualization, determine_direction. plot useful visually comparing performance insurance providers terms wait times.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_line_plot","dir":"","previous_headings":"","what":"tyler::create_line_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"create_line_plot function generates line plots visualizing trends time continuous variables. commonly used calculating descriptive statistics analyzing trends across time-based data. works well summarized grouped data functions like calculate_descriptive_stats.","code":"# Example: Creating a line plot create_line_plot(df, x = \"year\", y = \"appointments\", group = \"insurance\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_scatter_plot","dir":"","previous_headings":"","what":"tyler::create_scatter_plot","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function creates scatter plots, allowing comparison two continuous variables, optionally linear regression line. useful exploratory data analysis visualizing relationships variables. function complements descriptive statistics correlation analysis functions.","code":"# Example: Creating a scatter plot create_scatter_plot(df, x = \"income\", y = \"appointment_days\", add_regression = TRUE)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_and_plot_interaction-1","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerlinear_regression_summary_sentence","dir":"","previous_headings":"","what":"tyler::linear_regression_summary_sentence","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary sentence linear regression results, making easier present findings clear concise manner. typically used fitting linear regression model complements functions like generate_latex_equation providing textual formulaic summary results.","code":"# Example: Generate summary sentence for linear regression model <- lm(mpg ~ wt + hp, data = mtcars) summary_sentence <- linear_regression_summary_sentence(model)  # View the summary sentence print(summary_sentence)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerlogistic_regression","dir":"","previous_headings":"","what":"tyler::logistic_regression","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function generates summary sentence linear regression results, making easier present findings clear concise manner. typically used fitting linear regression model complements functions like generate_latex_equation providing textual formulaic summary results.","code":"# Example: Generate summary sentence for linear regression model <- lm(mpg ~ wt + hp, data = mtcars) summary_sentence <- linear_regression_summary_sentence(model)  # View the summary sentence print(summary_sentence)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylermaxtable","dir":"","previous_headings":"","what":"tyler::MaxTable","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"tyler::MaxTable function creates summary table highlights maximum values key metrics across categories. often used conjunction descriptive analysis functions, generate_overall_table, provide deeper insights maximum performance metrics.","code":"# Example: Generate a MaxTable for the dataset max_table <- MaxTable(df = mtcars, group_variable = \"cyl\", value_variable = \"mpg\")  # View the MaxTable print(max_table)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylermintable","dir":"","previous_headings":"","what":"tyler::MinTable","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"MinTable function creates summary table highlighting minimum values key metrics across different categories. often used alongside MaxTable provide complete range insights showing minimum performance outcomes within group. especially useful comparative studies.","code":"# Example: Generate a MinTable for the dataset min_table <- MinTable(df = mtcars, group_variable = \"cyl\", value_variable = \"mpg\")  # View the MinTable print(min_table)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylermost_common_gender_training_academic","dir":"","previous_headings":"","what":"tyler::most_common_gender_training_academic","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function identifies common gender training type among academic physicians. complements demographic analysis functions like physician_age useful studies focused characteristics healthcare providers, particularly academic settings.","code":"# Example: Identify the most common gender and training type among academic physicians common_gender_training <- most_common_gender_training_academic(df = physicians_data)  # View the results print(common_gender_training)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerphysician_age","dir":"","previous_headings":"","what":"tyler::physician_age","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"physician_age function calculates age physicians based birth year similar data. core function used demographic analyses often paired functions like most_common_gender_training_academic provide complete understanding physician workforce.","code":"Example: Calculate physician age physician_ages <- physician_age(df = physicians_data, birth_year_col = \"birth_year\")  # View the ages print(physician_ages)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerplot_and_save_emmeans","dir":"","previous_headings":"","what":"tyler::plot_and_save_emmeans","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function creates saves plots estimated marginal means (EMMeans) fitted model. often used running statistical models, Poisson mixed-effects models, visualize group differences. function complements fit_poisson_models fit_mixed_model_with_logging.","code":"# Example: Plot and save EMMeans from a fitted model emmeans_plot <- plot_and_save_emmeans(model = fitted_model, variables = c(\"age\", \"gender\"), output_dir = \"plots/\")  # View the plot print(emmeans_plot)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_and_plot_interaction-2","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_and_plot_interaction-3","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylercreate_and_plot_interaction-4","dir":"","previous_headings":"","what":"tyler::create_and_plot_interaction","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"script creates interaction models variables plots results.","code":"# Example: Creating an interaction plot data <- data.frame(var1 = c(1, 2, 3, 4), var2 = c(5, 6, 7, 8)) create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\")  # Example with customized plot settings create_and_plot_interaction(data, var1 = \"var1\", var2 = \"var2\", plot_title = \"Interaction Plot\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerinstall_missing_packages","dir":"","previous_headings":"","what":"tyler::install_missing_packages","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"utility function checks missing R packages installs necessary. often used start analysis script ensure required packages installed. function complements function relies external packages, ensuring smooth workflow without interruptions.","code":"# Example: Install missing packages required_packages <- c(\"ggplot2\", \"dplyr\", \"readr\") install_missing_packages(required_packages)"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylerload_data","dir":"","previous_headings":"","what":"tyler::load_data","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function loads data various file formats (e.g., CSV, RDS, Excel) prepares analysis. one first steps analysis workflow, providing clean data subsequent steps like model fitting visualization. complements functions rely dataset loaded memory.","code":"# Example: Load data from a CSV file data <- load_data(\"data/my_data.csv\")  # Example: Load data from an RDS file data <- load_data(\"data/my_data.rds\")"},{"path":[]},{"path":"https://mufflyt.github.io/tyler/index.html","id":"tylertm_write2pdf","dir":"","previous_headings":"","what":"tyler::tm_write2pdf","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"function saves arsenal-generated table object PDF file. logs PDF saved ensures PDF written without unnecessary output. function streamlines process converting arsenal tables PDF files, retaining markdown elements ensuring efficient saving multiple summaries.","code":""},{"path":"https://mufflyt.github.io/tyler/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"welcome contributions! ’d like help improve tyler package, feel free submit issues pull requests.","code":""},{"path":"https://mufflyt.github.io/tyler/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"use package, appreciate citation.","code":"citation(\"tyler\")"},{"path":"https://mufflyt.github.io/tyler/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/ACOG_Districts.html","id":null,"dir":"Reference","previous_headings":"","what":"ACOG Districts Data — ACOG_Districts","title":"ACOG Districts Data — ACOG_Districts","text":"dataset contains information American College Obstetricians Gynecologists (ACOG) districts, including two-letter state abbreviations full state names.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/ACOG_Districts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACOG Districts Data — ACOG_Districts","text":"","code":"ACOG_Districts"},{"path":"https://mufflyt.github.io/tyler/reference/ACOG_Districts.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACOG Districts Data — ACOG_Districts","text":"data frame following columns: ACOG_District Two-letter state abbreviations representing ACOG districts. name Full names states corresponding ACOG districts.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/ACOG_Districts.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACOG Districts Data — ACOG_Districts","text":"Data obtained official ACOG website: https://www.acog.org/community/districts--sections","code":""},{"path":"https://mufflyt.github.io/tyler/reference/ACOG_Districts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACOG Districts Data — ACOG_Districts","text":"","code":"# Load the ACOG Districts Data data(ACOG_Districts)  # View the first few rows of the dataset head(ACOG_Districts) #> # A tibble: 6 × 4 #>   State      ACOG_District Subregion     State_Abbreviations #>   <chr>      <chr>         <chr>         <chr>               #> 1 Alabama    District VII  District VII  AL                  #> 2 Alaska     District VIII District VIII AK                  #> 3 Arizona    District VIII District VIII AZ                  #> 4 Arkansas   District VII  District VII  AR                  #> 5 California District IX   District IX   CA                  #> 6 Colorado   District VIII District VIII CO                   # Get a summary of the dataset summary(ACOG_Districts) #>     State           ACOG_District       Subregion         State_Abbreviations #>  Length:52          Length:52          Length:52          Length:52           #>  Class :character   Class :character   Class :character   Class :character    #>  Mode  :character   Mode  :character   Mode  :character   Mode  :character     # Perform data analysis and exploration"},{"path":"https://mufflyt.github.io/tyler/reference/MaxTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"function returns level(s) corresponding maximum value(s) factor variable.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MaxTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"","code":"MaxTable(InVec, mult = FALSE)"},{"path":"https://mufflyt.github.io/tyler/reference/MaxTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"InVec Input vector, expected factor variable convertible factor. mult Logical value indicating whether return multiple maximum values just first one. Default FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MaxTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"mult FALSE, returns level corresponding maximum value factor variable. mult TRUE, returns character vector containing levels maximum value.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MaxTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable — MaxTable","text":"","code":"vec <- factor(c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\")) MaxTable(vec) # Returns \"A\" #> [1] \"B\" MaxTable(vec, mult = TRUE) # Returns c(\"A\", \"B\") #> [1] \"B\""},{"path":"https://mufflyt.github.io/tyler/reference/MinTable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"function returns level(s) corresponding minimum value(s) factor variable.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MinTable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"","code":"MinTable(InVec, mult = FALSE)"},{"path":"https://mufflyt.github.io/tyler/reference/MinTable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"InVec Input vector, expected factor variable convertible factor. mult Logical value indicating whether return multiple minimum values just first one. Default FALSE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MinTable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"mult FALSE, returns level corresponding minimum value factor variable. mult TRUE, returns character vector containing levels minimum value.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/MinTable.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable — MinTable","text":"","code":"vec <- factor(c(\"A\", \"B\", \"A\", \"C\", \"B\", \"B\")) MinTable(vec) # Returns \"C\" #> [1] \"C\" MinTable(vec, mult = TRUE) # Returns \"C\" #> [1] \"C\""},{"path":"https://mufflyt.github.io/tyler/reference/acgme.html","id":null,"dir":"Reference","previous_headings":"","what":"ACGME OBGYN Residency Data — acgme","title":"ACGME OBGYN Residency Data — acgme","text":"dataset provides information Obstetricians Gynecologists (OBGYN) residency programs accredited Accreditation Council Graduate Medical Education (ACGME).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/acgme.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ACGME OBGYN Residency Data — acgme","text":"","code":"acgme"},{"path":"https://mufflyt.github.io/tyler/reference/acgme.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ACGME OBGYN Residency Data — acgme","text":"data frame following columns: Program directors names program directors OBGYN residency programs. name names OBGYN residency programs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/acgme.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ACGME OBGYN Residency Data — acgme","text":"Data obtained ACGME website: https://apps.acgme.org/ads/Public/Programs/Search","code":""},{"path":"https://mufflyt.github.io/tyler/reference/acgme.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ACGME OBGYN Residency Data — acgme","text":"","code":"# Load the ACGME OBGYN Residency Data data(acgme)  # View the first few rows of the dataset head(acgme) #> # A tibble: 6 × 142 #>   program_name                  address zip   city  state sponsoring_instituti…¹ #>   <chr>                         <chr>   <chr> <chr> <chr> <chr>                  #> 1 University of Alabama Medica… \"Unive… 35249 Birm… Alab… 010498                 #> 2 USA Health Program            \"Unive… 36604 Mobi… Alab… 010406                 #> 3 University of Arizona Colleg… \"Banne… 85006 Phoe… Ariz… 038179                 #> 4 University of Arizona Colleg… \"Unive… 85724 Tucs… Ariz… 030509                 #> 5 Creighton University School … \"Creig… 85008 Phoe… Ariz… 309502                 #> 6 University of Arkansas for M… \"Unive… 72205 Litt… Arka… 049501                 #> # ℹ abbreviated name: ¹​sponsoring_institution_code #> # ℹ 136 more variables: sponsoring_institution_name <chr>, phone <chr>, #> #   original_accreditation_date <chr>, accreditation_status <chr>, #> #   director_name <chr>, director_date_appointed <chr>, #> #   coordinator_name_1 <chr>, coordinator_phone_1 <chr>, #> #   coordinator_email_1 <chr>, participation_site_code_1 <chr>, #> #   participation_site_name_1 <chr>, rotation_required_1 <chr>, …  # Get a summary of the dataset summary(acgme) #>  program_name         address              zip                city           #>  Length:318         Length:318         Length:318         Length:318         #>  Class :character   Class :character   Class :character   Class :character   #>  Mode  :character   Mode  :character   Mode  :character   Mode  :character   #>                                                                              #>                                                                              #>                                                                              #>                                                                              #>     state           sponsoring_institution_code sponsoring_institution_name #>  Length:318         Length:318                  Length:318                  #>  Class :character   Class :character            Class :character            #>  Mode  :character   Mode  :character            Mode  :character            #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>     phone           original_accreditation_date accreditation_status #>  Length:318         Length:318                  Length:318           #>  Class :character   Class :character            Class :character     #>  Mode  :character   Mode  :character            Mode  :character     #>                                                                      #>                                                                      #>                                                                      #>                                                                      #>  director_name      director_date_appointed coordinator_name_1 #>  Length:318         Length:318              Length:318         #>  Class :character   Class :character        Class :character   #>  Mode  :character   Mode  :character        Mode  :character   #>                                                                #>                                                                #>                                                                #>                                                                #>  coordinator_phone_1 coordinator_email_1 participation_site_code_1 #>  Length:318          Length:318          Length:318                #>  Class :character    Class :character    Class :character          #>  Mode  :character    Mode  :character    Mode  :character          #>                                                                    #>                                                                    #>                                                                    #>                                                                    #>  participation_site_name_1 rotation_required_1 rotation_months_y1_1 #>  Length:318                Length:318          Min.   : 0.000       #>  Class :character          Class :character    1st Qu.: 9.425       #>  Mode  :character          Mode  :character    Median :11.750       #>                                                Mean   :10.277       #>                                                3rd Qu.:12.000       #>                                                Max.   :13.000       #>                                                                     #>  rotation_months_y2_1 rotation_months_y3_1 rotation_months_y4_1 #>  Min.   : 0.000       Min.   : 0.000       Min.   : 0.000       #>  1st Qu.: 8.000       1st Qu.: 7.000       1st Qu.: 8.000       #>  Median :10.550       Median : 9.000       Median :10.500       #>  Mean   : 9.581       Mean   : 8.895       Mean   : 9.696       #>  3rd Qu.:12.000       3rd Qu.:11.000       3rd Qu.:12.000       #>  Max.   :13.000       Max.   :13.000       Max.   :13.000       #>                                                                 #>  participation_site_code_2 participation_site_name_2 rotation_required_2 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_2 rotation_months_y2_2 rotation_months_y3_2 #>  Min.   : 0.000       Min.   : 0.0         Min.   : 0.000       #>  1st Qu.: 0.000       1st Qu.: 0.0         1st Qu.: 0.500       #>  Median : 0.200       Median : 1.0         Median : 1.200       #>  Mean   : 1.494       Mean   : 1.9         Mean   : 2.066       #>  3rd Qu.: 2.000       3rd Qu.: 3.0         3rd Qu.: 3.000       #>  Max.   :13.000       Max.   :12.0         Max.   :12.000       #>  NA's   :39           NA's   :39           NA's   :39           #>  rotation_months_y4_2 participation_site_code_3 participation_site_name_3 #>  Min.   : 0.000       Length:318                Length:318                #>  1st Qu.: 0.000       Class :character          Class :character          #>  Median : 1.000       Mode  :character          Mode  :character          #>  Mean   : 1.719                                                           #>  3rd Qu.: 2.000                                                           #>  Max.   :12.000                                                           #>  NA's   :39                                                               #>  rotation_required_3 rotation_months_y1_3 rotation_months_y2_3 #>  Length:318          Min.   : 0.0000      Min.   : 0.000       #>  Class :character    1st Qu.: 0.0000      1st Qu.: 0.000       #>  Mode  :character    Median : 0.0000      Median : 0.300       #>                      Mean   : 0.7106      Mean   : 1.009       #>                      3rd Qu.: 1.0000      3rd Qu.: 1.500       #>                      Max.   :12.0000      Max.   :10.000       #>                      NA's   :91           NA's   :91           #>  rotation_months_y3_3 rotation_months_y4_3 participation_site_code_4 #>  Min.   :0.000        Min.   : 0.000       Length:318                #>  1st Qu.:0.000        1st Qu.: 0.000       Class :character          #>  Median :1.000        Median : 0.000       Mode  :character          #>  Mean   :1.298        Mean   : 1.041                                 #>  3rd Qu.:2.000        3rd Qu.: 1.500                                 #>  Max.   :9.000        Max.   :10.000                                 #>  NA's   :91           NA's   :91                                     #>  participation_site_name_4 rotation_required_4 rotation_months_y1_4 #>  Length:318                Length:318          Min.   :0.0000       #>  Class :character          Class :character    1st Qu.:0.0000       #>  Mode  :character          Mode  :character    Median :0.0000       #>                                                Mean   :0.3675       #>                                                3rd Qu.:0.1000       #>                                                Max.   :5.0000       #>                                                NA's   :149          #>  rotation_months_y2_4 rotation_months_y3_4 rotation_months_y4_4 #>  Min.   :0.0000       Min.   :0.0000       Min.   : 0.0000      #>  1st Qu.:0.0000       1st Qu.:0.0000       1st Qu.: 0.0000      #>  Median :0.0000       Median :0.5000       Median : 0.0000      #>  Mean   :0.6509       Mean   :0.8882       Mean   : 0.7787      #>  3rd Qu.:1.0000       3rd Qu.:1.0000       3rd Qu.: 1.0000      #>  Max.   :5.0000       Max.   :6.0000       Max.   :12.0000      #>  NA's   :149          NA's   :149          NA's   :149          #>  participation_site_code_5 participation_site_name_5 rotation_required_5 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_5 rotation_months_y2_5 rotation_months_y3_5 #>  Min.   : 0.0000      Min.   : 0.0000      Min.   : 0.0000      #>  1st Qu.: 0.0000      1st Qu.: 0.0000      1st Qu.: 0.0000      #>  Median : 0.0000      Median : 0.0000      Median : 0.5000      #>  Mean   : 0.2991      Mean   : 0.6487      Mean   : 0.9584      #>  3rd Qu.: 0.0000      3rd Qu.: 1.0000      3rd Qu.: 1.0000      #>  Max.   :12.0000      Max.   :12.0000      Max.   :12.0000      #>  NA's   :205          NA's   :205          NA's   :205          #>  rotation_months_y4_5 participation_site_code_6 rotation_required_6 #>  Min.   : 0.0000      Length:318                Length:318          #>  1st Qu.: 0.0000      Class :character          Class :character    #>  Median : 0.1000      Mode  :character          Mode  :character    #>  Mean   : 0.6912                                                    #>  3rd Qu.: 1.0000                                                    #>  Max.   :12.0000                                                    #>  NA's   :205                                                        #>  rotation_months_y1_6 rotation_months_y2_6 rotation_months_y3_6 #>  Min.   :0.000        Min.   :0.0000       Min.   :0.0000       #>  1st Qu.:0.000        1st Qu.:0.0000       1st Qu.:0.0000       #>  Median :0.000        Median :0.0000       Median :0.1000       #>  Mean   :0.175        Mean   :0.4789       Mean   :0.7421       #>  3rd Qu.:0.000        3rd Qu.:0.6000       3rd Qu.:1.0000       #>  Max.   :2.500        Max.   :5.0000       Max.   :6.0000       #>  NA's   :242          NA's   :242          NA's   :242          #>  rotation_months_y4_6 participation_site_code_7 participation_site_name_7 #>  Min.   :0.0000       Length:318                Length:318                #>  1st Qu.:0.0000       Class :character          Class :character          #>  Median :0.3000       Mode  :character          Mode  :character          #>  Mean   :0.7474                                                           #>  3rd Qu.:1.0000                                                           #>  Max.   :7.0000                                                           #>  NA's   :242                                                              #>  rotation_required_7 rotation_months_y1_7 rotation_months_y2_7 #>  Length:318          Min.   :0.0000       Min.   :0.0000       #>  Class :character    1st Qu.:0.0000       1st Qu.:0.0000       #>  Mode  :character    Median :0.0000       Median :0.0000       #>                      Mean   :0.1907       Mean   :0.4628       #>                      3rd Qu.:0.0000       3rd Qu.:0.5000       #>                      Max.   :4.0000       Max.   :4.5000       #>                      NA's   :275          NA's   :275          #>  rotation_months_y3_7 rotation_months_y4_7 participation_site_code_8 #>  Min.   :0.0000       Min.   :0.0000       Length:318                #>  1st Qu.:0.0000       1st Qu.:0.0000       Class :character          #>  Median :0.5000       Median :0.2000       Mode  :character          #>  Mean   :0.8605       Mean   :0.7814                                 #>  3rd Qu.:1.0000       3rd Qu.:1.0000                                 #>  Max.   :4.5000       Max.   :7.0000                                 #>  NA's   :275          NA's   :275                                    #>  participation_site_name_8 rotation_required_8 rotation_months_y1_8 #>  Length:318                Length:318          Min.   :0.0          #>  Class :character          Class :character    1st Qu.:0.0          #>  Mode  :character          Mode  :character    Median :0.0          #>                                                Mean   :0.3          #>                                                3rd Qu.:0.0          #>                                                Max.   :3.2          #>                                                NA's   :297          #>  rotation_months_y2_8 rotation_months_y3_8 rotation_months_y4_8 #>  Min.   :0.0000       Min.   :0.0000       Min.   :0.000        #>  1st Qu.:0.0000       1st Qu.:0.0000       1st Qu.:0.000        #>  Median :0.0000       Median :0.5000       Median :0.500        #>  Mean   :0.4857       Mean   :0.5524       Mean   :0.819        #>  3rd Qu.:1.0000       3rd Qu.:1.0000       3rd Qu.:1.000        #>  Max.   :3.0000       Max.   :2.0000       Max.   :4.000        #>  NA's   :297          NA's   :297          NA's   :297          #>  participation_site_code_9 participation_site_name_9 rotation_required_9 #>  Length:318                Length:318                Length:318          #>  Class :character          Class :character          Class :character    #>  Mode  :character          Mode  :character          Mode  :character    #>                                                                          #>                                                                          #>                                                                          #>                                                                          #>  rotation_months_y1_9 rotation_months_y2_9 rotation_months_y3_9 #>  Min.   :0.00000      Min.   :0.0000       Min.   :0.0000       #>  1st Qu.:0.00000      1st Qu.:0.0000       1st Qu.:0.0000       #>  Median :0.00000      Median :0.0000       Median :0.5000       #>  Mean   :0.06923      Mean   :0.2692       Mean   :0.5154       #>  3rd Qu.:0.00000      3rd Qu.:0.4000       3rd Qu.:1.0000       #>  Max.   :0.90000      Max.   :1.0000       Max.   :1.0000       #>  NA's   :305          NA's   :305          NA's   :305          #>  rotation_months_y4_9 participation_site_code_10 participation_site_name_10 #>  Min.   :0.0000       Length:318                 Length:318                 #>  1st Qu.:0.1000       Class :character           Class :character           #>  Median :1.0000       Mode  :character           Mode  :character           #>  Mean   :0.7769                                                             #>  3rd Qu.:1.0000                                                             #>  Max.   :3.0000                                                             #>  NA's   :305                                                                #>  rotation_required_10 rotation_months_y1_10 rotation_months_y2_10 #>  Length:318           Min.   :0.0000        Min.   :0.0000        #>  Class :character     1st Qu.:0.0000        1st Qu.:0.0000        #>  Mode  :character     Median :0.0000        Median :0.0000        #>                       Mean   :0.1111        Mean   :0.2333        #>                       3rd Qu.:0.0000        3rd Qu.:0.1000        #>                       Max.   :1.0000        Max.   :1.0000        #>                       NA's   :309           NA's   :309           #>  rotation_months_y3_10 rotation_months_y4_10 participation_site_code_11 #>  Min.   :0.0000        Min.   :0.0000        Length:318                 #>  1st Qu.:0.1000        1st Qu.:0.0000        Class :character           #>  Median :0.7000        Median :1.0000        Mode  :character           #>  Mean   :0.5889        Mean   :0.6778                                   #>  3rd Qu.:1.0000        3rd Qu.:1.0000                                   #>  Max.   :1.0000        Max.   :2.0000                                   #>  NA's   :309           NA's   :309                                      #>  participation_site_name_11 rotation_required_11 rotation_months_y1_11 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :313           #>  rotation_months_y2_11 rotation_months_y3_11 rotation_months_y4_11 #>  Min.   :0.0           Min.   :0.00          Min.   :0.0           #>  1st Qu.:0.0           1st Qu.:0.10          1st Qu.:0.0           #>  Median :1.0           Median :0.70          Median :0.0           #>  Mean   :0.6           Mean   :0.56          Mean   :0.4           #>  3rd Qu.:1.0           3rd Qu.:1.00          3rd Qu.:1.0           #>  Max.   :1.0           Max.   :1.00          Max.   :1.0           #>  NA's   :313           NA's   :313           NA's   :313           #>  participation_site_code_12 participation_site_name_12 rotation_required_12 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_12 rotation_months_y2_12 rotation_months_y3_12 #>  Min.   : 0.100        Min.   : 0.000        Min.   : 0.000        #>  1st Qu.: 0.775        1st Qu.: 0.075        1st Qu.: 0.075        #>  Median : 6.000        Median : 5.050        Median : 5.050        #>  Mean   : 5.775        Mean   : 5.025        Mean   : 5.025        #>  3rd Qu.:11.000        3rd Qu.:10.000        3rd Qu.:10.000        #>  Max.   :11.000        Max.   :10.000        Max.   :10.000        #>  NA's   :314           NA's   :314           NA's   :314           #>  rotation_months_y4_12 participation_site_code_13 participation_site_name_13 #>  Min.   : 0.000        Length:318                 Length:318                 #>  1st Qu.: 0.075        Class :character           Class :character           #>  Median : 6.050        Mode  :character           Mode  :character           #>  Mean   : 6.025                                                              #>  3rd Qu.:12.000                                                              #>  Max.   :12.000                                                              #>  NA's   :314                                                                 #>  rotation_required_13 rotation_months_y1_13 rotation_months_y2_13 #>  Length:318           Min.   :0             Min.   :0.0           #>  Class :character     1st Qu.:0             1st Qu.:0.1           #>  Mode  :character     Median :0             Median :0.2           #>                       Mean   :0             Mean   :0.2           #>                       3rd Qu.:0             3rd Qu.:0.3           #>                       Max.   :0             Max.   :0.4           #>                       NA's   :316           NA's   :316           #>  rotation_months_y3_13 rotation_months_y4_13 participation_site_code_14 #>  Min.   :0.000         Min.   :0.000         Length:318                 #>  1st Qu.:0.025         1st Qu.:0.025         Class :character           #>  Median :0.050         Median :0.050         Mode  :character           #>  Mean   :0.050         Mean   :0.050                                    #>  3rd Qu.:0.075         3rd Qu.:0.075                                    #>  Max.   :0.100         Max.   :0.100                                    #>  NA's   :316           NA's   :316                                      #>  participation_site_name_14 rotation_required_14 rotation_months_y1_14 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :317           #>  rotation_months_y2_14 rotation_months_y3_14 rotation_months_y4_14 #>  Min.   :0             Min.   :0.1           Min.   :0             #>  1st Qu.:0             1st Qu.:0.1           1st Qu.:0             #>  Median :0             Median :0.1           Median :0             #>  Mean   :0             Mean   :0.1           Mean   :0             #>  3rd Qu.:0             3rd Qu.:0.1           3rd Qu.:0             #>  Max.   :0             Max.   :0.1           Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  participation_site_code_15 participation_site_name_15 rotation_required_15 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_15 rotation_months_y2_15 rotation_months_y3_15 #>  Min.   :0.1           Min.   :0             Min.   :0             #>  1st Qu.:0.1           1st Qu.:0             1st Qu.:0             #>  Median :0.1           Median :0             Median :0             #>  Mean   :0.1           Mean   :0             Mean   :0             #>  3rd Qu.:0.1           3rd Qu.:0             3rd Qu.:0             #>  Max.   :0.1           Max.   :0             Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  rotation_months_y4_15 participation_site_code_16 participation_site_name_16 #>  Min.   :0             Length:318                 Length:318                 #>  1st Qu.:0             Class :character           Class :character           #>  Median :0             Mode  :character           Mode  :character           #>  Mean   :0                                                                   #>  3rd Qu.:0                                                                   #>  Max.   :0                                                                   #>  NA's   :317                                                                 #>  rotation_required_16 rotation_months_y1_16 rotation_months_y2_16 #>  Length:318           Min.   :0             Min.   :0             #>  Class :character     1st Qu.:0             1st Qu.:0             #>  Mode  :character     Median :0             Median :0             #>                       Mean   :0             Mean   :0             #>                       3rd Qu.:0             3rd Qu.:0             #>                       Max.   :0             Max.   :0             #>                       NA's   :317           NA's   :317           #>  rotation_months_y3_16 rotation_months_y4_16 participation_site_code_17 #>  Min.   :0             Min.   :0             Length:318                 #>  1st Qu.:0             1st Qu.:0             Class :character           #>  Median :0             Median :0             Mode  :character           #>  Mean   :0             Mean   :0                                        #>  3rd Qu.:0             3rd Qu.:0                                        #>  Max.   :0             Max.   :0                                        #>  NA's   :317           NA's   :317                                      #>  participation_site_name_17 rotation_required_17 rotation_months_y1_17 #>  Length:318                 Length:318           Min.   :0             #>  Class :character           Class :character     1st Qu.:0             #>  Mode  :character           Mode  :character     Median :0             #>                                                  Mean   :0             #>                                                  3rd Qu.:0             #>                                                  Max.   :0             #>                                                  NA's   :317           #>  rotation_months_y2_17 rotation_months_y3_17 rotation_months_y4_17 #>  Min.   :0             Min.   :0             Min.   :0             #>  1st Qu.:0             1st Qu.:0             1st Qu.:0             #>  Median :0             Median :0             Median :0             #>  Mean   :0             Mean   :0             Mean   :0             #>  3rd Qu.:0             3rd Qu.:0             3rd Qu.:0             #>  Max.   :0             Max.   :0             Max.   :0             #>  NA's   :317           NA's   :317           NA's   :317           #>  participation_site_code_18 participation_site_name_18 rotation_required_18 #>  Length:318                 Length:318                 Length:318           #>  Class :character           Class :character           Class :character     #>  Mode  :character           Mode  :character           Mode  :character     #>                                                                             #>                                                                             #>                                                                             #>                                                                             #>  rotation_months_y1_18 rotation_months_y2_18 rotation_months_y3_18 #>  Min.   :0             Min.   :0             Min.   :0.6           #>  1st Qu.:0             1st Qu.:0             1st Qu.:0.6           #>  Median :0             Median :0             Median :0.6           #>  Mean   :0             Mean   :0             Mean   :0.6           #>  3rd Qu.:0             3rd Qu.:0             3rd Qu.:0.6           #>  Max.   :0             Max.   :0             Max.   :0.6           #>  NA's   :317           NA's   :317           NA's   :317           #>  rotation_months_y4_18   website           program_code       #>  Min.   :0.3           Length:318         Min.   :2.200e+09   #>  1st Qu.:0.3           Class :character   1st Qu.:2.202e+09   #>  Median :0.3           Mode  :character   Median :2.203e+09   #>  Mean   :0.3                              Mean   :2.203e+09   #>  3rd Qu.:0.3                              3rd Qu.:2.204e+09   #>  Max.   :0.3                              Max.   :2.206e+09   #>  NA's   :317                                                   # Perform data analysis and exploration"},{"path":"https://mufflyt.github.io/tyler/reference/apply_and_save_tint.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply and Save Image Tint — apply_and_save_tint","title":"Apply and Save Image Tint — apply_and_save_tint","text":"function applies tint image saves tinted image. tints determined given color palette.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/apply_and_save_tint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply and Save Image Tint — apply_and_save_tint","text":"","code":"apply_and_save_tint(   image_path,   alpha = 0.5,   palette_name = \"viridis\",   save_dir = \".\",   n_colors = 5 )"},{"path":"https://mufflyt.github.io/tyler/reference/apply_and_save_tint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply and Save Image Tint — apply_and_save_tint","text":"image_path Character string specifying file path image tinted. alpha Numeric value 0 1 specifying degree tint. Higher values result stronger tints. palette_name Character string specifying color palette use tint. Default 'viridis'. save_dir Character string specifying directory save tinted images. n_colors Integer specifying number different colors use color palette.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/apply_and_save_tint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply and Save Image Tint — apply_and_save_tint","text":"Saves tinted images specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2pdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"function takes Arsenal Table object saves PDF file \"tables\" directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2pdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"","code":"arsenal_tables_write2pdf(object, filename)"},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2pdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"object Arsenal Table object. filename string representing desired filename without extension.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2pdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save Arsenal Table as PDF — arsenal_tables_write2pdf","text":"","code":"if (FALSE) { # \\dontrun{ overall <- summary(   overall_arsenal_table,   text = T,   title = \"Table: Characteristics of Obstetrics and Gynecology Subspecialists Practicing at Obstetrics and Gynecology Residency Programs\",   pfootnote = FALSE ) arsenal_tables_write2pdf(overall, \"arsenal_overall_table_one\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2word.html","id":null,"dir":"Reference","previous_headings":"","what":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","title":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","text":"Writes Arsenal table object Word document.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2word.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","text":"","code":"arsenal_tables_write2word(object, filename)"},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2word.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","text":"object object written Word, typically Arsenal table. filename filename (without extension) Word document.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2word.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/arsenal_tables_write2word.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Writes an Arsenal table object to a Word document. — arsenal_tables_write2word","text":"","code":"if (FALSE) { # \\dontrun{ arsenal_tables_write2word(my_table, \"output_table\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"function calculates percentage common value specified categorical variable data frame.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"","code":"calcpercentages(df, variable)"},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"df data frame containing categorical variable. variable character string representing name categorical variable within df.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"data frame containing common value count, along percentage total count represents.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"function converts specified variable character type factor, counts occurrences unique value. identifies common value returns count percentage.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calcpercentages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Percentage of the Most Common Value in a Categorical Variable — calcpercentages","text":"","code":"# Example 1: Basic usage with a simple dataset df <- data.frame(category = c(\"A\", \"B\", \"A\", \"C\", \"A\", \"B\", \"B\", \"A\")) result <- calcpercentages(df, \"category\") print(result) #>   n #> 1 1  # Example 2: Using a dataset with multiple most common values df_tie <- data.frame(category = c(\"A\", \"B\", \"A\", \"B\", \"C\", \"C\", \"C\", \"A\", \"B\")) result <- calcpercentages(df_tie, \"category\") print(result) #>   n #> 1 1  # Example 3: Handling a dataset with missing values df_na <- data.frame(category = c(\"A\", NA, \"A\", \"C\", \"A\", \"B\", \"B\", NA)) result <- calcpercentages(df_na, \"category\") print(result) #>   n #> 1 1"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_descriptive_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"function calculates median, 25th percentile (Q1), 75th percentile (Q3) specified column dataframe. function includes detailed logging inputs, outputs, data transformation.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_descriptive_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"","code":"calculate_descriptive_stats(df, column, verbose = TRUE)"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_descriptive_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"df dataframe containing data. column string representing column name calculate descriptive statistics. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_descriptive_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"list containing median, 25th percentile (Q1), 75th percentile (Q3) specified column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_descriptive_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Descriptive Statistics with Robust Logging — calculate_descriptive_stats","text":"","code":"# Example: Calculate descriptive statistics for a column with logging stats <- calculate_descriptive_stats(df, \"business_days_until_appointment\", verbose = TRUE) #> Function calculate_descriptive_stats called with the following inputs: #>   Column: business_days_until_appointment  #>   Dataframe has rows and columns #> Error in calculate_descriptive_stats(df, \"business_days_until_appointment\",     verbose = TRUE): Column not found in the dataframe: business_days_until_appointment"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate Demographic Distribution with Logging — calculate_distribution","title":"Calculate Demographic Distribution with Logging — calculate_distribution","text":"function calculates distribution categorical variable within data frame, including counts percentages. also logs inputs, outputs, transformations transparency debugging purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate Demographic Distribution with Logging — calculate_distribution","text":"","code":"calculate_distribution(df, column)"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate Demographic Distribution with Logging — calculate_distribution","text":"df data frame containing data. column string representing name column distribution calculated.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate Demographic Distribution with Logging — calculate_distribution","text":"data frame count, total, percentage level specified column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate Demographic Distribution with Logging — calculate_distribution","text":"","code":"df <- data.frame(gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Female\", NA)) result <- calculate_distribution(df, \"gender\") #> Starting calculate_distribution... #> Input Data Frame: #>   gender #> 1   Male #> 2 Female #> 3 Female #> 4   Male #> 5   Male #> 6 Female #> Column to Calculate Distribution For: gender  #> Filtered Data Frame (NA removed): #>   gender #> 1   Male #> 2 Female #> 3 Female #> 4   Male #> 5   Male #> 6 Female #> Final Distribution Result: #> # A tibble: 1 × 4 #>   gender count total percent #>   <chr>  <int> <int>   <dbl> #> 1 Female     3     6      50 print(result) #> # A tibble: 1 × 4 #>   gender count total percent #>   <chr>  <int> <int>   <dbl> #> 1 Female     3     6      50"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_intersection_overlap_and_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"function calculates intersection block groups isochrones specific drive time saves results shapefile.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_intersection_overlap_and_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"","code":"calculate_intersection_overlap_and_save(   block_groups,   isochrones_joined,   drive_time,   output_dir )"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_intersection_overlap_and_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"block_groups sf object representing block groups. isochrones_joined sf object representing isochrones. drive_time drive time value calculate intersection. output_dir directory intersection shapefile saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_intersection_overlap_and_save.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"None. function saves intersection shapefile provides logging.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_intersection_overlap_and_save.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate intersection overlap and save results to shapefiles. — calculate_intersection_overlap_and_save","text":"","code":"calculate_intersection_overlap_and_save(block_groups, isochrones_joined, 30L, \"data/shp/\") #> Error: object 'block_groups' not found"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"function calculates proportion level specified categorical variable within data frame. returns data frame counts percentages level, logging process.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"","code":"calculate_proportion(df, variable_name, log_file = \"calculate_proportion.log\")"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"df data frame containing categorical variable. variable_name name categorical variable proportions calculated, passed unquoted expression. log_file path log file logs saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"data frame two columns: n (count level) percent (percentage total count represented level).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the Proportion of Each Level in a Categorical Variable with Logging — calculate_proportion","text":"function counts occurrences unique value specified variable calculates percentage value represents total count. percentages rounded two decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion_variable.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Proportion Variable — calculate_proportion_variable","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"function calculates returns proportion variable, maximum percentage tabulation result.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion_variable.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"","code":"calculate_proportion_variable(tabyl_result)"},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion_variable.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"tabyl_result tabulation result data frame.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/calculate_proportion_variable.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Proportion Variable — calculate_proportion_variable","text":"proportion variable percentage.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/check_normality.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Normality and Summarize Data — check_normality","title":"Check Normality and Summarize Data — check_normality","text":"function checks normality specified variable dataframe using Shapiro-Wilk test provides summary statistics (mean standard deviation normal, median IQR normal).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/check_normality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Normality and Summarize Data — check_normality","text":"","code":"check_normality(data, variable)"},{"path":"https://mufflyt.github.io/tyler/reference/check_normality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Normality and Summarize Data — check_normality","text":"data dataframe containing data. variable string specifying column name variable checked summarized.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/check_normality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Normality and Summarize Data — check_normality","text":"list containing summary statistics (mean standard deviation normal, median IQR normal).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/check_normality.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Normality and Summarize Data — check_normality","text":"","code":"# Example usage with a dataframe 'df' and outcome variable 'business_days_until_appointment' check_normality(df, \"business_days_until_appointment\") #> Starting normality check and summary calculation for variable: business_days_until_appointment #> Error in data[[variable]]: object of type 'closure' is not subsettable"},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"city_state_assign_scenarios function designed assign cases professionals based specialty location (city state). function particularly useful managing scenarios professionals, physicians healthcare workers, need assigned cases administrative analytical purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"","code":"city_state_assign_scenarios(   data,   generalist = \"General Dermatology\",   specialty = \"Pediatric Dermatology\",   case_names = c(\"Case Alpha\", \"Case Beta\", \"Case Gamma\"),   output_csv_path = \"Lizzy/data/city_state_assign_scenarios.csv\",   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. generalist character string specifying specialty name generalists. Default \"Generalist\". specialty character string specifying specialty name specialists. Default \"Specialist\". case_names character vector case names assign. Default c(\"Alpha\", \"Beta\", \"Gamma\"). output_csv_path character string specifying file path save output CSV. Default \"output/city_state_assign_scenarios.csv\". seed optional integer value set random seed reproducibility. Default NULL (seed set).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"data frame assigned cases.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"Generalists vs. Specialists: function differentiates generalists specialists, assigning cases accordingly. CSV Output: final output, including case assignments professional, saved CSV file using write_output_csv function.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"use-cases-","dir":"Reference","previous_headings":"","what":"Use Cases:","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"Healthcare Assignment: Assigning different types cases healthcare professionals based specialties cities/states practice. Research Studies: Managing scenarios research studies professionals need randomly assigned cases.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_assign_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Cases to Professionals by City and State — city_state_assign_scenarios","text":"","code":"# Example 1: Using default parameters data <- data.frame(   city = c(\"CityA\", \"CityA\", \"CityB\", \"CityB\"),   state_code = c(\"State1\", \"State1\", \"State2\", \"State2\"),   specialty_primary = c(\"Generalist\", \"Specialist\", \"Generalist\", \"Specialist\"),   stringsAsFactors = FALSE ) result <- city_state_assign_scenarios(data) #> File successfully saved to: Lizzy/data/city_state_assign_scenarios.csv  #> [1] \"Please check the column called `case_assigned` for the assigned scenario. Scenarios will be spread out across generalists but specialists will get every scenario.\" print(result) #> # A tibble: 4 × 4 #>   city  state_code specialty_primary case_assigned #>   <chr> <chr>      <chr>             <chr>         #> 1 CityA State1     Generalist        NA            #> 2 CityA State1     Specialist        NA            #> 3 CityB State2     Generalist        NA            #> 4 CityB State2     Specialist        NA"},{"path":"https://mufflyt.github.io/tyler/reference/city_state_check_specialty_generalist_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"function checks city-state combination required number generalists specialists. logs inputs, transformations, outputs, returns two data frames: one failing city-state-specialty combinations one successful combinations. Optionally, results can saved CSV files.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_check_specialty_generalist_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"","code":"city_state_check_specialty_generalist_counts(   data,   min_generalists,   min_specialists,   generalist_name = \"General Dermatology\",   specialist_name = \"Pediatric Dermatology\",   failing_csv_path = NULL,   successful_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/city_state_check_specialty_generalist_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. min_generalists integer specifying minimum number generalists required per city-state combination. min_specialists integer specifying minimum number specialists required per city-state combination. generalist_name string specifying specialty name generalists. Default \"General Dermatology\". specialist_name string specifying specialty name specialists. Default \"Pediatric Dermatology\". failing_csv_path optional string specifying file path save failing combinations CSV. Default NULL (file saved). successful_csv_path optional string specifying file path save successful combinations CSV. Default NULL (file saved).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_check_specialty_generalist_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"list containing two data frames: failing_combinations successful_combinations.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/city_state_check_specialty_generalist_counts.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — city_state_check_specialty_generalist_counts","text":"Generalists vs. Specialists: can specify names generalists specialists, function checks city-state combination required number . Logging: Extensive logging ensures inputs, transformations, results tracked. CSV Output: Optionally, function writes failing successful city-state-specialty combinations separate CSV files. Summary Logging: summary min_generalists, min_specialists, results logged end.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_sample_specialists.html","id":null,"dir":"Reference","previous_headings":"","what":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"function samples specialists generalists given dataset based city-state combinations. allows sampling three types specialists generalists customizable sample sizes . results can saved CSV file returned dataframe.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_sample_specialists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"","code":"city_state_sample_specialists(   data,   generalist = \"General Dermatology\",   specialist1 = \"Pediatric Dermatology\",   general_sample_size = 4,   specialist1_sample_size = 2,   specialist2 = NULL,   specialist2_sample_size = 0,   specialist3 = NULL,   specialist3_sample_size = 0,   same_phone_number = TRUE,   output_csv_path = NULL,   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/reference/city_state_sample_specialists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"data dataframe containing specialist information. must columns: city, state_code, specialty_primary, phone_number. generalist character string specifying generalist specialty sample. Default \"General Dermatology\". specialist1 character string specifying first specialist specialty sample. Default \"Pediatric Dermatology\". general_sample_size integer specifying many generalists sample city-state combination. Default 4. specialist1_sample_size integer specifying many first specialists sample city-state combination. Default 1. specialist2 character string specifying second specialist specialty sample. Optional. Default NULL. specialist2_sample_size integer specifying many second specialists sample. Default 0. specialist3 character string specifying third specialist specialty sample. Optional. Default NULL. specialist3_sample_size integer specifying many third specialists sample. Default 0. same_phone_number logical value indicating whether sample generalists specialists phone number (TRUE) different phone numbers (FALSE). Default TRUE. output_csv_path character string specifying path save output CSV. provided, result returned. seed integer setting seed reproducibility. Default 1978.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_sample_specialists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"dataframe containing sampled generalists specialists city-state combination.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/city_state_sample_specialists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"#' Sample Generalists and Specialists by City-State Combination — city_state_sample_specialists","text":"","code":"# Example 1: Basic usage with default generalist and specialist data <- data.frame(   city = rep(c(\"New York\", \"Los Angeles\"), each = 6),   state_code = rep(c(\"NY\", \"CA\"), each = 6),   specialty_primary = c(     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\"   ),   phone_number = rep(c(\"123\", \"456\", \"789\"), 4) ) result <- city_state_sample_specialists(data) print(result) #> # A tibble: 0 × 4 #> # Groups:   phone_number [0] #> # ℹ 4 variables: city <chr>, state_code <chr>, specialty_primary <chr>, #> #   phone_number <chr>"},{"path":"https://mufflyt.github.io/tyler/reference/clean_npi_entries.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean NPI Entries Function — clean_npi_entries","title":"Clean NPI Entries Function — clean_npi_entries","text":"function cleans NPI search results normalizing credentials, applying filters taxonomies, summarizing entries NPI. includes console logging key steps.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_npi_entries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean NPI Entries Function — clean_npi_entries","text":"","code":"clean_npi_entries(   npi_entries,   basic_credentials = c(\"MD\", \"DO\"),   taxonomy_filter = \"Obstetrics & Gynecology\" )"},{"path":"https://mufflyt.github.io/tyler/reference/clean_npi_entries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean NPI Entries Function — clean_npi_entries","text":"npi_entries dataframe containing NPI search results. basic_credentials character vector credentials filter (default c(\"MD\", \"\")). taxonomy_filter string filtering taxonomies (default \"Obstetrics & Gynecology\").","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_npi_entries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean NPI Entries Function — clean_npi_entries","text":"cleaned dataframe summarized NPI entries.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_npi_entries.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean NPI Entries Function — clean_npi_entries","text":"","code":"# Example 1: Basic cleaning of NPI entries with default parameters clean_npi_entries(npi_results) #> INFO [2024-10-30 12:19:18] Starting clean_npi_entries function #> INFO [2024-10-30 12:19:18] Input basic credentials: MD #> INFO [2024-10-30 12:19:18] Input basic credentials: DO #> INFO [2024-10-30 12:19:18] Input taxonomy filter: Obstetrics & Gynecology #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(npi_entries)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), clean_npi_entries(npi_results), logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             level = `<loglevel>`, .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results), .topenv = `<env>`,             namespace = \"tyler\"), do.call(formatter, c(res$params,             list(.logcall = substitute(.logcall), .topcall = substitute(.topcall),                 .topenv = .topenv))), `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results), .topenv = `<env>`),         withCallingHandlers(glue::glue(..., .envir = .topenv),             error = function(e) {                args <- paste0(capture.output(str(...)), collapse = \"\\n\")                stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                   args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                   \"\\n\\nPlease consider using another `log_formatter` or \",                   \"`skip_formatter` on strings with curly braces.\"))            }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(npi_entries)\"), .transformer(expr,             env) %||% .null, .transformer(expr, env), with_glue_error(eval(expr,             envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(npi_entries),         .handleSimpleError(`<fn>`, \"object 'npi_results' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'npi_results' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Initial NPI entries: {nrow(npi_entries)} rows\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(npi_entries)} #> Caused by error: #> ! object 'npi_results' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 2: Cleaning NPI entries, filtering for a specific taxonomy clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\") #> INFO [2024-10-30 12:19:18] Starting clean_npi_entries function #> INFO [2024-10-30 12:19:18] Input basic credentials: MD #> INFO [2024-10-30 12:19:18] Input basic credentials: DO #> INFO [2024-10-30 12:19:18] Input taxonomy filter: Anesthesiology #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(npi_entries)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\"),         logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             level = `<loglevel>`, .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\"),             .topenv = `<env>`, namespace = \"tyler\"), do.call(formatter,             c(res$params, list(.logcall = substitute(.logcall),                 .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results, taxonomy_filter = \"Anesthesiology\"),             .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(npi_entries)\"), .transformer(expr,             env) %||% .null, .transformer(expr, env), with_glue_error(eval(expr,             envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(npi_entries),         .handleSimpleError(`<fn>`, \"object 'npi_results' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'npi_results' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Initial NPI entries: {nrow(npi_entries)} rows\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(npi_entries)} #> Caused by error: #> ! object 'npi_results' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 3: Cleaning NPI entries, specifying different credentials clean_npi_entries(npi_results, basic_credentials = c(\"PA\", \"NP\")) #> INFO [2024-10-30 12:19:18] Starting clean_npi_entries function #> INFO [2024-10-30 12:19:18] Input basic credentials: PA #> INFO [2024-10-30 12:19:18] Input basic credentials: NP #> INFO [2024-10-30 12:19:18] Input taxonomy filter: Obstetrics & Gynecology #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(npi_entries)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), clean_npi_entries(npi_results, basic_credentials = c(\"PA\",             \"NP\")), logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             level = `<loglevel>`, .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results, basic_credentials = c(\"PA\",                 \"NP\")), .topenv = `<env>`, namespace = \"tyler\"),         do.call(formatter, c(res$params, list(.logcall = substitute(.logcall),             .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Initial NPI entries: {nrow(npi_entries)} rows\",             .logcall = logger::log_info(\"Initial NPI entries: {nrow(npi_entries)} rows\"),             .topcall = clean_npi_entries(npi_results, basic_credentials = c(\"PA\",                 \"NP\")), .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(npi_entries)\"), .transformer(expr,             env) %||% .null, .transformer(expr, env), with_glue_error(eval(expr,             envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(npi_entries),         .handleSimpleError(`<fn>`, \"object 'npi_results' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'npi_results' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Initial NPI entries: {nrow(npi_entries)} rows\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(npi_entries)} #> Caused by error: #> ! object 'npi_results' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces."},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_1_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean Phase 1 Results — clean_phase_1_results","title":"Clean Phase 1 Results — clean_phase_1_results","text":"Cleans, processes, prepares Phase 1 results data analysis. function allows row replication, optional label assignment, saving cleaned data CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_1_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean Phase 1 Results — clean_phase_1_results","text":"","code":"clean_phase_1_results(   phase1_input,   replicate_rows = NULL,   replication_labels = NULL,   csv_file_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_1_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean Phase 1 Results — clean_phase_1_results","text":"phase1_input data frame containing Phase 1 results data. replicate_rows optional integer specifying many times replicate rows (e.g., 2, 3, 4). Default NULL (replication). replication_labels optional character vector labels (e.g., races) assign replicated rows. Default NULL. csv_file_path optional path save cleaned data CSV file. NULL, file saved timestamp cleaned_data directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_1_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean Phase 1 Results — clean_phase_1_results","text":"cleaned data frame processing.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_1_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean Phase 1 Results — clean_phase_1_results","text":"","code":"if (FALSE) { # \\dontrun{ cleaned_phase1 <- clean_phase_1_results(my_data, 4, c(\"White\", \"Black\", \"Asian\", \"HIPI\"), \"output.csv\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_2_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and process Phase 2 data — clean_phase_2_data","title":"Clean and process Phase 2 data — clean_phase_2_data","text":"function reads data file data frame, cleans column names, applies renaming based specified criteria facilitate data analysis. function logs step process, including data loading, column cleaning, renaming transparency.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_2_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and process Phase 2 data — clean_phase_2_data","text":"","code":"clean_phase_2_data(   data_or_path,   required_strings,   standard_names,   output_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_2_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and process Phase 2 data — clean_phase_2_data","text":"data_or_path Path data file data frame. required_strings Vector substrings search column names. standard_names Vector new names apply matched columns. output_csv_path Optional. provided, cleaned data saved path.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_2_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and process Phase 2 data — clean_phase_2_data","text":"data frame processed data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/clean_phase_2_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and process Phase 2 data — clean_phase_2_data","text":"","code":"# Example 1: Cleaning data from a CSV file input_path <- \"path_to_your_data.csv\" required_strings <- c(\"physician_information\", \"able_to_contact_office\") standard_names <- c(\"physician_info\", \"contact_office\") cleaned_data <- clean_phase_2_data(input_path, required_strings, standard_names) #> --- Starting data cleaning process --- #> Input data or path:  path_to_your_data.csv  #> Required strings for renaming:  physician_information, able_to_contact_office  #> Standard names to apply:  physician_info, contact_office  #> Error in clean_phase_2_data(input_path, required_strings, standard_names): Error: File does not exist at the specified path: path_to_your_data.csv # cleaned_data will now have updated column names  # Example 2: Directly using a data frame df <- data.frame(   doc_info = 1:5,   contact_data = 6:10 ) required_strings <- c(\"doc_info\", \"contact_data\") standard_names <- c(\"doctor_info\", \"patient_contact_info\") cleaned_df <- clean_phase_2_data(df, required_strings, standard_names) #> --- Starting data cleaning process --- #> Input data or path:   #> Error in cat(\"Input data or path: \", data_or_path, \"\\n\"): argument 2 (type 'list') cannot be handled by 'cat' print(cleaned_df)  # Should show updated column names #> Error: object 'cleaned_df' not found  # Example 3: Attempting to clean with missing columns df2 <- data.frame(   appointment_date = 1:5,   patient_name = 6:10 ) required_strings <- c(\"doctor_info\", \"contact_data\")  # Note: these do not exist standard_names <- c(\"physician_info\", \"patient_contact_info\") cleaned_df2 <- clean_phase_2_data(df2, required_strings, standard_names) #> --- Starting data cleaning process --- #> Input data or path:   #> Error in cat(\"Input data or path: \", data_or_path, \"\\n\"): argument 2 (type 'list') cannot be handled by 'cat' print(cleaned_df2)  # Should issue warnings about missing columns #> Error: object 'cleaned_df2' not found"},{"path":"https://mufflyt.github.io/tyler/reference/convert_list_to_df_expanded.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"helper function converts named list column names, grouped table, expanded data frame column name placed separate column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/convert_list_to_df_expanded.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"","code":"convert_list_to_df_expanded(column_list)"},{"path":"https://mufflyt.github.io/tyler/reference/convert_list_to_df_expanded.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"column_list named list element contains column names table.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/convert_list_to_df_expanded.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"data frame table name corresponding columns.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/convert_list_to_df_expanded.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a List of Column Names to an Expanded Data Frame — convert_list_to_df_expanded","text":"","code":"# Example 1: Convert a list of column names to an expanded data frame test_list <- list(table1 = c(\"col1\", \"col2\"), table2 = c(\"col1\", \"col2\", \"col3\")) expanded_df <- convert_list_to_df_expanded(test_list) #> Converting list to data frame... #> Filtered tables:  #> Warning: no non-missing arguments to max; returning -Inf #> Error in seq_len(max_cols): argument must be coercible to non-negative integer print(expanded_df) #> Error: object 'expanded_df' not found  # Example 2: Handling missing columns in some tables test_list <- list(table1 = c(\"col1\", \"col2\"), table2 = c(\"col1\")) expanded_df <- convert_list_to_df_expanded(test_list) #> Converting list to data frame... #> Filtered tables:  #> Warning: no non-missing arguments to max; returning -Inf #> Error in seq_len(max_cols): argument must be coercible to non-negative integer print(expanded_df) #> Error: object 'expanded_df' not found  # Example 3: Convert an empty list empty_list <- list() expanded_df_empty <- convert_list_to_df_expanded(empty_list) #> Converting list to data frame... #> Filtered tables:  #> Warning: no non-missing arguments to max; returning -Inf #> Error in seq_len(max_cols): argument must be coercible to non-negative integer print(expanded_df_empty)  # Should return an empty data frame #> Error: object 'expanded_df_empty' not found"},{"path":"https://mufflyt.github.io/tyler/reference/count_physicians_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Count Physicians by State or Subdivision — count_physicians_by_group","title":"Count Physicians by State or Subdivision — count_physicians_by_group","text":"function counts number physicians available per state US Census Bureau subdivision. can optionally save counts CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_physicians_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count Physicians by State or Subdivision — count_physicians_by_group","text":"","code":"count_physicians_by_group(   data,   state_name_column = \"state_code\",   phone_column = \"phone_number\",   first_name_column = \"first\",   last_name_column = \"last\",   group_by = \"state\",   output_to_csv = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/count_physicians_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count Physicians by State or Subdivision — count_physicians_by_group","text":"data dataframe containing physician data. state_name_column column name dataframe containing state names abbreviations (default \"state_code\"). phone_column column name dataframe containing phone numbers (default \"phone_number\"). first_name_column column name dataframe containing first names (default \"first\"). last_name_column column name dataframe containing last names (default \"last\"). group_by string indicating whether group \"state\" \"subdivision\" (default \"state\"). output_to_csv (Optional) file path save state/subdivision counts CSV file. NULL, file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_physicians_by_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count Physicians by State or Subdivision — count_physicians_by_group","text":"tibble counts physicians per state subdivision.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_physicians_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count Physicians by State or Subdivision — count_physicians_by_group","text":"","code":"# Example 1: Count physicians by state count_physicians_by_group(taxonomy_and_aaos_data) #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'taxonomy_and_aaos_data' not found  # Example 2: Count physicians by U.S. Census Bureau subdivision count_physicians_by_group(taxonomy_and_aaos_data, group_by = \"subdivision\") #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'taxonomy_and_aaos_data' not found"},{"path":"https://mufflyt.github.io/tyler/reference/count_unique_physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"function filters dataframe physician data based insurance type, reason exclusion, appointment availability, counts number unique physicians meet criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_unique_physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"","code":"count_unique_physicians(   df,   insurance_type,   reason_for_exclusion = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/count_unique_physicians.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"df dataframe containing physician data. Must include columns 'insurance', 'reason_for_exclusions', 'business_days_until_appointment', 'phone'. insurance_type string specifying insurance type filter (e.g., \"Medicaid\"). reason_for_exclusion string specifying reason exclusion filter . Default NULL, includes rows regardless exclusion reason. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_unique_physicians.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"integer representing number unique physicians meet specified criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/count_unique_physicians.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count Unique Physicians Based on Insurance Type and Exclusion Reason — count_unique_physicians","text":"","code":"# Example 1: Counting unique physicians with specific insurance type and reason for exclusion df <- data.frame(   insurance = c(\"Medicaid\", \"Medicaid\", \"Blue Cross/Blue Shield\", \"Medicaid\"),   reason_for_exclusions = c(\"Able to contact\", \"Not available\", \"Able to contact\", \"Able to contact\"),   business_days_until_appointment = c(5, 0, 10, 3),   phone = c(\"123-456-7890\", \"123-456-7890\", \"098-765-4321\", \"234-567-8901\") ) unique_count <- count_unique_physicians(df, insurance_type = \"Medicaid\", reason_for_exclusion = \"Able to contact\") #> Starting count_unique_physicians function... #> Insurance Type: Medicaid #> Reason for Exclusion: Able to contact #> Filtered rows by insurance type: 3 remaining. #> Filtered rows by reason for exclusion: 2 remaining. #> Filtered rows with positive business days until appointment: 2 remaining. #> Error: 'nrow' is not an exported object from 'namespace:dplyr' print(unique_count)  # Expected output: 1 #> Error: object 'unique_count' not found  # Example 2: Counting unique physicians without specifying a reason for exclusion df2 <- data.frame(   insurance = c(\"Blue Cross/Blue Shield\", \"Blue Cross/Blue Shield\", \"Medicaid\"),   reason_for_exclusions = c(\"Able to contact\", \"Not available\", \"Able to contact\"),   business_days_until_appointment = c(3, 5, 1),   phone = c(\"321-654-0987\", \"321-654-0987\", \"654-321-0987\") ) unique_count2 <- count_unique_physicians(df2, insurance_type = \"Blue Cross/Blue Shield\") #> Starting count_unique_physicians function... #> Insurance Type: Blue Cross/Blue Shield #> No specific reason for exclusion is provided. #> Filtered rows by insurance type: 2 remaining. #> Filtered rows with positive business days until appointment: 2 remaining. #> Error: 'nrow' is not an exported object from 'namespace:dplyr' print(unique_count2)  # Expected output: 1 #> Error: object 'unique_count2' not found  # Example 3: Using verbose logging to see detailed steps df3 <- data.frame(   insurance = c(\"Medicaid\", \"Medicaid\", \"Medicaid\", \"Medicaid\"),   reason_for_exclusions = c(\"Able to contact\", \"Able to contact\", \"Not available\", \"Able to contact\"),   business_days_until_appointment = c(2, 1, 0, 4),   phone = c(\"111-222-3333\", \"111-222-3333\", \"222-333-4444\", \"333-444-5555\") ) unique_count3 <- count_unique_physicians(df3, insurance_type = \"Medicaid\", verbose = TRUE) #> Starting count_unique_physicians function... #> Insurance Type: Medicaid #> No specific reason for exclusion is provided. #> Filtered rows by insurance type: 4 remaining. #> Filtered rows with positive business days until appointment: 3 remaining. #> Error: 'nrow' is not an exported object from 'namespace:dplyr' print(unique_count3)  # Expected output: 2 #> Error: object 'unique_count3' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"function reads data specified file, fits generalized linear mixed model (GLMM) specified interaction term, creates plot visualize interaction effects. plot saved specified directory. particularly useful analyzing two categorical variables interact affect response variable, controlling random intercept.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"","code":"create_and_plot_interaction(   data_path,   response_variable,   variable_of_interest,   interaction_variable,   random_intercept,   output_path,   resolution = 100 )"},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"data_path character string specifying path .rds file containing dataset. response_variable character string specifying name response variable dataset. variable_of_interest character string specifying first categorical predictor variable interaction. interaction_variable character string specifying second categorical predictor variable interaction. random_intercept character string specifying variable used random intercept model (e.g., \"city\"). output_path character string specifying directory interaction plot saved. resolution integer specifying resolution (DPI) saving plot. Defaults 100.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"list containing fitted GLMM (model) summarized data used effects plot (effects_plot_data).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"function performs following steps: Reads data provided file path. Renames converts specified columns appropriate types. Fits Poisson GLMM interaction term random intercept. Summarizes interaction effects creates plot. Saves plot specified directory. function useful scenarios want examine interaction two categorical variables combined effect count-based response variable (e.g., number business days appointment). can handle complex data structures controls variability across groups using random intercepts.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_plot_interaction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and Plot Interaction Effects in GLMM — create_and_plot_interaction","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Analyzing the effect of gender and appointment center on wait times result <- create_and_plot_interaction(   data_path = \"data/phase2_analysis.rds\",   response_variable = \"business_days_until_appointment\",   variable_of_interest = \"central_number_e_g_appointment_center\",   interaction_variable = \"gender\",   random_intercept = \"city\",   output_path = \"results/figures\",   resolution = 100 )  # Example 2: Examining the interaction between insurance type and scenario on appointment delays result <- create_and_plot_interaction(   data_path = \"data/healthcare_calls.rds\",   response_variable = \"business_days_until_appointment\",   variable_of_interest = \"insurance_type\",   interaction_variable = \"scenario\",   random_intercept = \"state\",   output_path = \"results/insurance_scenario_interaction\",   resolution = 150 )  # Example 3: Studying the interaction between gender and subspecialty in wait times result <- create_and_plot_interaction(   data_path = \"data/mystery_caller_study.rds\",   response_variable = \"waiting_time_days\",   variable_of_interest = \"subspecialty\",   interaction_variable = \"gender\",   random_intercept = \"clinic_id\",   output_path = \"results/waiting_times\",   resolution = 300 ) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_and_save_physician_dot_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"function creates Leaflet dot map physicians using longitude latitude coordinates. also adds ACOG district boundaries map saves HTML file accompanying PNG screenshot.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_save_physician_dot_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"","code":"create_and_save_physician_dot_map(   physician_data,   jitter_range = 0.05,   color_palette = \"magma\",   popup_var = \"name\" )"},{"path":"https://mufflyt.github.io/tyler/reference/create_and_save_physician_dot_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"physician_data sf object containing physician data \"long\" \"lat\" columns. jitter_range range adding jitter latitude longitude coordinates. color_palette color palette ACOG district colors. popup_var variable use popup text.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_save_physician_dot_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_and_save_physician_dot_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create and Save a Leaflet Dot Map of Physicians — create_and_save_physician_dot_map","text":"","code":"if (FALSE) { # \\dontrun{ # Load required libraries library(viridis) library(leaflet)  # Generate physician data (replace with your own data) physician_data <- data.frame(   long = c(-95.363271, -97.743061, -98.493628, -96.900115, -95.369803),   lat = c(29.763283, 30.267153, 29.424349, 32.779167, 29.751808),   name = c(\"Physician 1\", \"Physician 2\", \"Physician 3\", \"Physician 4\", \"Physician 5\"),   ACOG_District = c(\"District I\", \"District II\", \"District III\", \"District IV\", \"District V\") )  # Create and save the dot map create_and_save_physician_dot_map(physician_data) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_bar_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"function generates bar plot based categorical variable facets plot grouping variable. plot title automatically includes total sample size (N). function also supports custom axis labels, returns plot object manipulation saving.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_bar_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"","code":"create_bar_plot(   input_data,   category_var,   grouping_var,   title = NULL,   x_axis_label = NULL,   y_axis_label = \"Count\",   output_directory = \"output\",   filename_prefix = \"bar_plot\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_bar_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"input_data dataframe containing data plotted. category_var string representing column name categorical variable x-axis (e.g., insurance type). grouping_var string representing column name facet wrap (grouping variable). title string specifying title plot. Default NULL, function generate title based category_var grouping_var. x_axis_label string specifying label x-axis. Default NULL, function use column name x-axis variable. y_axis_label string specifying label y-axis. Default \"Count\". output_directory string representing directory plot files saved. Default \"output\". filename_prefix string used prefix generated plot filenames. Default \"bar_plot\". verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_bar_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"function returns plot object manipulation saving.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_bar_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Bar Plot with Total Sample Size in the Title — create_bar_plot","text":"","code":"# Example 1: Basic usage with a categorical and facet variable create_bar_plot(   input_data = my_data,   category_var = \"insurance_type\",   grouping_var = \"region\",   title = \"Insurance Type Distribution by Region\",   x_axis_label = \"Insurance Type\",   y_axis_label = \"Number of Observations\" ) #> Error in create_bar_plot(input_data = my_data, category_var = \"insurance_type\",     grouping_var = \"region\", title = \"Insurance Type Distribution by Region\",     x_axis_label = \"Insurance Type\", y_axis_label = \"Number of Observations\"): could not find function \"create_bar_plot\"  # Example 2: Using default axis labels and auto-generated title create_bar_plot(   input_data = my_data,   category_var = \"insurance_type\",   grouping_var = \"region\" ) #> Error in create_bar_plot(input_data = my_data, category_var = \"insurance_type\",     grouping_var = \"region\"): could not find function \"create_bar_plot\"  # Example 3: Saving the plot with a custom filename prefix and disabling verbose output create_bar_plot(   input_data = my_data,   category_var = \"insurance_type\",   grouping_var = \"region\",   output_directory = \"plots\",   filename_prefix = \"insurance_vs_region\",   verbose = FALSE ) #> Error in create_bar_plot(input_data = my_data, category_var = \"insurance_type\",     grouping_var = \"region\", output_directory = \"plots\", filename_prefix = \"insurance_vs_region\",     verbose = FALSE): could not find function \"create_bar_plot\""},{"path":"https://mufflyt.github.io/tyler/reference/create_base_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Base Leaflet Map — create_base_map","title":"Create a Base Leaflet Map — create_base_map","text":"function creates base Leaflet map custom title.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_base_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Base Leaflet Map — create_base_map","text":"","code":"create_base_map(title)"},{"path":"https://mufflyt.github.io/tyler/reference/create_base_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Base Leaflet Map — create_base_map","text":"title character string containing HTML title map.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_base_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Base Leaflet Map — create_base_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_base_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Base Leaflet Map — create_base_map","text":"","code":"if (FALSE) { # \\dontrun{ # Create a base map with a custom title my_map <- create_base_map(\"<h1>Custom Map Title<\/h1>\")  # Display the map and add circle markers my_map <- my_map %>%   leaflet::addCircleMarkers(lng = ~longitude,                            lat = ~latitude,                            data = data_points,                            popup = ~popup_text,                            radius = ~radius,                            color = ~color,                            fill = TRUE,                            stroke = FALSE,                            fillOpacity = 0.8) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_block_group_overlap_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"function creates map displays block groups overlap isochrones.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_block_group_overlap_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"","code":"create_block_group_overlap_map(   bg_data,   isochrones_data,   output_dir = \"figures/\" )"},{"path":"https://mufflyt.github.io/tyler/reference/create_block_group_overlap_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"bg_data SpatialPolygonsDataFrame representing block group data. isochrones_data SpatialPolygonsDataFrame representing isochrone data. output_dir Directory path exporting map files. Default \"figures/\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_block_group_overlap_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_block_group_overlap_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to create and export a map showing block group overlap with isochrones — create_block_group_overlap_map","text":"","code":"if (FALSE) { # \\dontrun{ # Create and export the map with the default output directory create_block_group_overlap_map(block_groups, isochrones_joined_map)  # Create and export the map with a custom output directory create_block_group_overlap_map(block_groups, isochrones_joined_map, \"custom_output/\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_density_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"function generates density plot designed mystery caller studies, allowing visualization waiting times similar outcomes across different categories, insurance types. function supports transformations x-axis custom labels.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_density_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"","code":"create_density_plot(   data,   x_var,   fill_var,   x_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"density_plot\",   x_label = NULL,   y_label = \"Density\",   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_density_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"data dataframe containing data plotted. Must contain variables specified x_var fill_var. x_var string representing column name x-axis variable. numeric variable (e.g., waiting time days). fill_var string representing column name fill variable. categorical factor variable (e.g., insurance type). x_transform string specifying transformation x-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"density_plot\". x_label string specifying label x-axis. Default NULL (uses x_var). y_label string specifying label y-axis. Default \"Density\". plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_density_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_density_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Density Plot for Mystery Caller Studies with Optional Transformations — create_density_plot","text":"","code":"# Example 1: Basic density plot with log transformation create_density_plot(     data = df3,     x_var = \"business_days_until_appointment\",     fill_var = \"insurance\",     x_transform = \"log\",  # Log transformation     dpi = 100,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_density\",     x_label = \"Log (Waiting Times in Days)\",     y_label = \"Density\",     plot_title = \"Density Plot of Waiting Times by Insurance\" ) #> Error: object 'df3' not found  # Example 2: Density plot with square root transformation create_density_plot(     data = df3,     x_var = \"business_days_until_appointment\",     fill_var = \"insurance\",     x_transform = \"sqrt\",  # Square root transformation     dpi = 150,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_density_sqrt\",     x_label = \"Sqrt (Waiting Times in Days)\",     y_label = \"Density\",     plot_title = \"Square Root Transformed Density Plot\" ) #> Error: object 'df3' not found  # Example 3: Density plot without any transformation create_density_plot(     data = df3,     x_var = \"business_days_until_appointment\",     fill_var = \"insurance\",     x_transform = \"none\",  # No transformation     dpi = 200,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_density_none\",     x_label = \"Waiting Times in Days\",     y_label = \"Density\",     plot_title = \"Density Plot Without Transformation\" ) #> Error: object 'df3' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_dot_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"function generates dot plot visualizing median values error bars across different categories, insurance types. includes error handling, meaningful variable names, default behaviors ease use. Extensive logging tracks inputs, transformations, outputs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_dot_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"","code":"create_dot_plot(   dataset,   category_var,   value_var = \"median_days\",   lower_bound_var = \"q1\",   upper_bound_var = \"q3\",   dpi = 100,   output_directory = \"output\",   filename_prefix = \"dot_plot\",   x_label = NULL,   y_label = NULL,   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_dot_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"dataset dataframe containing data plotted. Must contain variables specified category_var, value_var, lower_bound_var, upper_bound_var. category_var string representing column name categorical variable x-axis (e.g., insurance type). value_var string representing column name numeric variable y-axis (e.g., median days). lower_bound_var string representing column name lower bound error bars (e.g., first quartile). upper_bound_var string representing column name upper bound error bars (e.g., third quartile). dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_directory string representing directory plot files saved. Default \"output\". filename_prefix string used prefix generated plot filenames. filenames timestamp appended uniqueness. x_label string specifying label x-axis. Default NULL (uses category_var). y_label string specifying label y-axis. Default NULL (uses value_var). plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_dot_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_dot_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Dot Plot with Error Bars for Mystery Caller Studies with Logging and Error Handling — create_dot_plot","text":"","code":"create_dot_plot(     dataset = df_plot,     category_var = \"insurance\",     value_var = \"median_days\",     lower_bound_var = \"q1\",     upper_bound_var = \"q3\",     dpi = 100,     output_directory = \"output/plots\",     filename_prefix = \"insurance_vs_days\",     x_label = \"Insurance\",     y_label = \"Median Business Days\",     plot_title = \"Comparison of Business Days by Insurance\" ) #> Error: object 'df_plot' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_forest_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"function creates forest plot significant predictors, showing coefficients confidence intervals. function logs process, including inputs outputs transparency.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_forest_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"","code":"create_forest_plot(df, target_variable, significant_vars)"},{"path":"https://mufflyt.github.io/tyler/reference/create_forest_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"df data frame containing dataset. target_variable string representing name target variable. significant_vars data frame containing significant predictors, coefficients, confidence intervals.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_forest_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"ggplot object representing forest plot.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_forest_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Forest Plot for Significant Predictors with Logging — create_forest_plot","text":"","code":"# Example usage:  # 1. Prepare the dataset predictor_vars <- prepare_dataset(df, target_variable = \"cleaned_does_the_physician_accept_medicaid_numeric\",                                   excluded_columns = c(\"does_the_physician_accept_medicaid\",                                                        \"cleaned_does_the_physician_accept_medicaid\")) #> Preparing dataset... #> Target Variable: cleaned_does_the_physician_accept_medicaid_numeric  #> Excluded Columns: does_the_physician_accept_medicaid, cleaned_does_the_physician_accept_medicaid  #> Predictor Variables Identified:    # 2. Fit Poisson models and extract p-values results <- fit_poisson_models(df, target_variable = \"cleaned_does_the_physician_accept_medicaid_numeric\",                               predictor_vars = predictor_vars) #> Error in fit_poisson_models(df, target_variable = \"cleaned_does_the_physician_accept_medicaid_numeric\",     predictor_vars = predictor_vars): unused arguments (target_variable = \"cleaned_does_the_physician_accept_medicaid_numeric\", predictor_vars = predictor_vars)  # 3. Determine the direction of significant variables significant_vars <- results %>%   dplyr::filter(P_Value < 0.2)  # Apply significance filter #> Error: object 'results' not found significant_vars_with_direction <- determine_direction(df, \"cleaned_does_the_physician_accept_medicaid_numeric\",                                                        significant_vars) #> Starting the determine_direction function... #> Target Variable: cleaned_does_the_physician_accept_medicaid_numeric  #> Significant Variables Data Frame: #> Error: object 'significant_vars' not found  # 4. Create a forest plot for significant predictors forest_plot <- create_forest_plot(df, \"cleaned_does_the_physician_accept_medicaid_numeric\",                                   significant_vars_with_direction) #> Creating forest plot... #> Target Variable: cleaned_does_the_physician_accept_medicaid_numeric  #> Error: object 'significant_vars_with_direction' not found print(forest_plot) #> Error: object 'forest_plot' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_formula.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Formula for Poisson Model — create_formula","title":"Create a Formula for Poisson Model — create_formula","text":"function creates formula Poisson model based provided data, response variable, optional random effect.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_formula.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Formula for Poisson Model — create_formula","text":"","code":"create_formula(data, response_var, random_effect = NULL)"},{"path":"https://mufflyt.github.io/tyler/reference/create_formula.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Formula for Poisson Model — create_formula","text":"data dataframe containing predictor response variables. response_var name response variable dataframe. random_effect Optional. name random effect variable formula.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_formula.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Formula for Poisson Model — create_formula","text":"formula object suitable modeling R.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_formula.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Formula for Poisson Model — create_formula","text":"","code":"# Example usage: response_variable <- \"days\" random_effect_term <- \"name\"  # Change this to the desired random effect variable df3_filtered <- data.frame(days = c(5, 10, 15), age = c(30, 40, 50), name = c(\"A\", \"B\", \"C\")) formula <- create_formula(df3_filtered, response_variable, random_effect_term) #> Creating formula with response variable: days  #> Predictor variables identified: age, name  #> Predictor variables after formatting: `age`, `name`  #> Initial formula string: days ~ `age` + `name`  #> Formula string with random effect: days ~ `age` + `name` + (1 | name )  #> Final formula object created: #> days ~ age + name + (1 | name) #> <environment: 0x7feb671534d8> formula #> days ~ age + name + (1 | name) #> <environment: 0x7feb671534d8>"},{"path":"https://mufflyt.github.io/tyler/reference/create_geocode.html","id":null,"dir":"Reference","previous_headings":"","what":"Geocode Unique Addresses — create_geocode","title":"Geocode Unique Addresses — create_geocode","text":"function geocodes unique addresses using Google Maps API appends latitude longitude original dataset. Please ensure every data set must column named 'address'.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_geocode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geocode Unique Addresses — create_geocode","text":"","code":"create_geocode(csv_file)"},{"path":"https://mufflyt.github.io/tyler/reference/create_geocode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geocode Unique Addresses — create_geocode","text":"file_path Path input file (CSV, RDS, XLSX) containing address data. google_maps_api_key Google Maps API key. output_file_path Path output CSV file geocoded data saved. (Optional) location sf object representing location isolines calculated. range numeric vector time ranges seconds. posix_time POSIXct object representing date time calculation. Default \"2023-10-20 08:00:00\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_geocode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geocode Unique Addresses — create_geocode","text":"dataframe containing geocoded address data latitude longitude. list isolines different time ranges, error message calculation fails.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_geocode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geocode Unique Addresses — create_geocode","text":"","code":"if (FALSE) { # \\dontrun{ # Define the input file path, Google Maps API key, and output file path (optional) file_path <- \"input_data.csv\" google_maps_api_key <- \"your_api_key\" output_file_path <- \"output_data.csv\"  # Optional  # Call the geocode_unique_addresses function with or without specifying output_file_path geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key) # or geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key, output_file_path) } # }  if (FALSE) { # \\dontrun{  # Set your HERE API key in your Renviron file using the following steps: # 1. Add key to .Renviron Sys.setenv(HERE_API_KEY = \"your_api_key_here\") # 2. Reload .Renviron readRenviron(\"~/.Renviron\")  # Define a sf object for the location location <- sf::st_point(c(-73.987, 40.757))  # Calculate isolines for the location with a 30-minute, 60-minute, 120-minute, and 180-minute range isolines <- create_isochrones(location = location, range = c(1800, 3600, 7200, 10800))  # Print the isolines print(isolines)  } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_individual_isochrone_plots.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"function creates individual Leaflet maps shapefiles specified drive times based isochrone data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_individual_isochrone_plots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"","code":"create_individual_isochrone_plots(isochrones, drive_times)"},{"path":"https://mufflyt.github.io/tyler/reference/create_individual_isochrone_plots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"isochrones sf object containing isochrone data. drive_times vector unique drive times (minutes) maps shapefiles created.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_individual_isochrone_plots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"None. function creates saves individual maps shapefiles.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_individual_isochrone_plots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Individual Isochrone Maps and Shapefiles — create_individual_isochrone_plots","text":"","code":"if (FALSE) { # \\dontrun{ # Load required libraries library(sf) library(leaflet) library(tyler)  # Load isochrone data isochrones <- readRDS(\"path_to_isochrones.rds\")  # List of unique drive times for which you want to create plots and shapefiles drive_times <- unique(isochrones$drive_time)  # Create individual isochrone maps and shapefiles create_individual_isochrone_plots(isochrones, drive_times) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_insurance_by_insurance_scatter_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"function creates scatter plot comparing waiting times (days) appointment two different insurance types. plot saved TIFF PNG file specified output directory. function allows customization plot aesthetics, including axis labels, point size, alpha transparency. logs process ensures output directory exists.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_insurance_by_insurance_scatter_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"","code":"create_insurance_by_insurance_scatter_plot(   df,   unique_variable,   insurance1 = \"medicaid\",   insurance2 = \"blue cross/blue shield\",   output_directory = \"ortho_sports_med/figures/\",   dpi = 100,   height = 8,   width = 11,   x_label = \"Time in days to appointment\\nBlue Cross Blue Shield (Log Scale)\",   y_label = \"Time in days to appointment\\nMedicaid (Log Scale)\",   plot_title = \"Comparison of Waiting Times: Medicaid vs Blue Cross Blue Shield\",   point_size = 3,   point_alpha = 0.6,   add_confidence_interval = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_insurance_by_insurance_scatter_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"df data frame containing data plotted. unique_variable string representing column name uniquely identifies entity data (e.g., \"phone\" \"npi\"). insurance1 string representing first insurance type compared (default \"medicaid\"). insurance2 string representing second insurance type compared (default \"blue cross/blue shield\"). output_directory string specifying directory plot files saved. default \"ortho_sports_med/figures/\". dpi integer specifying resolution saved plot files dots per inch (DPI). default 100. height numeric value specifying height saved plot files inches. default 8 inches. width numeric value specifying width saved plot files inches. default 11 inches. x_label string representing label x-axis. default \"Time days appointment Blue Cross Blue Shield (Log Scale)\". y_label string representing label y-axis. default \"Time days appointment Medicaid (Log Scale)\". plot_title string representing title plot. default \"Comparison Waiting Times: Medicaid vs Blue Cross Blue Shield\". point_size numeric value specifying size points scatter plot. default 3. point_alpha numeric value 0 1 specifying transparency level points scatter plot. default 0.6. add_confidence_interval logical value indicating whether add confidence interval around linear fit line. default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_insurance_by_insurance_scatter_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"ggplot2 scatter plot object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_insurance_by_insurance_scatter_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types — create_insurance_by_insurance_scatter_plot","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Basic usage with default values scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3,  # The data frame to be used   unique_variable = \"phone\"  # The unique identifier variable )  # Example 2: Custom DPI, height, and width for better resolution scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3,  # The data frame to be used   unique_variable = \"phone\",  # The unique identifier variable   dpi = 300,  # Higher DPI for better resolution   height = 10,  # Custom height for the plot   width = 15  # Custom width for the plot )  # Example 3: Customized axis labels, plot title, and transparency settings scatterplot <- create_insurance_by_insurance_scatter_plot(   df = df3,  # The data frame to be used   unique_variable = \"npi\",  # The unique identifier variable   x_label = \"Appointment Time (days) - Blue Cross Blue Shield (Log Scale)\",  # Custom x-axis label   y_label = \"Appointment Time (days) - Medicaid (Log Scale)\",  # Custom y-axis label   plot_title = \"Custom Plot Title: Waiting Times Comparison\",  # Custom plot title   point_alpha = 0.8  # Custom transparency for points ) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Memoized function to try a location with isoline calculations — create_isochrones","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"function calculates isolines given location using hereR package.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"","code":"create_isochrones(   location,   range,   posix_time = as.POSIXct(\"2023-10-20 08:00:00\", format = \"%Y-%m-%d %H:%M:%S\") )"},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"location sf object representing location isolines calculated. range numeric vector time ranges seconds. posix_time POSIXct object representing date time calculation. Default \"2023-10-20 08:00:00\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"list isolines different time ranges, error message calculation fails.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Memoized function to try a location with isoline calculations — create_isochrones","text":"","code":"if (FALSE) { # \\dontrun{ # Set your HERE API key in your Renviron file using the following steps: # 1. Add key to .Renviron Sys.setenv(HERE_API_KEY = \"your_api_key_here\") # 2. Reload .Renviron readRenviron(\"~/.Renviron\")  # Define a sf object for the location location <- sf::st_point(c(-73.987, 40.757))  # Calculate isolines for the location with a 30-minute, 60-minute, 120-minute, and 180-minute range isolines <- create_isochrones(location = location, range = c(1800, 3600, 7200, 10800))  # Print the isolines print(isolines)  } # }"},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones_for_dataframe.html","id":null,"dir":"Reference","previous_headings":"","what":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"function retrieves isochrones point given dataframe looping rows calling create_isochrones function point.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones_for_dataframe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"","code":"create_isochrones_for_dataframe(   input_file,   breaks = c(1800, 3600, 7200, 10800) )"},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones_for_dataframe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"input_file path input file containing points isochrones retrieved. breaks numeric vector specifying breaks categorizing drive times (default c(1800, 3600, 7200, 10800)).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_isochrones_for_dataframe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get isochrones for each point in a dataframe — create_isochrones_for_dataframe","text":"dataframe containing isochrones data added 'name' column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_line_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"function creates line plot using ggplot2 options transforming y-axis, grouping lines, saving plot specified resolution. plot can saved TIFF PNG formats automatic filename generation.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_line_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"","code":"create_line_plot(   data,   x_var,   y_var,   y_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"line_plot\",   use_geom_line = FALSE,   geom_line_group = NULL,   point_color = \"viridis\",   line_color = \"red\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_line_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"data dataframe containing data plotted. Must contain variables specified x_var y_var. x_var string representing column name x-axis variable. categorical factor variable. y_var string representing column name y-axis variable. numeric variable. y_transform string specifying transformation y-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"line_plot\". use_geom_line boolean indicating whether include lines connecting points grouped data. Default FALSE. geom_line_group string representing column name group lines use_geom_line TRUE. categorical factor variable. point_color string specifying color points. Default \"viridis\", uses viridis color palette. line_color string specifying color summary line (median). Default \"red\". verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_line_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"function saves plot specified directory returns value.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_line_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Line Plot with Optional Transformations and Grouping — create_line_plot","text":"","code":"# Example 1: Basic plot with log transformation create_line_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"log\",  # Log transformation     dpi = 100,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance\" ) #> Error: object 'df3' not found  # Example 2: Plot with square root transformation and lines grouped by 'last' create_line_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"sqrt\",  # Square root transformation     dpi = 150,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_sqrt\",     use_geom_line = TRUE,  # Include lines     geom_line_group = \"last\"  # Group lines by 'last' column ) #> Error: object 'df3' not found  # Example 3: Plot without any transformation and without lines create_line_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"none\",  # No transformation     dpi = 200,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_none\" ) #> Error: object 'df3' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_region_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","title":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"function generates map U.S. states, coloring regions, divisions, ACOG districts, ENT Board Governors Regions. Users can customize grouping use (ACOG Districts, ENT_Board_of_Governors_Regions, US Census Subdivisions).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_region_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"","code":"create_region_map(   remove_ak_hi = TRUE,   districts_per_group = \"acog_districts\",   save_path = NULL,   alpha_level = 0.4,   title = \"U.S. States by Region/Division or Custom Districts\",   subtitle = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/create_region_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"remove_ak_hi Logical, whether exclude Alaska Hawaii map. Default TRUE. districts_per_group Character, grouping use regions. Options : \"acog_districts\", \"ENT_Board_of_Governors_Regions\", \"US_Census_Subdivisions\". save_path Character, optional file path save map image. NULL, map saved. Default NULL. alpha_level Numeric, transparency level map's fill color, 0 fully transparent 1 fully opaque. Default 0.2. title Character, title map. Default \"U.S. States Region/Division Custom Districts\". subtitle Character, optional subtitle map.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_region_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"ggplot map object representing U.S. states colored region/division, ACOG districts, ENT_Board_of_Governors_Regions.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_region_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Main function to create a map of U.S. States by Region, Division, or Custom Districts — create_region_map","text":"","code":"# Example 1: Create a map of U.S. states by region/division, excluding Alaska and Hawaii create_region_map(remove_ak_hi = TRUE, districts_per_group = \"US_Census_Subdivisions\", alpha_level = 0.2) #> Logging inputs... #> Error in validate_inputs(remove_ak_hi, districts_per_group, alpha_level,     save_path): Error: 'significant_predictors' must be a data frame.  # Example 2: Create a map of U.S. states by ACOG districts, including Alaska and Hawaii create_region_map(remove_ak_hi = FALSE, districts_per_group = \"acog_districts\", alpha_level = 0.5) #> Logging inputs... #> Error in validate_inputs(remove_ak_hi, districts_per_group, alpha_level,     save_path): Error: 'significant_predictors' must be a data frame.  # Example 3: Create a map of U.S. states by ENT Board of Governors Regions and save it to a file create_region_map(remove_ak_hi = TRUE, districts_per_group = \"ENT_Board_of_Governors_Regions\", save_path = \"ent_bog_regions_map.png\", alpha_level = 0.3) #> Logging inputs... #> Error in validate_inputs(remove_ak_hi, districts_per_group, alpha_level,     save_path): Error: 'significant_predictors' must be a data frame."},{"path":"https://mufflyt.github.io/tyler/reference/create_scatter_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"function generates scatter plot designed mystery caller studies, allowing visualization waiting times similar outcomes across different categories, insurance types. function supports transformations y-axis, custom jitter, colors category x-axis using viridis color palette. plot automatically displayed saved specified resolution.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_scatter_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"","code":"create_scatter_plot(   data,   x_var,   y_var,   y_transform = \"none\",   dpi = 100,   output_dir = \"output\",   file_prefix = \"scatter_plot\",   jitter_width = 0.2,   jitter_height = 0,   point_alpha = 0.6,   x_label = NULL,   y_label = NULL,   plot_title = NULL,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/create_scatter_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"data dataframe containing data plotted. Must contain variables specified x_var y_var. x_var string representing column name x-axis variable. categorical factor variable (e.g., insurance type). y_var string representing column name y-axis variable. numeric variable (e.g., waiting time days). y_transform string specifying transformation y-axis: \"log\" log transformation (log1p), \"sqrt\" square root transformation, \"none\" transformation. Default \"none\". dpi integer specifying resolution saved plot dots per inch (DPI). Default 100. output_dir string representing directory plot files saved. Default \"output\". file_prefix string used prefix generated plot filenames. filenames timestamp appended ensure uniqueness. Default \"scatter_plot\". jitter_width numeric value specifying width jitter along x-axis. Default 0.2. jitter_height numeric value specifying height jitter along y-axis. Default 0. point_alpha numeric value specifying transparency level points. Default 0.6. x_label string specifying label x-axis. Default NULL (uses x_var). y_label string specifying label y-axis. Default NULL (uses y_var transformed version). plot_title string specifying title plot. Default NULL (title). verbose boolean indicating whether print messages saved plot locations. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_scatter_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"function displays plot saves specified directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_scatter_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels — create_scatter_plot","text":"","code":"# Example 1: Basic scatter plot with log transformation create_scatter_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"log\",  # Log transformation     dpi = 100,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance\",     x_label = \"Insurance\",     y_label = \"Log (Waiting Times in Days)\",     plot_title = \"Scatter Plot of Waiting Times by Insurance\" ) #> Error: object 'df3' not found  # Example 2: Scatter plot with square root transformation and custom jitter create_scatter_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"sqrt\",  # Square root transformation     dpi = 150,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_sqrt\",     jitter_width = 0.3,     jitter_height = 0.1,     x_label = \"Insurance\",     y_label = \"Square Root (Waiting Times in Days)\",     plot_title = \"Square Root Transformed Scatter Plot\" ) #> Error: object 'df3' not found  # Example 3: Scatter plot without any transformation and increased transparency create_scatter_plot(     data = df3,     x_var = \"insurance\",     y_var = \"business_days_until_appointment\",     y_transform = \"none\",  # No transformation     dpi = 200,     output_dir = \"ortho_sports_med/Figures\",     file_prefix = \"ortho_sports_vs_insurance_none\",     point_alpha = 0.8,     x_label = \"Insurance\",     y_label = \"Waiting Times in Days\",     plot_title = \"Scatter Plot Without Transformation\" ) #> Error: object 'df3' not found"},{"path":"https://mufflyt.github.io/tyler/reference/create_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"function generates summary sentence based significant predictor variables Medicaid acceptance rates. logs process, performs error checking, includes default behavior easy testing robust execution.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"","code":"create_summary_sentence(   significant_predictors,   medicaid_acceptance_rate,   accepted_medicaid_count,   total_medicaid_physicians_count )"},{"path":"https://mufflyt.github.io/tyler/reference/create_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"significant_predictors data frame containing significant predictor variables, directions, formatted p-values. columns: \"Variable\", \"Direction\", \"Formatted_P_Value\". medicaid_acceptance_rate numeric value representing Medicaid acceptance rate (percentage). accepted_medicaid_count integer representing count physicians accepting Medicaid. total_medicaid_physicians_count integer representing total count physicians considered Medicaid.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"character string representing summary sentence.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/create_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Summary Sentence with Logging and Error Handling — create_summary_sentence","text":"","code":"# Example 1: Basic usage with default values significant_vars <- data.frame(   Variable = c(\"Specialty\", \"Region\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.01\", \"0.02\") ) create_summary_sentence(significant_vars, 45.6, 100, 200) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>    Variable             Direction Formatted_P_Value #> 1 Specialty positively associated             <0.01 #> 2    Region negatively associated              0.02 #> Input: Medicaid Acceptance Rate: 45.6  #> Input: Count of Physicians Accepting Medicaid: 100  #> Input: Total Count of Physicians Considered for Medicaid: 200  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).\" #> [1] \"Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).\"  # Example 2: Using larger physician counts significant_vars <- data.frame(   Variable = c(\"Experience\", \"Training Level\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.05\", \"0.01\") ) create_summary_sentence(significant_vars, 35.2, 500, 1200) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>         Variable             Direction Formatted_P_Value #> 1     Experience positively associated             <0.05 #> 2 Training Level negatively associated              0.01 #> Input: Medicaid Acceptance Rate: 35.2  #> Input: Count of Physicians Accepting Medicaid: 500  #> Input: Total Count of Physicians Considered for Medicaid: 1200  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).\" #> [1] \"Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).\"  # Example 3: Using a single significant predictor significant_vars <- data.frame(   Variable = c(\"Age\"),   Direction = c(\"positively associated\"),   Formatted_P_Value = c(\"<0.001\") ) create_summary_sentence(significant_vars, 60.0, 750, 1250) #> Starting to create the summary sentence... #> Logging inputs... #> Input: Significant Predictors Data Frame: #>   Variable             Direction Formatted_P_Value #> 1      Age positively associated            <0.001 #> Input: Medicaid Acceptance Rate: 60  #> Input: Count of Physicians Accepting Medicaid: 750  #> Input: Total Count of Physicians Considered for Medicaid: 1250  #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #> Age positively associated (p = <0.001 )  #> Step 2: Constructing the final summary sentence... #> Final summary sentence constructed: #> Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).  #> Summary sentence creation complete. Returning the output... #> [1] \"Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).\" #> [1] \"Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).\""},{"path":"https://mufflyt.github.io/tyler/reference/determine_direction.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"function determines whether effect significant predictor positive (\"Higher\") negative (\"Lower\"). logs process, including inputs, outputs, step analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/determine_direction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"","code":"determine_direction(data, target_var, significant_vars)"},{"path":"https://mufflyt.github.io/tyler/reference/determine_direction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"data data frame containing dataset. target_var string representing name target variable (e.g., outcome dependent variable). significant_vars data frame containing significant predictors column \"Variable\" p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/determine_direction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"data frame additional column \"Direction\" indicating whether significant predictor associated \"Higher\" \"Lower\" effect.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/determine_direction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Determine the Direction of Effects for Significant Variables with Logging and Error Handling — determine_direction","text":"","code":"# Example 1: Determine the direction of effects with a basic dataset df <- data.frame(   age = rnorm(100, mean = 50, sd = 10),   gender = factor(sample(c(\"Male\", \"Female\"), 100, replace = TRUE)),   accepts_medicaid = rbinom(100, 1, 0.5) ) significant_vars <- data.frame(Variable = c(\"age\", \"gender\")) determine_direction(df, \"accepts_medicaid\", significant_vars) #> Starting the determine_direction function... #> Target Variable: accepts_medicaid  #> Significant Variables Data Frame: #>   Variable #> 1      age #> 2   gender #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: age  #> Direction for variable age : Higher (Coefficient = 0.006560521 ) #> Processing variable: gender  #> Direction for variable gender : Lower (Coefficient = -0.218193 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>   Variable Direction #> 1      age    Higher #> 2   gender     Lower #> determine_direction function completed successfully. Returning the result... #>   Variable Direction #> 1      age    Higher #> 2   gender     Lower  # Example 2: A dataset with multiple continuous predictors df2 <- data.frame(   income = rnorm(100, mean = 60000, sd = 15000),   education_years = rnorm(100, mean = 16, sd = 2),   accepts_insurance = rbinom(100, 1, 0.6) ) significant_vars2 <- data.frame(Variable = c(\"income\", \"education_years\")) determine_direction(df2, \"accepts_insurance\", significant_vars2) #> Starting the determine_direction function... #> Target Variable: accepts_insurance  #> Significant Variables Data Frame: #>          Variable #> 1          income #> 2 education_years #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: income  #> Direction for variable income : Lower (Coefficient = -2.780792e-06 ) #> Processing variable: education_years  #> Direction for variable education_years : Higher (Coefficient = 0.04495715 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>          Variable Direction #> 1          income     Lower #> 2 education_years    Higher #> determine_direction function completed successfully. Returning the result... #>          Variable Direction #> 1          income     Lower #> 2 education_years    Higher  # Example 3: Handling a dataset with categorical and continuous predictors df3 <- data.frame(   years_experience = rnorm(100, mean = 10, sd = 5),   specialty = factor(sample(c(\"Cardiology\", \"Neurology\"), 100, replace = TRUE)),   accepts_medicare = rbinom(100, 1, 0.7) ) significant_vars3 <- data.frame(Variable = c(\"years_experience\", \"specialty\")) determine_direction(df3, \"accepts_medicare\", significant_vars3) #> Starting the determine_direction function... #> Target Variable: accepts_medicare  #> Significant Variables Data Frame: #>           Variable #> 1 years_experience #> 2        specialty #> Step 1: Processing significant variables and fitting Poisson models... #> Processing variable: years_experience  #> Direction for variable years_experience : Higher (Coefficient = 0.01906572 ) #> Processing variable: specialty  #> Direction for variable specialty : Higher (Coefficient = 0.2335616 ) #> Step 2: Adding the direction column to the significant_vars data frame... #> Final significant variables with directions: #>           Variable Direction #> 1 years_experience    Higher #> 2        specialty    Higher #> determine_direction function completed successfully. Returning the result... #>           Variable Direction #> 1 years_experience    Higher #> 2        specialty    Higher"},{"path":"https://mufflyt.github.io/tyler/reference/find_common_columns_in_years_of_open_payments_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"function identifies compares column names multiple years Open Payments data stored DuckDB. synchronizes column names across different tables, writes result Excel file, provides detailed logging.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/find_common_columns_in_years_of_open_payments_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"","code":"find_common_columns_in_years_of_open_payments_data(   con,   output_excel_path,   table_names = c(\"OP_DTL_GNRL_PGYR2014_P06302021\", \"OP_DTL_GNRL_PGYR2015_P06302021\",     \"OP_DTL_GNRL_PGYR2016_P01182024\", \"OP_DTL_GNRL_PGYR2017_P01182024\",     \"OP_DTL_GNRL_PGYR2018_P01182024\", \"OP_DTL_GNRL_PGYR2019_P01182024\",     \"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\",     \"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   beep_on_complete = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/find_common_columns_in_years_of_open_payments_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"con DuckDB connection object. output_excel_path string specifying path save output Excel file. table_names character vector specifying table names process. Defaults Open Payments data 2014 2023. beep_on_complete Logical value indicating beep sound played function completes. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/find_common_columns_in_years_of_open_payments_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"tibble column names table.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/find_common_columns_in_years_of_open_payments_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Common Columns in Years of Open Payments Data — find_common_columns_in_years_of_open_payments_data","text":"","code":"# Example with default table names: con <- DBI::dbConnect(duckdb::duckdb(), \"path_to_duckdb.duckdb\") find_common_columns_in_years_of_open_payments_data(con, \"output.xlsx\") #> Starting find_common_columns_in_years_of_open_payments_data with inputs: #> Output Excel Path: output.xlsx #> Tables: OP_DTL_GNRL_PGYR2014_P06302021, OP_DTL_GNRL_PGYR2015_P06302021, OP_DTL_GNRL_PGYR2016_P01182024, OP_DTL_GNRL_PGYR2017_P01182024, OP_DTL_GNRL_PGYR2018_P01182024, OP_DTL_GNRL_PGYR2019_P01182024, OP_DTL_GNRL_PGYR2020_P01182024, OP_DTL_GNRL_PGYR2021_P01182024, OP_DTL_GNRL_PGYR2022_P01182024, OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> Processing table: OP_DTL_GNRL_PGYR2014_P06302021 #> Error in purrr::map(table_names, function(table) {    message(\"Processing table: \", table)    columns <- DBI::dbGetQuery(con, paste0(\"PRAGMA table_info(\",         table, \")\")) %>% dplyr::pull(name)    message(\"Retrieved \", length(columns), \" columns from table: \",         table)    return(columns)}): ℹ In index: 1. #> Caused by error: #> ! rapi_prepare: Failed to prepare query PRAGMA table_info(OP_DTL_GNRL_PGYR2014_P06302021) #> Error: Catalog Error: Table with name OP_DTL_GNRL_PGYR2014_P06302021 does not exist! #> Did you mean \"pg_am\"?  # Example with custom table names: custom_tables <- c(\"OP_DTL_GNRL_PGYR2014_P06302021\", \"OP_DTL_GNRL_PGYR2015_P06302021\") find_common_columns_in_years_of_open_payments_data(con, \"output_custom.xlsx\", custom_tables) #> Starting find_common_columns_in_years_of_open_payments_data with inputs: #> Output Excel Path: output_custom.xlsx #> Tables: OP_DTL_GNRL_PGYR2014_P06302021, OP_DTL_GNRL_PGYR2015_P06302021 #> Processing table: OP_DTL_GNRL_PGYR2014_P06302021 #> Error in purrr::map(table_names, function(table) {    message(\"Processing table: \", table)    columns <- DBI::dbGetQuery(con, paste0(\"PRAGMA table_info(\",         table, \")\")) %>% dplyr::pull(name)    message(\"Retrieved \", length(columns), \" columns from table: \",         table)    return(columns)}): ℹ In index: 1. #> Caused by error: #> ! rapi_prepare: Failed to prepare query PRAGMA table_info(OP_DTL_GNRL_PGYR2014_P06302021) #> Error: Catalog Error: Table with name OP_DTL_GNRL_PGYR2014_P06302021 does not exist! #> Did you mean \"pg_am\"?"},{"path":"https://mufflyt.github.io/tyler/reference/fips.html","id":null,"dir":"Reference","previous_headings":"","what":"Data of FIPS codes — fips","title":"Data of FIPS codes — fips","text":"3 columns 51 observations.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fips.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data of FIPS codes — fips","text":"","code":"fips"},{"path":"https://mufflyt.github.io/tyler/reference/fips.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data of FIPS codes — fips","text":"tibble 243 rows 10 variables:","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fips.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Data of FIPS codes — fips","text":"https://github.com/kjhealy/fips-codes/blob/master/state_and_county_fips_master.csv","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_mixed_model_with_logging.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","title":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","text":"function fits either linear mixed-effects model (lmer) robust linear mixed-effects model (rlmer) logs every step. designed handle errors robustly, provide progress completion signals system beeps, optionally save results file. function highly configurable works box default settings.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_mixed_model_with_logging.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","text":"","code":"fit_mixed_model_with_logging(   data,   response_var = \"log_business_days_until_appointment\",   random_effect = \"(1 | NPI)\",   exclude_vars = c(response_var, \"last\", \"business_days_until_appointment\",     \"cleaned_does_the_physician_accept_medicaid\", \"record_id\", \"ID\", \"middle\",     \"physician_information\", \"address\", \"offered_a_clinic_appointment_to_be_seen\",     \"reason_for_exclusions\", \"state\", \"Grd_yr\", \"age_category\", \"notes\", \"first\",     \"does_the_physician_accept_medicaid\", \"insurance_type\", \"zip\", \"Subspecialty\", \"NPI\",     \"lng\", \"lat\", \"including_this_physician_in_the_study\",     \"told_to_go_to_the_emergency_department\"),   model_type = \"lmer\",   significance_cutoff = 0.2,   save_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/fit_mixed_model_with_logging.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","text":"data data frame containing dataset. response_var character string specifying response variable. Default \"log_business_days_until_appointment\". random_effect character string specifying random effect. Default \"(1 | NPI)\". exclude_vars character vector specifying columns exclude predictor variables. Default list variables typically excluded analysis. model_type character string indicating model type: \"lmer\" linear mixed-effects \"rlmer\" robust linear mixed-effects. Default \"lmer\". significance_cutoff numeric value p-value threshold filtering significant predictors. Default 0.2. save_path Optional. character string specifying file path results saved CSV. provided, results saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_mixed_model_with_logging.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","text":"tibble significant predictors, p-values, IRR (Incident Rate Ratios), confidence intervals, associated wait time effects.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_mixed_model_with_logging.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Mixed-Effects Model with Logging and Robust Error Handling — fit_mixed_model_with_logging","text":"","code":"# Example 1: Basic usage with default settings df <- my_data_frame #> Error: object 'my_data_frame' not found result <- fit_mixed_model_with_logging(data = df) #> ERROR [2024-10-30 12:19:24] Input data is not a valid data frame. #> Error in fit_mixed_model_with_logging(data = df): Data must be a data frame.  # Example 2: Using a robust linear mixed-effects model (rlmer) and saving the results result <- fit_mixed_model_with_logging(data = df, model_type = \"rlmer\", save_path = \"results_rlmer.csv\") #> ERROR [2024-10-30 12:19:24] Input data is not a valid data frame. #> Error in fit_mixed_model_with_logging(data = df, model_type = \"rlmer\",     save_path = \"results_rlmer.csv\"): Data must be a data frame.  # Example 3: Custom response variable and random effect, with significance level 0.05 result <- fit_mixed_model_with_logging(data = df,                                        response_var = \"some_other_response\",                                        random_effect = \"(1 | group_id)\",                                        significance_cutoff = 0.05) #> ERROR [2024-10-30 12:19:24] Input data is not a valid data frame. #> Error in fit_mixed_model_with_logging(data = df, response_var = \"some_other_response\",     random_effect = \"(1 | group_id)\", significance_cutoff = 0.05): Data must be a data frame."},{"path":"https://mufflyt.github.io/tyler/reference/fit_poisson_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","title":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","text":"function fits Poisson models predictor variable target variable. logs process, including inputs, outputs, transformations, variables skipped due insufficient variance.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_poisson_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","text":"","code":"fit_poisson_models(data, target_var, predictors)"},{"path":"https://mufflyt.github.io/tyler/reference/fit_poisson_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","text":"data data frame containing dataset. target_var string representing name target variable. predictors vector strings representing names predictor variables.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_poisson_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","text":"tibble containing predictor variables, corresponding p-values, formatted p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/fit_poisson_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit Poisson Models and Extract P-Values with Logging — fit_poisson_models","text":"","code":"# Example 1: Basic usage with a dataset and predictor variables result <- fit_poisson_models(data = my_data_frame, target_var = \"outcome\", predictors = c(\"age\", \"gender\", \"income\")) #> Error: object 'my_data_frame' not found  # Example 2: Using a larger dataset with many predictors predictor_vars <- names(my_large_data_frame)[3:20]  # Exclude first two columns #> Error: object 'my_large_data_frame' not found result <- fit_poisson_models(data = my_large_data_frame, target_var = \"accepts_medicaid\", predictors = predictor_vars) #> Error: object 'my_large_data_frame' not found  # Example 3: Handling a dataset where some predictors may have only one unique value result <- fit_poisson_models(data = my_data_frame, target_var = \"outcome\", predictors = c(\"region\", \"specialty\", \"age_group\")) #> Error: object 'my_data_frame' not found"},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":null,"dir":"Reference","previous_headings":"","what":"Format a Numeric Value as a Percentage — format_pct","title":"Format a Numeric Value as a Percentage — format_pct","text":"function formats numeric value percentage specified number decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format a Numeric Value as a Percentage — format_pct","text":"","code":"format_pct(x, my_digits = 1)"},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format a Numeric Value as a Percentage — format_pct","text":"x numeric value vector want format percentage. my_digits integer specifying number decimal places include formatted percentage. default 1.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format a Numeric Value as a Percentage — format_pct","text":"character vector representing formatted percentage(s) specified number decimal places.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Format a Numeric Value as a Percentage — format_pct","text":"function converts numeric value percentage format specified number decimal places. useful consistent display percentage values reports visualizations.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/format_pct.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Format a Numeric Value as a Percentage — format_pct","text":"","code":"# Example 1: Format a single numeric value result <- format_pct(0.12345) print(result)  # Output: \"12.3%\" #> [1] \"0.1\"  # Example 2: Format a vector of numeric values with 2 decimal places values <- c(0.12345, 0.6789, 0.54321) formatted_values <- format_pct(values, my_digits = 2) print(formatted_values)  # Output: \"12.35%\", \"67.89%\", \"54.32%\" #> [1] \"0.12\" \"0.68\" \"0.54\"  # Example 3: Format a value with no decimal places no_decimal <- format_pct(0.5, my_digits = 0) #> Error in prettyNum(.Internal(format(x, trim, digits, nsmall, width, 3L,     na.encode, scientific, decimal.mark)), big.mark = big.mark,     big.interval = big.interval, small.mark = small.mark, small.interval = small.interval,     decimal.mark = decimal.mark, input.d.mark = decimal.mark,     zero.print = zero.print, drop0trailing = drop0trailing, is.cmplx = is.complex(x),     preserve.width = if (trim) \"individual\" else \"common\"): invalid value 0 for 'digits' argument print(no_decimal)  # Output: \"50%\" #> Error: object 'no_decimal' not found"},{"path":"https://mufflyt.github.io/tyler/reference/genderize_physicians.html","id":null,"dir":"Reference","previous_headings":"","what":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","title":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","text":"function reads CSV file containing physician data, genderizes first names, joins gender information back original data. logs entire process, including inputs, transformations, outputs, saves result new CSV file timestamp.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/genderize_physicians.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","text":"","code":"genderize_physicians(input_csv, output_dir = getwd())"},{"path":"https://mufflyt.github.io/tyler/reference/genderize_physicians.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","text":"input_csv string representing path input CSV file containing physician data. CSV contain column named 'first_name' genderization. output_dir string representing directory output CSV file saved. default current working directory.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/genderize_physicians.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","text":"tibble genderized information joined original data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/genderize_physicians.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Genderize Physicians Data with Logging and Error Handling — genderize_physicians","text":"","code":"# Example 1: Basic usage with the default output directory if (FALSE) { # \\dontrun{ result <- genderize_physicians(\"physicians_data.csv\") } # }  # Example 2: Custom output directory for saving the result if (FALSE) { # \\dontrun{ result <- genderize_physicians(\"physicians_data.csv\", output_dir = \"output_directory/\") } # }  # Example 3: Handling missing first names gracefully if (FALSE) { # \\dontrun{ result <- genderize_physicians(\"physicians_data_missing_names.csv\") } # }  # Example 4: Using a CSV without the required 'first_name' column if (FALSE) { # \\dontrun{ result <- genderize_physicians(\"invalid_data.csv\") } # }  # Example 5: Saving the output in a different directory with a custom name if (FALSE) { # \\dontrun{ result <- genderize_physicians(\"physicians_data.csv\", output_dir = \"custom_output/\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/generate_acog_districts_sf.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate ACOG Districts sf Object — generate_acog_districts_sf","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"function generates sf object representing ACOG districts grouping states particular district together.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_acog_districts_sf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"","code":"generate_acog_districts_sf(filepath = NULL)"},{"path":"https://mufflyt.github.io/tyler/reference/generate_acog_districts_sf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate ACOG Districts sf Object — generate_acog_districts_sf","text":"sf object containing grouped ACOG districts.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_interaction_sentences.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","title":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","text":"function generates sentences based interaction two variables fitted model. logs steps, including inputs, outputs, data transformations, p-value extraction.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_interaction_sentences.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","text":"","code":"generate_interaction_sentences(   interaction_model,   variable1,   variable2,   model_summary = NULL,   confidence_level = 0.95,   log_transform = TRUE,   output_format = \"text\" )"},{"path":"https://mufflyt.github.io/tyler/reference/generate_interaction_sentences.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","text":"interaction_model fitted model object (e.g., glm lmer). variable1 string representing first interacting variable. variable2 string representing second interacting variable. model_summary summary object model. NULL, generated automatically. confidence_level confidence level confidence intervals. Default 0.95. log_transform logical indicating rate log-transformed. Default TRUE. output_format string specifying output format: \"text\" \"markdown\". Default \"text\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_interaction_sentences.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","text":"list sentences describing interaction two variables.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_interaction_sentences.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Interaction Sentences with Logging and Error Handling — generate_interaction_sentences","text":"","code":"# Example 1: Basic usage with text output if (FALSE) { # \\dontrun{ result <- generate_interaction_sentences(interaction_model, \"scenario\", \"insurance\", output_format = \"text\") } # }  # Example 2: Markdown format output if (FALSE) { # \\dontrun{ result <- generate_interaction_sentences(interaction_model, \"scenario\", \"insurance\", output_format = \"markdown\") } # }  # Example 3: Custom confidence level if (FALSE) { # \\dontrun{ result <- generate_interaction_sentences(interaction_model, \"scenario\", \"insurance\", confidence_level = 0.90) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate LaTeX Equation with Logging — generate_latex_equation","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"function generates LaTeX equation incorporates provided \"patient_scenario_label\". function logs input, processes input string escaping LaTeX characters, constructs LaTeX equation. logs operations can output console log file (provided).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"","code":"generate_latex_equation(   patient_scenario_label = \"Default Patient Scenario\",   log_file = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"patient_scenario_label string representing text (typically \"Patient Scenario\") inserted LaTeX equation. Default \"Default Patient Scenario\". log_file string representing full path file logs written. NULL (default), logs printed console.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"LaTeX code string, ready inserted RMarkdown LaTeX document.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"function generates dynamic LaTeX code using provided patient_scenario_label, ensuring special LaTeX characters (underscores) escaped properly. logs entire process, including input validation, transformations, final output. patient_scenario_label provided, default value used.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_latex_equation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate LaTeX Equation with Logging — generate_latex_equation","text":"","code":"# Example 1: Basic usage with logging to the console if (FALSE) { # \\dontrun{ generate_latex_equation(\"Patient Scenario\") } # }  # Example 2: Handle underscores in the patient_scenario_label if (FALSE) { # \\dontrun{ generate_latex_equation(\"Patient_Scenario_With_Underscores\") } # }  # Example 3: Logging the process to a file if (FALSE) { # \\dontrun{ log_file_path <- \"latex_generation_log.txt\" generate_latex_equation(\"Patient Scenario\", log_file = log_file_path) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/generate_leaflet_base_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Leaflet Base Map — generate_leaflet_base_map","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"function creates Leaflet BASE map specific configurations, including base tile layer, scale bar, default view settings, layers control.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_leaflet_base_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"","code":"generate_leaflet_base_map()"},{"path":"https://mufflyt.github.io/tyler/reference/generate_leaflet_base_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"Leaflet map object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_leaflet_base_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Leaflet Base Map — generate_leaflet_base_map","text":"","code":"map <- generate_leaflet_base_map()"},{"path":"https://mufflyt.github.io/tyler/reference/generate_overall_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate overall table — generate_overall_table","title":"Generate overall table — generate_overall_table","text":"Generate overall table summarizing demographics Table 1.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_overall_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate overall table — generate_overall_table","text":"","code":"generate_overall_table(input_file, output_dir)"},{"path":"https://mufflyt.github.io/tyler/reference/generate_overall_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate overall table — generate_overall_table","text":"input_file path data file (RDS, CSV, XLS format). output_dir directory output table file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/generate_overall_table.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate overall table — generate_overall_table","text":"","code":"if (FALSE) { # Generate the overall table generate_overall_table(\"data/Table1.rds\", \"output_tables\") }"},{"path":"https://mufflyt.github.io/tyler/reference/geocode_unique_addresses.html","id":null,"dir":"Reference","previous_headings":"","what":"Geocode Unique Addresses — geocode_unique_addresses","title":"Geocode Unique Addresses — geocode_unique_addresses","text":"function geocodes unique addresses using Google Maps API appends latitude longitude original dataset. Please ensure every dataset must column named 'address'.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/geocode_unique_addresses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Geocode Unique Addresses — geocode_unique_addresses","text":"","code":"geocode_unique_addresses(   file_path,   google_maps_api_key,   output_file_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/geocode_unique_addresses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Geocode Unique Addresses — geocode_unique_addresses","text":"file_path Path input file (CSV, RDS, XLSX) containing address data. google_maps_api_key Google Maps API key. output_file_path Path output CSV file geocoded data saved. (Optional)","code":""},{"path":"https://mufflyt.github.io/tyler/reference/geocode_unique_addresses.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Geocode Unique Addresses — geocode_unique_addresses","text":"dataframe containing geocoded address data latitude longitude.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/geocode_unique_addresses.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Geocode Unique Addresses — geocode_unique_addresses","text":"","code":"if (FALSE) { # \\dontrun{ # Define the input file path, Google Maps API key, and output file path (optional) file_path <- \"input_data.csv\" google_maps_api_key <- \"your_api_key\" output_file_path <- \"output_data.csv\"  # Optional  # Call the geocode_unique_addresses function with or without specifying output_file_path geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key) # or geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key, output_file_path) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/get_census_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Census data of all state block groups — get_census_data","title":"Get Census data of all state block groups — get_census_data","text":"function retrieves Census data state block groups looping specified list state FIPS codes.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/get_census_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Census data of all state block groups — get_census_data","text":"","code":"get_census_data(us_fips_list, vintage = 2022)"},{"path":"https://mufflyt.github.io/tyler/reference/get_census_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Census data of all state block groups — get_census_data","text":"us_fips_list vector state FIPS codes Census data retrieved. vintage vintage year Census data (default 2022).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/get_census_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Census data of all state block groups — get_census_data","text":"dataframe containing Census data state block groups.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/get_most_common.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Most Common Value — get_most_common","title":"Get the Most Common Value — get_most_common","text":"function calculates returns common value tabulation result.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/get_most_common.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Most Common Value — get_most_common","text":"","code":"get_most_common(tabyl_result, variable_name)"},{"path":"https://mufflyt.github.io/tyler/reference/get_most_common.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Most Common Value — get_most_common","text":"tabyl_result tabulation result data frame. variable_name name categorical variable.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/get_most_common.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Most Common Value — get_most_common","text":"common value.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/honeycomb_generate_maps.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"function generates hexagon maps ACOG districts, using physician location data. provides extensive logging, error handling, defaults sample data inputs provided.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/honeycomb_generate_maps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"","code":"honeycomb_generate_maps(   physician_locations_sf = NULL,   acog_districts_sf = NULL,   trait_map = \"all\",   honeycomb_map = \"all\",   hex_grid_size = c(0.3, 0.3),   target_district = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/honeycomb_generate_maps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"physician_locations_sf Simple Features (sf) object containing physician data coordinates. NULL, default sample data used. acog_districts_sf Simple Features (sf) object containing grouped ACOG districts. NULL, default ACOG districts used. trait_map string specifying trait map (default \"\"). honeycomb_map string specifying honey map (default \"\"). hex_grid_size numeric vector length 2 specifying grid size hexagon map (default c(0.3, 0.3)). target_district string NULL specify specific district generating map (default NULL, processes districts).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/honeycomb_generate_maps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"list ggplot objects generated maps specified districts. ggplot object hexagon map showing physician distribution.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/honeycomb_generate_maps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Hexagon Maps by ACOG District with Logging and Error Handling — honeycomb_generate_maps","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Generate maps using default ACOG districts and sample physician data hex_maps <- honeycomb_generate_maps()  # Example 2: Generate maps for a specific ACOG district with default sample data hex_maps <- honeycomb_generate_maps(target_district = \"District IV\")  # Example 3: Generate maps for all districts, with custom grid size and custom data custom_physicians <- tyler::get_custom_physician_data()  # Assume a custom data function custom_acog_districts <- tyler::get_custom_acog_data()  # Assume custom ACOG districts data hex_maps <- honeycomb_generate_maps(   physician_locations_sf = custom_physicians,   acog_districts_sf = custom_acog_districts,   hex_grid_size = c(0.5, 0.5) ) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/hrr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Hospital Referral Region Shapefile — hrr","title":"Get Hospital Referral Region Shapefile — hrr","text":"function loads hospital referral region shapefile optionally removes Hawaii Alaska.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/hrr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Hospital Referral Region Shapefile — hrr","text":"","code":"hrr(remove_HI_AK = TRUE)"},{"path":"https://mufflyt.github.io/tyler/reference/hrr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Hospital Referral Region Shapefile — hrr","text":"remove_HI_AK Logical, Hawaii Alaska removed? Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/hrr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Hospital Referral Region Shapefile — hrr","text":"sf object containing hospital referral region data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/hrr_generate_maps.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"function generates hexagon maps hospital referral regions.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/hrr_generate_maps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"","code":"hrr_generate_maps(physician_sf, trait_map = \"all\", honey_map = \"all\")"},{"path":"https://mufflyt.github.io/tyler/reference/hrr_generate_maps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"physician_sf sf object containing physician data coordinates. trait_map string specifying trait map (default \"\"). honey_map string specifying honey map (default \"\").","code":""},{"path":"https://mufflyt.github.io/tyler/reference/hrr_generate_maps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Hexagon Maps for Hospital Referral Regions (HRR) — hrr_generate_maps","text":"ggplot object generated map.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/install_missing_packages.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Missing CRAN Packages — install_missing_packages","title":"Install Missing CRAN Packages — install_missing_packages","text":"function checks specified CRAN packages installed. package installed, installed automatically. packages already installed, message printed indicating installed along version.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/install_missing_packages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Missing CRAN Packages — install_missing_packages","text":"","code":"install_missing_packages(cran_pkgs)"},{"path":"https://mufflyt.github.io/tyler/reference/install_missing_packages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Missing CRAN Packages — install_missing_packages","text":"cran_pkgs character vector CRAN package names check install missing.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/install_missing_packages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Install Missing CRAN Packages — install_missing_packages","text":"","code":"# Example 1: Install ggplot2 and dplyr if missing install_missing_packages(c(\"ggplot2\", \"dplyr\")) #> Already installed: ggplot2 3.5.1  #> Already installed: dplyr 1.1.4   # Example 2: Install multiple packages including data.table and tidyr install_missing_packages(c(\"data.table\", \"tidyr\", \"readr\")) #> Already installed: data.table 1.16.2  #> Already installed: tidyr 1.3.1  #> Already installed: readr 2.1.5   # Example 3: Install a large set of packages commonly used in data science install_missing_packages(c(\"ggplot2\", \"dplyr\", \"tidyverse\", \"caret\", \"randomForest\")) #> Already installed: ggplot2 3.5.1  #> Already installed: dplyr 1.1.4  #> Already installed: tidyverse 2.0.0  #> Already installed: caret 6.0-94  #> Already installed: randomForest 4.7-1.1"},{"path":"https://mufflyt.github.io/tyler/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"function generates summary sentence using linear regression analyze trend proportion women without access gynecologic oncologist time specified race driving time. includes raw proportions first last years dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"","code":"linear_regression_race_drive_time_generate_summary_sentence(   tabulated_data,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" )"},{"path":"https://mufflyt.github.io/tyler/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"tabulated_data data frame containing data analyze. Must include columns Driving Time (minutes), Year, columns race proportions like White_prop, Black_prop, etc. driving_time_minutes numeric value specifying driving time minutes filter data. Default 180. race character string specifying race generate summary sentence. Supported values \"White\", \"Black\", \"American Indian/Alaska Native\", \"Asian\", \"Native Hawaiian Pacific Islander\", \"\" generate sentences supported races. Default \"American Indian/Alaska Native\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"character string containing summary sentence, list summary sentences race = \"\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/linear_regression_race_drive_time_generate_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions — linear_regression_race_drive_time_generate_summary_sentence","text":"","code":"# Example usage summary_sentence <- linear_regression_race_drive_time_generate_summary_sentence(   tabulated_data = tabulated_all_years_numeric,   driving_time_minutes = 180,   race = \"White\" ) #> Function linear_regression_race_drive_time_generate_summary_sentence called with inputs: #> Driving Time (minutes): 180 #> Race: White #> Race column used for analysis: White_prop #> Error: object 'tabulated_all_years_numeric' not found print(summary_sentence) #> Error: object 'summary_sentence' not found"},{"path":"https://mufflyt.github.io/tyler/reference/load_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load and Process Data from an RDS File with Robust Logging — load_data","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"function loads data RDS file, renames 'ID' column 'id_number', logs every step process.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/load_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"","code":"load_data(data_dir, file_name, verbose = TRUE)"},{"path":"https://mufflyt.github.io/tyler/reference/load_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"data_dir string specifying directory RDS file located. file_name string specifying name RDS file load. verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/load_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"data frame 'ID' column renamed 'id_number'.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/load_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load and Process Data from an RDS File with Robust Logging — load_data","text":"","code":"# Example: Load data from a specified directory with logging df <- load_data(data_dir = \"data\", file_name = \"Phase_2.rds\", verbose = TRUE) #> Function load_data called with the following inputs: #>   data_dir: data  #>   file_name: Phase_2.rds  #>   Constructed file path: data/Phase_2.rds  #> Error in load_data(data_dir = \"data\", file_name = \"Phase_2.rds\", verbose = TRUE): File not found at path: data/Phase_2.rds"},{"path":"https://mufflyt.github.io/tyler/reference/logistic_regression.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"function performs logistic regression multiple predictor variables specified target variable. logs process, including inputs, data transformations, results. function filters predictors based given significance level.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/logistic_regression.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"","code":"logistic_regression(df, target_variable, predictor_vars, significance_level)"},{"path":"https://mufflyt.github.io/tyler/reference/logistic_regression.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"df data frame containing target predictor variables. target_variable string representing name target variable. predictor_vars vector strings representing names predictor variables. significance_level numeric value specifying significance level filtering predictors.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/logistic_regression.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"data frame containing significant predictors p-values formatted p-values.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/logistic_regression.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Logistic Regression on Multiple Predictors with Logging — logistic_regression","text":"","code":"# Assuming df is your data frame target_variable <- \"cleaned_does_the_physician_accept_medicaid_numeric\" predictor_vars <- setdiff(names(df), c(target_variable, \"does_the_physician_accept_medicaid\", \"cleaned_does_the_physician_accept_medicaid\")) significance_logistic_regression <- 0.2  significant_vars <- logistic_regression(df, target_variable, predictor_vars, significance_level = significance_logistic_regression) #> Starting logistic_regression... #> Target Variable: cleaned_does_the_physician_accept_medicaid_numeric  #> Predictor Variables:   #> Significance Level: 0.2  #> Significant Predictors: #> [1] Variable          P_Value           Formatted_P_Value #> <0 rows> (or 0-length row.names) print(significant_vars) #> [1] Variable          P_Value           Formatted_P_Value #> <0 rows> (or 0-length row.names)"},{"path":"https://mufflyt.github.io/tyler/reference/map_physicians_by_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Map Physicians by State or Subdivision — map_physicians_by_group","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"function generates choropleth map based physician counts saves map PNG file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/map_physicians_by_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"","code":"map_physicians_by_group(   state_counts,   output_file_prefix = \"choropleth_map\",   group_by = \"state\" )"},{"path":"https://mufflyt.github.io/tyler/reference/map_physicians_by_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"state_counts tibble containing counts physicians per state subdivision. output_file_prefix prefix output PNG file choropleth map (default \"choropleth_map\"). group_by string indicating whether counts grouped \"state\" \"subdivision\" (default \"state\").","code":""},{"path":"https://mufflyt.github.io/tyler/reference/map_physicians_by_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"ggplot2 object representing choropleth map.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/map_physicians_by_group.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Map Physicians by State or Subdivision — map_physicians_by_group","text":"","code":"# Example 1: Map physicians by state state_counts <- count_physicians_by_group(taxonomy_and_aaos_data) #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'taxonomy_and_aaos_data' not found map_physicians_by_group(state_counts) #> INFO [2024-10-30 12:19:28] Mapping physicians grouped by state #> Error in merge_state_counts_with_map(state_counts): could not find function \"merge_state_counts_with_map\"  # Example 2: Map physicians by U.S. Census Bureau subdivision subdivision_counts <- count_physicians_by_group(taxonomy_and_aaos_data, group_by = \"subdivision\") #> Error: Failed to evaluate glue component {nrow(data)} #> Caused by error: #> ! object 'taxonomy_and_aaos_data' not found map_physicians_by_group(subdivision_counts, group_by = \"subdivision\") #> INFO [2024-10-30 12:19:28] Mapping physicians grouped by subdivision #> Error in merge_subdivision_counts_with_map(state_counts): could not find function \"merge_subdivision_counts_with_map\""},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"function calculates returns sentence describes common gender, specialty, training, academic affiliation provided dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"","code":"most_common_gender_training_academic(df)"},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"df data frame containing columns gender, specialty, Provider.Credential.Text, academic_affiliation.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"character string summarizing common gender, specialty, training, academic affiliation along respective proportions.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"function filters missing values column determining common value. calculates proportion common value relative total non-missing values column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/most_common_gender_training_academic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation — most_common_gender_training_academic","text":"","code":"# Example 1: Basic usage with a small dataset df <- data.frame(   gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", \"Cardiology\", \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\") ) result <- most_common_gender_training_academic(df) print(result) #> [1] \"The most common gender in the dataset was male (60%). The most common specialty was Cardiology (60%). The most common training was MD (60%). The academic affiliation status most frequently occurring was yes (60%).\"  # Example 2: Handling missing data df_with_na <- data.frame(   gender = c(\"Male\", NA, \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", NA, \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", NA) ) result <- most_common_gender_training_academic(df_with_na) print(result) #> [1] \"The most common gender in the dataset was male (75%). The most common specialty was Cardiology (50%). The most common training was MD (60%). The academic affiliation status most frequently occurring was no (50%).\"  # Example 3: Different proportions with a larger dataset df_large <- data.frame(   gender = c(rep(\"Male\", 70), rep(\"Female\", 30)),   specialty = c(rep(\"Cardiology\", 50), rep(\"Neurology\", 30), rep(\"Orthopedics\", 20)),   Provider.Credential.Text = c(rep(\"MD\", 60), rep(\"DO\", 40)),   academic_affiliation = c(rep(\"Yes\", 40), rep(\"No\", 60)) ) result <- most_common_gender_training_academic(df_large) print(result) #> [1] \"The most common gender in the dataset was male (70%). The most common specialty was Cardiology (50%). The most common training was MD (60%). The academic affiliation status most frequently occurring was no (60%).\""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect and clean the processed data — nppes_collect_and_clean_data","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"function collects cleans processed data DuckDB.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"","code":"nppes_collect_and_clean_data(processed_data)"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"processed_data tbl object containing processed data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"cleaned data frame.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"collecting cleaning data, next step save cleaned data CSV file using nppes_save_data_to_csv().","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_collect_and_clean_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect and clean the processed data — nppes_collect_and_clean_data","text":"","code":"cleaned_data <- nppes_collect_and_clean_data(processed_data) #> Error in nppes_collect_and_clean_data(processed_data): could not find function \"nppes_collect_and_clean_data\" # Next step: nppes_save_data_to_csv()"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_connect_to_duckdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to DuckDB — nppes_connect_to_duckdb","title":"Connect to DuckDB — nppes_connect_to_duckdb","text":"function establishes connection DuckDB database.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_connect_to_duckdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to DuckDB — nppes_connect_to_duckdb","text":"","code":"nppes_connect_to_duckdb(duckdb_file_path)"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_connect_to_duckdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to DuckDB — nppes_connect_to_duckdb","text":"duckdb_file_path file path DuckDB database file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_connect_to_duckdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to DuckDB — nppes_connect_to_duckdb","text":"DuckDB connection object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_connect_to_duckdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to DuckDB — nppes_connect_to_duckdb","text":"","code":"con <- nppes_connect_to_duckdb(\"/path/to/your/duckdb_file.duckdb\") #> Connecting to DuckDB at /path/to/your/duckdb_file.duckdb ... #> Warning: cannot open file '/path/to/your/duckdb_file.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"function creates table DuckDB reading data CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"","code":"nppes_create_table_from_csv(con, file_path, table_name = \"npi_2024\")"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"con DuckDB connection object. file_path character string specifying path CSV file. table_name character string specifying name table created. Default \"npi_2024\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"creating table, next step query sample data using nppes_query_sample_data().","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_create_table_from_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a table in DuckDB from a CSV file — nppes_create_table_from_csv","text":"","code":"con <- nppes_connect_to_duckdb(\"/path/to/your/duckdb_file.duckdb\") #> Connecting to DuckDB at /path/to/your/duckdb_file.duckdb ... #> Warning: cannot open file '/path/to/your/duckdb_file.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection nppes_create_table_from_csv(con, \"/path/to/your/csv_file.csv\", \"npi_2024\") #> Error in nppes_create_table_from_csv(con, \"/path/to/your/csv_file.csv\",     \"npi_2024\"): could not find function \"nppes_create_table_from_csv\" # Next step: nppes_query_sample_data()  con <- nppes_connect_to_duckdb(\"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\") #> Connecting to DuckDB at /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb ... #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection nppes_create_table_from_csv(con, \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data.csv\", \"npi_2024\") #> Error in nppes_create_table_from_csv(con, \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data.csv\",     \"npi_2024\"): could not find function \"nppes_create_table_from_csv\" # Next step: nppes_query_sample_data()"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":null,"dir":"Reference","previous_headings":"","what":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"function disconnects DuckDB database connection operations complete.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"","code":"nppes_disconnect_from_duckdb(con)"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"con DuckDB connection object.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"Use function completed operations involving DuckDB connection.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_disconnect_from_duckdb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Disconnect from DuckDB — nppes_disconnect_from_duckdb","text":"","code":"con <- nppes_connect_to_duckdb(\"/path/to/your/duckdb_file.duckdb\") #> Connecting to DuckDB at /path/to/your/duckdb_file.duckdb ... #> Warning: cannot open file '/path/to/your/duckdb_file.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection # After all operations, disconnect from the database nppes_disconnect_from_duckdb(con) #> Disconnecting from DuckDB... #> Error in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbDisconnect': object 'con' not found  con <- nppes_connect_to_duckdb(\"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\") #> Connecting to DuckDB at /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb ... #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection # After all operations, disconnect from the database nppes_disconnect_from_duckdb(con) #> Disconnecting from DuckDB... #> Error in h(simpleError(msg, call)): error in evaluating the argument 'conn' in selecting a method for function 'dbDisconnect': object 'con' not found"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":null,"dir":"Reference","previous_headings":"","what":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"function processes NPPES data one year reading CSV file, filtering based taxonomy codes, writing results output CSV file. processes data chunks improve memory efficiency includes system beeps progress completion. Logs provided step using log_message function.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"","code":"nppes_get_data_for_one_year(   npi_file_path,   output_csv_path,   duckdb_file_path =     \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\",   taxonomy_codes_1 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   taxonomy_codes_2 = c(\"207V00000X\", \"207VB0002X\", \"207VC0300X\", \"207VC0200X\",     \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\", \"207VX0000X\", \"207VE0102X\",     \"207VF0040X\"),   save_column_in_each_nppes_year = FALSE,   excel_file_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"npi_file_path character string specifying path NPI CSV file. file expected contain raw NPPES data one year. output_csv_path character string specifying path save output cleaned data CSV file. file created overwritten process. duckdb_file_path character string specifying path DuckDB database file. Default predefined file path. taxonomy_codes_1 character vector specifying taxonomy codes filter Healthcare Provider Taxonomy Code_1. Default predefined set taxonomy codes. taxonomy_codes_2 character vector specifying taxonomy codes filter Healthcare Provider Taxonomy Code_2. Default set taxonomy_codes_1. save_column_in_each_nppes_year logical value indicating whether save sample data Excel file year. Default FALSE. excel_file_path character string specifying path save Excel file save_column_in_each_nppes_year TRUE. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"cleaned data frame containing NPPES data one year. data saved CSV file specified output_csv_path argument.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"function processes large datasets chunks reduce memory usage, filtering data based specified taxonomy codes. logs progress using log_message helper function gives auditory feedback system beeps. function saves cleaned data specified output_csv_path can optionally save sample data Excel. completion, message displayed indicating nppes_save_summary_statistics next function run.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_get_data_for_one_year.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get NPPES Data for One Year with Chunked Processing — nppes_get_data_for_one_year","text":"","code":"# Example 1: Basic usage with default taxonomy codes result <- nppes_get_data_for_one_year(   npi_file_path = \"/path/to/npi_file.csv\",   output_csv_path = \"/path/to/output.csv\" ) #> [2024-10-30 12:19:30.077816] Starting the NPPES data processing for one year... #> [2024-10-30 12:19:30.078129] Inputs: #> [2024-10-30 12:19:30.07826]   npi_file_path: /path/to/npi_file.csv #> [2024-10-30 12:19:30.078901]   output_csv_path: /path/to/output.csv #> [2024-10-30 12:19:30.079364]   duckdb_file_path: /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb #> [2024-10-30 12:19:30.079621]   taxonomy_codes_1: 207V00000X, 207VB0002X, 207VC0300X, 207VC0200X, 207VX0201X, 207VG0400X, 207VH0002X, 207VM0101X, 207VX0000X, 207VE0102X, 207VF0040X #> [2024-10-30 12:19:30.079856]   taxonomy_codes_2: 207V00000X, 207VB0002X, 207VC0300X, 207VC0200X, 207VX0201X, 207VG0400X, 207VH0002X, 207VM0101X, 207VX0000X, 207VE0102X, 207VF0040X #> Initializing environment... #> Error in nppes_initialize_environment(): argument \"setup_file\" is missing, with no default  # Example 2: Using custom taxonomy codes for filtering result <- nppes_get_data_for_one_year(   npi_file_path = \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data_Disseminat_September_2024/npidata_pfile_20050523-20240811.csv\",   output_csv_path = \"/path/to/output.csv\",   taxonomy_codes_1 = c(\"207X00000X\", \"207Y00000X\"),   taxonomy_codes_2 = c(\"207X00000X\", \"207Y00000X\") ) #> [2024-10-30 12:19:30.256133] Starting the NPPES data processing for one year... #> [2024-10-30 12:19:30.256409] Inputs: #> [2024-10-30 12:19:30.256553]   npi_file_path: /Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data_Disseminat_September_2024/npidata_pfile_20050523-20240811.csv #> [2024-10-30 12:19:30.25697]   output_csv_path: /path/to/output.csv #> [2024-10-30 12:19:30.257382]   duckdb_file_path: /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb #> [2024-10-30 12:19:30.257912]   taxonomy_codes_1: 207X00000X, 207Y00000X #> [2024-10-30 12:19:30.258334]   taxonomy_codes_2: 207X00000X, 207Y00000X #> Initializing environment... #> Error in nppes_initialize_environment(): argument \"setup_file\" is missing, with no default  # Example 3: Saving sample data to an Excel file result <- nppes_get_data_for_one_year(   npi_file_path = \"/path/to/npi_file.csv\",   output_csv_path = \"/path/to/output.csv\",   save_column_in_each_nppes_year = TRUE,   excel_file_path = \"/path/to/sample_data.xlsx\" ) #> [2024-10-30 12:19:30.328177] Starting the NPPES data processing for one year... #> [2024-10-30 12:19:30.328379] Inputs: #> [2024-10-30 12:19:30.328501]   npi_file_path: /path/to/npi_file.csv #> [2024-10-30 12:19:30.328952]   output_csv_path: /path/to/output.csv #> [2024-10-30 12:19:30.329412]   duckdb_file_path: /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb #> [2024-10-30 12:19:30.329715]   taxonomy_codes_1: 207V00000X, 207VB0002X, 207VC0300X, 207VC0200X, 207VX0201X, 207VG0400X, 207VH0002X, 207VM0101X, 207VX0000X, 207VE0102X, 207VF0040X #> [2024-10-30 12:19:30.329998]   taxonomy_codes_2: 207V00000X, 207VB0002X, 207VC0300X, 207VC0200X, 207VX0201X, 207VG0400X, 207VH0002X, 207VM0101X, 207VX0000X, 207VE0102X, 207VF0040X #> Initializing environment... #> Error in nppes_initialize_environment(): argument \"setup_file\" is missing, with no default"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_initialize_environment.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","title":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","text":"function sets environment sourcing setup script resolving package conflicts.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_initialize_environment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","text":"","code":"nppes_initialize_environment(setup_file)"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_initialize_environment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_initialize_environment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","text":"initializing environment, next step connect DuckDB using connect_to_duckdb().","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_initialize_environment.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Initialize the environment by loading necessary libraries and resolving conflicts — nppes_initialize_environment","text":"","code":"nppes_initialize_environment() #> Error in nppes_initialize_environment(): could not find function \"nppes_initialize_environment\" # Next step: tyler::connect_to_duckdb()"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":null,"dir":"Reference","previous_headings":"","what":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"function processes NPI table DuckDB, filtering taxonomy codes cleaning data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"","code":"nppes_process_npi_table_chunk(   con,   table_name = \"npi_2024\",   taxonomy_codes_1 = c(\"390200000X\", \"174400000X\", \"207V00000X\", \"207VB0002X\",     \"207VC0300X\", \"207VC0200X\", \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\",     \"207VX0000X\", \"207VE0102X\", \"207VF0040X\"),   taxonomy_codes_2 = c(\"390200000X\", \"174400000X\", \"207V00000X\", \"207VB0002X\",     \"207VC0300X\", \"207VC0200X\", \"207VX0201X\", \"207VG0400X\", \"207VH0002X\", \"207VM0101X\",     \"207VX0000X\", \"207VE0102X\", \"207VF0040X\") )"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"con DuckDB connection object. table_name character string specifying name table process. Default \"npi_2024\". taxonomy_codes_1 character vector specifying taxonomy codes filter Healthcare Provider Taxonomy Code_1. Default predefined set codes. taxonomy_codes_2 character vector specifying taxonomy codes filter Healthcare Provider Taxonomy Code_2. Default predefined set codes.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"tbl object containing processed data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"processing NPI table, next step collect clean data using nppes_collect_and_clean_data().","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_process_npi_table_chunk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process an NPI table in DuckDB — nppes_process_npi_table_chunk","text":"","code":"con <- nppes_connect_to_duckdb(\"/path/to/your/duckdb_file.duckdb\") #> Connecting to DuckDB at /path/to/your/duckdb_file.duckdb ... #> Warning: cannot open file '/path/to/your/duckdb_file.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection processed_data <- nppes_process_npi_table_chunk(con, \"npi_2024\", c(\"207V00000X\"), c(\"207V00000X\")) #> Error in nppes_process_npi_table_chunk(con, \"npi_2024\", c(\"207V00000X\"),     c(\"207V00000X\")): could not find function \"nppes_process_npi_table_chunk\" # Next step: nppes_collect_and_clean_data()  con <- nppes_connect_to_duckdb(\"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\") #> Connecting to DuckDB at /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb ... #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection processed_data <- nppes_process_npi_table_chunk(con) #> Error in nppes_process_npi_table_chunk(con): could not find function \"nppes_process_npi_table_chunk\" # Next step: nppes_collect_and_clean_data()"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Query sample data from a DuckDB table — nppes_query_sample_data","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"function queries sample data specified DuckDB table.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"","code":"nppes_query_sample_data(   con,   table_name = \"npi_2024\",   limit = 5,   save_column_in_each_nppes_year = TRUE,   excel_file_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"con DuckDB connection object. table_name character string specifying name table query. Default \"npi_2024\". limit integer specifying number rows retrieve. Default 5. save_column_in_each_nppes_year logical value indicating whether save sample data Excel file. Default FALSE. excel_file_path character string specifying path Excel file save_column_in_each_nppes_year TRUE. Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"data frame containing queried sample data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"querying sample data, next step process NPI table using nppes_process_npi_table_chunk().","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_query_sample_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query sample data from a DuckDB table — nppes_query_sample_data","text":"","code":"con <- nppes_connect_to_duckdb(\"/path/to/your/duckdb_file.duckdb\") #> Connecting to DuckDB at /path/to/your/duckdb_file.duckdb ... #> Warning: cannot open file '/path/to/your/duckdb_file.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection nppes_query_sample_data(con, \"npi_2024\", 5) #> Error in nppes_query_sample_data(con, \"npi_2024\", 5): could not find function \"nppes_query_sample_data\" # Next step: nppes_process_npi_table_chunk()  con <- nppes_connect_to_duckdb(\"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb\") #> Connecting to DuckDB at /Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb ... #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb': No such file or directory #> Error in file(con, \"w\"): cannot open the connection nppes_query_sample_data(con, \"npi_2024\", 10, TRUE, \"/path/to/excel_file.xlsx\") #> Error in nppes_query_sample_data(con, \"npi_2024\", 10, TRUE, \"/path/to/excel_file.xlsx\"): could not find function \"nppes_query_sample_data\" # Next step: nppes_process_npi_table_chunk()"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Save the final data to CSV — nppes_save_data_to_csv","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"function saves cleaned data CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"","code":"nppes_save_data_to_csv(data, file_path)"},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"data cleaned data frame. file_path character string specifying path CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"None","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"Use function save final cleaned data specified file path.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/nppes_save_data_to_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Save the final data to CSV — nppes_save_data_to_csv","text":"","code":"nppes_save_data_to_csv(cleaned_data, \"/path/to/your/file.csv\") #> Error in nppes_save_data_to_csv(cleaned_data, \"/path/to/your/file.csv\"): could not find function \"nppes_save_data_to_csv\""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"function reads filtered OBGYN data CSV file, merges crosswalk data RDS file NPI column, adds prefixes distinguish columns dataset, writes merged data CSV file. Detailed logging performed step process.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"","code":"open_payments_collect_and_convert(   duckdb_file_path,   specialty_csv_path,   crosswalk_rds_path,   log_path,   output_csv =     \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/end_csv_output_open_payments_collect_and_convert.csv\" )"},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"duckdb_file_path string representing full path DuckDB database file. used logging purposes, although data read directly DuckDB function. specialty_csv_path string representing full path CSV file containing filtered OBGYN data Open Payments dataset. file contain \"Covered_Recipient_NPI\" column. crosswalk_rds_path string representing full path RDS file containing crosswalk data. crosswalk data contain \"NPI\" column matches \"Covered_Recipient_NPI\" OBGYN data. log_path string representing full path text file logging details execution function written. output_csv string representing full path output CSV file merged data written. default \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/end_csv_output_open_payments_collect_and_convert.csv\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"data frame containing merged data OBGYN crosswalk datasets. columns OBGYN dataset prefixed \"OP_\" columns crosswalk dataset prefixed \"lo_data_\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"function follows steps: Reads OBGYN data provided CSV file renames \"Covered_Recipient_NPI\" column \"NPI\". Adds prefix \"OP_\" columns OBGYN data avoid column name conflicts final merged data. Reads crosswalk data provided RDS file, ensures unique NPI values retained using distinct(), adds prefix \"lo_data_\" columns. Performs left join OBGYN data crosswalk data \"NPI\" column (now prefixed \"OP_NPI\" \"lo_data_NPI\"). Logs number rows merged data writes merged data specified output CSV file. Logs steps, including DuckDB connection, data loading, column renaming, merging, output writing.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_collect_and_convert.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Collect and Convert Open Payments Data with Crosswalk Merging — open_payments_collect_and_convert","text":"","code":"# Example 1: Using default output path and logging to a file if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\",   specialty_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv\",   crosswalk_rds_path = \"/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/merged_into_a_crosswalk.rds\",   log_path = \"/path/to/log_file.txt\" ) } # }  # Example 2: Custom output path for CSV and detailed logging if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/path/to/duckdb_file.duckdb\",   specialty_csv_path = \"/path/to/specialty_data.csv\",   crosswalk_rds_path = \"/path/to/crosswalk_data.rds\",   log_path = \"/path/to/detailed_log.txt\",   output_csv = \"/path/to/merged_output_data.csv\" ) } # }  # Example 3: Using different file locations for each argument if (FALSE) { # \\dontrun{ open_payments_collect_and_convert(   duckdb_file_path = \"/my_data/nppes_historical_downloads/duckdb_file.duckdb\",   specialty_csv_path = \"/my_data/open_payments_data/filtered_data.csv\",   crosswalk_rds_path = \"/my_data/crosswalk_data/crosswalk_file.rds\",   log_path = \"/my_data/logs/processing_log.txt\",   output_csv = \"/my_data/output/merged_data.csv\" ) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_processed_per_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"function reads data CSV file chunks, processes grouping data Covered_Recipient_NPI, writes result specified CSV file current date file name. includes logging, error handling, system beeps progress completion.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_processed_per_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"","code":"open_payments_processed_per_npi(   input_csv_path,   output_csv_path =     \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/end_payments_per_npi.csv\",   chunk_size = 1e+05,   log_function = message,   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_processed_per_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"input_csv_path file path read input CSV file. output_csv_path file path write resulting CSV file (current date appended). path checked existence writability writing. chunk_size Integer, size chunks process. Defaults 100,000 rows per chunk. log_function logging function, defaults message. verbose Logical, whether display detailed logging. Defaults TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_processed_per_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"list containing total rows processed output CSV file path.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_processed_per_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename — open_payments_processed_per_npi","text":"","code":"# Example 1: Process payments per NPI from CSV and write output with date result <- open_payments_processed_per_npi(   input_csv_path = \"path/to/input.csv\",   output_csv_path = \"path/to/output.csv\" ) #> [2024-10-06 19:54:36.361408] Starting open_payments_processed_per_npi. #> Error in open_payments_processed_per_npi(input_csv_path = \"path/to/input.csv\",     output_csv_path = \"path/to/output.csv\"): The input file 'path/to/input.csv' does not exist.  # Example 2: Custom logging function and different output path result <- open_payments_processed_per_npi(   input_csv_path = \"path/to/input.csv\",   output_csv_path = \"custom/output/path.csv\",   log_function = print ) #> [2024-10-06 19:54:36.363739] Starting open_payments_processed_per_npi. #> Error in open_payments_processed_per_npi(input_csv_path = \"path/to/input.csv\",     output_csv_path = \"custom/output/path.csv\", log_function = print): The input file 'path/to/input.csv' does not exist."},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_specialty_cleaning.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"function connects DuckDB database, processes tables containing Open Payments data, filters records based specified medical specialties. data processed chunks minimize memory usage, filtered results saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_specialty_cleaning.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"","code":"open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(),     \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\",     \"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   output_csv_path =     \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv\",   specialties = c(\"Specialist\",     \"Student in an Organized Health Care Education/Training Program\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Critical Care Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Female Pelvic Medicine and Reconstructive Surgery\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecologic Oncology\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecology\",           \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Hospice and Palliative Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Obstetrics\",     \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology\"),   chunk_size = 1e+05 )"},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_specialty_cleaning.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"con Database connection DuckDB database. Defaults connection using provided DuckDB file path. table_names character vector table names DuckDB processed. table processed chunks reduce memory load. output_csv_path file path cleaned filtered Open Payments data saved. file path directory exist, created. specialties character vector medical specialties filter data. function return rows Covered_Recipient_Specialty_X columns match one given specialties. chunk_size size data chunk process time. allows function handle large datasets efficiently breaking query smaller parts. Defaults 100,000 rows per chunk.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_specialty_cleaning.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"list containing names processed tables.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/open_payments_specialty_cleaning.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean and Filter Open Payments Data by Specialty — open_payments_specialty_cleaning","text":"","code":"# Example 1: Process two Open Payments tables for specific specialties and save to a CSV open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(), \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2020_P01182024\", \"OP_DTL_GNRL_PGYR2021_P01182024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",                   \"Student in an Organized Health Care Education/Training Program\"),   chunk_size = 50000 ) #> [2024-10-06 19:54:36.657799] Function inputs: #> [2024-10-06 19:54:36.658138] Tables: OP_DTL_GNRL_PGYR2020_P01182024, OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:54:36.658427] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:36.658718] Specialties: Allopathic & Osteopathic Physicians|Obstetrics & Gynecology, Student in an Organized Health Care Education/Training Program #> [2024-10-06 19:54:36.659018] Processing table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:38.958986] Table OP_DTL_GNRL_PGYR2020_P01182024 has 5835911 total rows #> [2024-10-06 19:54:38.959226] Processing rows 1 to 50000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:40.421807] Filtered data collected for rows 1 to 50000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:40.627625] Data for rows 1 to 50000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:40.89257] Processing rows 50001 to 100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:44.522848] Filtered data collected for rows 50001 to 100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:44.623869] Data for rows 50001 to 100000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:44.887473] Processing rows 100001 to 150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:48.903924] Filtered data collected for rows 100001 to 150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:48.991548] Data for rows 100001 to 150000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:54:49.251499] Processing rows 150001 to 200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.684632] Filtered data collected for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.685222] No data to write for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:50.945733] Processing rows 200001 to 250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.419465] Filtered data collected for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.419875] No data to write for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:52.68299] Processing rows 250001 to 300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.11445] Filtered data collected for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.114688] No data to write for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:54.374917] Processing rows 300001 to 350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:55.763748] Filtered data collected for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:55.763973] No data to write for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:56.027827] Processing rows 350001 to 400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.458076] Filtered data collected for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.458312] No data to write for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:57.719885] Processing rows 400001 to 450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.143698] Filtered data collected for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.143927] No data to write for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:54:59.438829] Processing rows 450001 to 500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:00.865342] Filtered data collected for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:00.86556] No data to write for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:01.126091] Processing rows 500001 to 550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.599896] Filtered data collected for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.600117] No data to write for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:02.86258] Processing rows 550001 to 600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.308192] Filtered data collected for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.308429] No data to write for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:04.572263] Processing rows 600001 to 650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.00788] Filtered data collected for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.008109] No data to write for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:06.269521] Processing rows 650001 to 700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.711779] Filtered data collected for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.712027] No data to write for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:07.973281] Processing rows 700001 to 750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.471128] Filtered data collected for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.471354] No data to write for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:09.731884] Processing rows 750001 to 800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.208401] Filtered data collected for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.208644] No data to write for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:11.468094] Processing rows 800001 to 850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:12.909468] Filtered data collected for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:12.909686] No data to write for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:13.169408] Processing rows 850001 to 900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.575704] Filtered data collected for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.575945] No data to write for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:14.840325] Processing rows 900001 to 950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.311841] Filtered data collected for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.312085] No data to write for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:16.572346] Processing rows 950001 to 1000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:17.998696] Filtered data collected for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:17.998961] No data to write for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:18.291436] Processing rows 1000001 to 1050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.700554] Filtered data collected for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.700817] No data to write for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:19.960134] Processing rows 1050001 to 1100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.427145] Filtered data collected for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.427369] No data to write for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:21.691807] Processing rows 1100001 to 1150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.132789] Filtered data collected for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.133026] No data to write for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:23.395072] Processing rows 1150001 to 1200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:24.841939] Filtered data collected for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:24.842164] No data to write for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:25.104086] Processing rows 1200001 to 1250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.613225] Filtered data collected for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.613458] No data to write for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:26.872022] Processing rows 1250001 to 1300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.355621] Filtered data collected for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.355847] No data to write for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:28.615198] Processing rows 1300001 to 1350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.01858] Filtered data collected for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.018818] No data to write for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:30.276112] Processing rows 1350001 to 1400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.726144] Filtered data collected for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.726379] No data to write for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:31.986853] Processing rows 1400001 to 1450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.455851] Filtered data collected for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.456073] No data to write for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:33.71734] Processing rows 1450001 to 1500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.175378] Filtered data collected for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.175613] No data to write for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:35.437781] Processing rows 1500001 to 1550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:36.874159] Filtered data collected for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:36.874383] No data to write for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:37.135656] Processing rows 1550001 to 1600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.582873] Filtered data collected for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.583103] No data to write for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:38.842219] Processing rows 1600001 to 1650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.276898] Filtered data collected for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.277125] No data to write for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:40.536771] Processing rows 1650001 to 1700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:41.97152] Filtered data collected for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:41.971752] No data to write for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:42.231693] Processing rows 1700001 to 1750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.64521] Filtered data collected for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.645442] No data to write for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:43.938619] Processing rows 1750001 to 1800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.425223] Filtered data collected for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.425454] No data to write for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:45.684846] Processing rows 1800001 to 1850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.191397] Filtered data collected for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.191633] No data to write for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:47.452163] Processing rows 1850001 to 1900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:48.847165] Filtered data collected for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:48.847405] No data to write for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:49.107323] Processing rows 1900001 to 1950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.541264] Filtered data collected for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.541499] No data to write for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:50.801466] Processing rows 1950001 to 2000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.206171] Filtered data collected for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.206402] No data to write for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:52.467806] Processing rows 2000001 to 2050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:53.902115] Filtered data collected for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:53.902334] No data to write for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:54.162373] Processing rows 2050001 to 2100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.625052] Filtered data collected for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.625291] No data to write for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:55.889374] Processing rows 2100001 to 2150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.362016] Filtered data collected for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.362248] No data to write for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:57.621066] Processing rows 2150001 to 2200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.015406] Filtered data collected for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.015647] No data to write for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:55:59.278348] Processing rows 2200001 to 2250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.731394] Filtered data collected for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.731655] No data to write for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:00.990942] Processing rows 2250001 to 2300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.442929] Filtered data collected for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.443162] No data to write for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:02.704529] Processing rows 2300001 to 2350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.153256] Filtered data collected for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.153492] No data to write for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:04.414202] Processing rows 2350001 to 2400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:05.890313] Filtered data collected for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:05.890594] No data to write for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:06.198068] Processing rows 2400001 to 2450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.662549] Filtered data collected for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.662792] No data to write for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:07.921897] Processing rows 2450001 to 2500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.30842] Filtered data collected for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.308645] No data to write for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:09.600486] Processing rows 2500001 to 2550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.000366] Filtered data collected for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.000603] No data to write for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:11.259308] Processing rows 2550001 to 2600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.72769] Filtered data collected for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.727932] No data to write for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:12.990421] Processing rows 2600001 to 2650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.431199] Filtered data collected for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.431422] No data to write for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:14.693147] Processing rows 2650001 to 2700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.143099] Filtered data collected for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.143351] No data to write for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:16.405428] Processing rows 2700001 to 2750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:17.829377] Filtered data collected for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:17.829597] No data to write for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:18.08851] Processing rows 2750001 to 2800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.578764] Filtered data collected for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.579129] No data to write for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:19.84311] Processing rows 2800001 to 2850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.265371] Filtered data collected for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.265602] No data to write for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:21.522713] Processing rows 2850001 to 2900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.006424] Filtered data collected for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.006644] No data to write for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:23.263785] Processing rows 2900001 to 2950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.704706] Filtered data collected for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.704945] No data to write for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:24.962184] Processing rows 2950001 to 3000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.490486] Filtered data collected for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.490789] No data to write for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:26.753399] Processing rows 3000001 to 3050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.235956] Filtered data collected for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.236199] No data to write for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:28.501364] Processing rows 3050001 to 3100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:29.992328] Filtered data collected for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:29.992553] No data to write for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:30.250914] Processing rows 3100001 to 3150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.662256] Filtered data collected for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.662489] No data to write for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:31.923192] Processing rows 3150001 to 3200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.417629] Filtered data collected for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.417838] No data to write for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:33.677806] Processing rows 3200001 to 3250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.082577] Filtered data collected for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.082967] No data to write for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:35.38358] Processing rows 3250001 to 3300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:36.799019] Filtered data collected for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:36.799253] No data to write for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:37.059796] Processing rows 3300001 to 3350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.542105] Filtered data collected for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.542336] No data to write for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:38.804422] Processing rows 3350001 to 3400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.228064] Filtered data collected for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.228296] No data to write for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:40.48674] Processing rows 3400001 to 3450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.038292] Filtered data collected for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.038532] No data to write for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:42.297583] Processing rows 3450001 to 3500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.706204] Filtered data collected for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.706608] No data to write for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:43.966891] Processing rows 3500001 to 3550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.424101] Filtered data collected for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.424379] No data to write for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:45.692744] Processing rows 3550001 to 3600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.210792] Filtered data collected for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.211031] No data to write for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:47.473136] Processing rows 3600001 to 3650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:48.882321] Filtered data collected for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:48.88255] No data to write for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:49.143464] Processing rows 3650001 to 3700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.626803] Filtered data collected for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.627024] No data to write for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:50.887584] Processing rows 3700001 to 3750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.346517] Filtered data collected for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.346749] No data to write for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:52.608893] Processing rows 3750001 to 3800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.077097] Filtered data collected for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.077329] No data to write for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:54.337561] Processing rows 3800001 to 3850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:55.789885] Filtered data collected for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:55.790121] No data to write for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:56.051101] Processing rows 3850001 to 3900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.51004] Filtered data collected for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.51028] No data to write for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:57.769909] Processing rows 3900001 to 3950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.222468] Filtered data collected for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.222702] No data to write for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:56:59.486793] Processing rows 3950001 to 4000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:00.959999] Filtered data collected for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:00.960235] No data to write for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:01.224419] Processing rows 4000001 to 4050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.659759] Filtered data collected for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.659979] No data to write for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:02.920195] Processing rows 4050001 to 4100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.384705] Filtered data collected for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.384959] No data to write for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:04.647682] Processing rows 4100001 to 4150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.051038] Filtered data collected for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.051277] No data to write for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:06.353998] Processing rows 4150001 to 4200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:07.773527] Filtered data collected for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:07.773765] No data to write for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:08.034433] Processing rows 4200001 to 4250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.500248] Filtered data collected for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.501363] No data to write for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:09.770062] Processing rows 4250001 to 4300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.172485] Filtered data collected for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.172717] No data to write for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:11.433908] Processing rows 4300001 to 4350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:12.950702] Filtered data collected for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:12.95094] No data to write for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:13.215777] Processing rows 4350001 to 4400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.655557] Filtered data collected for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.655787] No data to write for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:14.924993] Processing rows 4400001 to 4450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.369574] Filtered data collected for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.369901] No data to write for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:16.635862] Processing rows 4450001 to 4500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.090189] Filtered data collected for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.090429] No data to write for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:18.351101] Processing rows 4500001 to 4550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:19.869756] Filtered data collected for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:19.869998] No data to write for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:20.132582] Processing rows 4550001 to 4600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.578934] Filtered data collected for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.579203] No data to write for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:21.841353] Processing rows 4600001 to 4650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.325049] Filtered data collected for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.325288] No data to write for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:23.589463] Processing rows 4650001 to 4700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.071945] Filtered data collected for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.072168] No data to write for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:25.336121] Processing rows 4700001 to 4750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:26.797755] Filtered data collected for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:26.797985] No data to write for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:27.061469] Processing rows 4750001 to 4800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.564097] Filtered data collected for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.56441] No data to write for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:28.862098] Processing rows 4800001 to 4850000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.327391] Filtered data collected for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.327623] No data to write for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:30.59112] Processing rows 4850001 to 4900000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.141984] Filtered data collected for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.142339] No data to write for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:32.40635] Processing rows 4900001 to 4950000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:33.866783] Filtered data collected for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:33.867007] No data to write for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:34.130757] Processing rows 4950001 to 5000000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.575404] Filtered data collected for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.575638] No data to write for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:35.838615] Processing rows 5000001 to 5050000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.278492] Filtered data collected for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.278778] No data to write for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:37.543004] Processing rows 5050001 to 5100000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:38.988772] Filtered data collected for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:38.989153] No data to write for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:39.252715] Processing rows 5100001 to 5150000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.677503] Filtered data collected for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.677743] No data to write for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:40.979359] Processing rows 5150001 to 5200000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.408938] Filtered data collected for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.409188] No data to write for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:42.671386] Processing rows 5200001 to 5250000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.142124] Filtered data collected for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.142352] No data to write for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:44.403373] Processing rows 5250001 to 5300000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:45.894613] Filtered data collected for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:45.906492] No data to write for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:46.172643] Processing rows 5300001 to 5350000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.70332] Filtered data collected for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.703562] No data to write for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:47.966771] Processing rows 5350001 to 5400000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.384105] Filtered data collected for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.38434] No data to write for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:49.646742] Processing rows 5400001 to 5450000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.084234] Filtered data collected for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.08463] No data to write for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:51.346518] Processing rows 5450001 to 5500000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:52.758238] Filtered data collected for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:52.75854] No data to write for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:53.023334] Processing rows 5500001 to 5550000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.47876] Filtered data collected for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.478999] No data to write for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:54.742678] Processing rows 5550001 to 5600000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.177177] Filtered data collected for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.17745] No data to write for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:56.479924] Processing rows 5600001 to 5650000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:57.976968] Filtered data collected for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:57.977227] No data to write for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:58.24307] Processing rows 5650001 to 5700000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:59.76101] Filtered data collected for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:57:59.761252] No data to write for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:00.026108] Processing rows 5700001 to 5750000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.455962] Filtered data collected for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.456197] No data to write for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:01.71507] Processing rows 5750001 to 5800000 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.252373] Filtered data collected for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.252606] No data to write for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:03.52362] Processing rows 5800001 to 5835911 for table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:04.993605] Filtered data collected for rows 5800001 to 5835911 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:04.993843] No data to write for rows 5800001 to 5835911 in table: OP_DTL_GNRL_PGYR2020_P01182024 #> [2024-10-06 19:58:05.337747] Processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:05.401432] Table OP_DTL_GNRL_PGYR2021_P01182024 has 11496362 total rows #> [2024-10-06 19:58:05.401686] Processing rows 1 to 50000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:07.582037] Filtered data collected for rows 1 to 50000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:07.800402] Data for rows 1 to 50000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:08.072737] Processing rows 50001 to 100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:12.528184] Filtered data collected for rows 50001 to 100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:12.635881] Data for rows 50001 to 100000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:12.90246] Processing rows 100001 to 150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:17.348593] Filtered data collected for rows 100001 to 150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:17.484077] Data for rows 100001 to 150000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:17.751745] Processing rows 150001 to 200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:24.137358] Filtered data collected for rows 150001 to 200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:24.246593] Data for rows 150001 to 200000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:24.511436] Processing rows 200001 to 250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:27.173458] Filtered data collected for rows 200001 to 250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:27.21521] Data for rows 200001 to 250000 written to: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-06 19:58:27.480693] Processing rows 250001 to 300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:29.893727] Filtered data collected for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:29.89397] No data to write for rows 250001 to 300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:30.154751] Processing rows 300001 to 350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.600959] Filtered data collected for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.601204] No data to write for rows 300001 to 350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:32.868283] Processing rows 350001 to 400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.383161] Filtered data collected for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.383375] No data to write for rows 350001 to 400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:35.645769] Processing rows 400001 to 450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.265249] Filtered data collected for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.265484] No data to write for rows 400001 to 450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:38.530297] Processing rows 450001 to 500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.219242] Filtered data collected for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.219488] No data to write for rows 450001 to 500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:41.488003] Processing rows 500001 to 550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.030537] Filtered data collected for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.030794] No data to write for rows 500001 to 550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:44.29309] Processing rows 550001 to 600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:46.815922] Filtered data collected for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:46.816153] No data to write for rows 550001 to 600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:47.075839] Processing rows 600001 to 650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.625032] Filtered data collected for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.625268] No data to write for rows 600001 to 650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:49.884464] Processing rows 650001 to 700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.291134] Filtered data collected for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.291361] No data to write for rows 650001 to 700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:52.554202] Processing rows 700001 to 750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:54.956599] Filtered data collected for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:54.956834] No data to write for rows 700001 to 750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:55.216258] Processing rows 750001 to 800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.644885] Filtered data collected for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.645125] No data to write for rows 750001 to 800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:58:57.905648] Processing rows 800001 to 850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.322526] Filtered data collected for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.322763] No data to write for rows 800001 to 850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:00.624552] Processing rows 850001 to 900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.074821] Filtered data collected for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.075059] No data to write for rows 850001 to 900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:03.341261] Processing rows 900001 to 950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:05.749777] Filtered data collected for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:05.750014] No data to write for rows 900001 to 950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:06.011076] Processing rows 950001 to 1000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.454576] Filtered data collected for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.454812] No data to write for rows 950001 to 1000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:08.720477] Processing rows 1000001 to 1050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.127091] Filtered data collected for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.127337] No data to write for rows 1000001 to 1050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:11.388277] Processing rows 1050001 to 1100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:13.824014] Filtered data collected for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:13.824264] No data to write for rows 1050001 to 1100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:14.086795] Processing rows 1100001 to 1150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.509707] Filtered data collected for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.509974] No data to write for rows 1100001 to 1150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:16.814213] Processing rows 1150001 to 1200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.291625] Filtered data collected for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.291862] No data to write for rows 1150001 to 1200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:19.557403] Processing rows 1200001 to 1250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:21.964573] Filtered data collected for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:21.9648] No data to write for rows 1200001 to 1250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:22.225079] Processing rows 1250001 to 1300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.725541] Filtered data collected for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.725764] No data to write for rows 1250001 to 1300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:24.987933] Processing rows 1300001 to 1350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.408375] Filtered data collected for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.408822] No data to write for rows 1300001 to 1350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:27.679193] Processing rows 1350001 to 1400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.18136] Filtered data collected for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.181601] No data to write for rows 1350001 to 1400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:30.443418] Processing rows 1400001 to 1450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:32.858113] Filtered data collected for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:32.858364] No data to write for rows 1400001 to 1450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:33.120106] Processing rows 1450001 to 1500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.626115] Filtered data collected for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.626354] No data to write for rows 1450001 to 1500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:35.891241] Processing rows 1500001 to 1550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.581989] Filtered data collected for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.582248] No data to write for rows 1500001 to 1550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:38.847952] Processing rows 1550001 to 1600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.365471] Filtered data collected for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.365699] No data to write for rows 1550001 to 1600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:41.624983] Processing rows 1600001 to 1650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.590481] Filtered data collected for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.59117] No data to write for rows 1600001 to 1650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:44.89358] Processing rows 1650001 to 1700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.687234] Filtered data collected for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.687456] No data to write for rows 1650001 to 1700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:47.954553] Processing rows 1700001 to 1750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.597477] Filtered data collected for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.597767] No data to write for rows 1700001 to 1750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:50.887989] Processing rows 1750001 to 1800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.481372] Filtered data collected for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.481602] No data to write for rows 1750001 to 1800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:53.74289] Processing rows 1800001 to 1850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.187929] Filtered data collected for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.188161] No data to write for rows 1800001 to 1850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:56.449193] Processing rows 1850001 to 1900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:58.877325] Filtered data collected for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:58.877564] No data to write for rows 1850001 to 1900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 19:59:59.136961] Processing rows 1900001 to 1950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.617168] Filtered data collected for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.617405] No data to write for rows 1900001 to 1950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:01.880214] Processing rows 1950001 to 2000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.297137] Filtered data collected for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.297358] No data to write for rows 1950001 to 2000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:04.557476] Processing rows 2000001 to 2050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:06.988654] Filtered data collected for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:06.988889] No data to write for rows 2000001 to 2050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:07.256729] Processing rows 2050001 to 2100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.712555] Filtered data collected for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.712792] No data to write for rows 2050001 to 2100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:09.975483] Processing rows 2100001 to 2150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.471211] Filtered data collected for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.471442] No data to write for rows 2100001 to 2150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:12.738418] Processing rows 2150001 to 2200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.202185] Filtered data collected for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.202435] No data to write for rows 2150001 to 2200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:15.461956] Processing rows 2200001 to 2250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:17.905893] Filtered data collected for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:17.90613] No data to write for rows 2200001 to 2250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:18.166549] Processing rows 2250001 to 2300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.560704] Filtered data collected for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.560944] No data to write for rows 2250001 to 2300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:20.823409] Processing rows 2300001 to 2350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.337865] Filtered data collected for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.33816] No data to write for rows 2300001 to 2350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:23.603997] Processing rows 2350001 to 2400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:25.986081] Filtered data collected for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:25.986314] No data to write for rows 2350001 to 2400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:26.247822] Processing rows 2400001 to 2450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.67258] Filtered data collected for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.672824] No data to write for rows 2400001 to 2450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:28.93457] Processing rows 2450001 to 2500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.335461] Filtered data collected for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.335689] No data to write for rows 2450001 to 2500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:31.59521] Processing rows 2500001 to 2550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.039298] Filtered data collected for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.039531] No data to write for rows 2500001 to 2550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:34.308888] Processing rows 2550001 to 2600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:36.990506] Filtered data collected for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:36.990732] No data to write for rows 2550001 to 2600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:37.255638] Processing rows 2600001 to 2650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.032619] Filtered data collected for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.032852] No data to write for rows 2600001 to 2650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:40.297722] Processing rows 2650001 to 2700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:42.832194] Filtered data collected for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:42.832421] No data to write for rows 2650001 to 2700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:43.094306] Processing rows 2700001 to 2750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.607377] Filtered data collected for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.607688] No data to write for rows 2700001 to 2750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:45.902113] Processing rows 2750001 to 2800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.385558] Filtered data collected for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.385767] No data to write for rows 2750001 to 2800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:48.64796] Processing rows 2800001 to 2850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.144631] Filtered data collected for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.144866] No data to write for rows 2800001 to 2850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:51.40666] Processing rows 2850001 to 2900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:53.938791] Filtered data collected for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:53.939023] No data to write for rows 2850001 to 2900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:54.207244] Processing rows 2900001 to 2950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.661421] Filtered data collected for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.661657] No data to write for rows 2900001 to 2950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:56.928862] Processing rows 2950001 to 3000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.420586] Filtered data collected for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.420823] No data to write for rows 2950001 to 3000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:00:59.684029] Processing rows 3000001 to 3050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.182985] Filtered data collected for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.183226] No data to write for rows 3000001 to 3050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:02.446375] Processing rows 3050001 to 3100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:04.895276] Filtered data collected for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:04.895515] No data to write for rows 3050001 to 3100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:05.187097] Processing rows 3100001 to 3150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:07.830431] Filtered data collected for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:07.830658] No data to write for rows 3100001 to 3150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:08.089275] Processing rows 3150001 to 3200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.56057] Filtered data collected for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.560807] No data to write for rows 3150001 to 3200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:10.819742] Processing rows 3200001 to 3250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.350614] Filtered data collected for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.35082] No data to write for rows 3200001 to 3250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:13.612518] Processing rows 3250001 to 3300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.032512] Filtered data collected for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.032761] No data to write for rows 3250001 to 3300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:16.292694] Processing rows 3300001 to 3350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:18.781331] Filtered data collected for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:18.781577] No data to write for rows 3300001 to 3350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:19.049154] Processing rows 3350001 to 3400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.530672] Filtered data collected for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.530994] No data to write for rows 3350001 to 3400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:21.801162] Processing rows 3400001 to 3450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.358126] Filtered data collected for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.358357] No data to write for rows 3400001 to 3450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:24.621932] Processing rows 3450001 to 3500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.077936] Filtered data collected for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.078302] No data to write for rows 3450001 to 3500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:27.382312] Processing rows 3500001 to 3550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:29.80266] Filtered data collected for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:29.80302] No data to write for rows 3500001 to 3550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:30.063239] Processing rows 3550001 to 3600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.586549] Filtered data collected for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.586981] No data to write for rows 3550001 to 3600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:32.848767] Processing rows 3600001 to 3650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.298182] Filtered data collected for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.298417] No data to write for rows 3600001 to 3650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:35.558455] Processing rows 3650001 to 3700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.14124] Filtered data collected for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.141453] No data to write for rows 3650001 to 3700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:38.403082] Processing rows 3700001 to 3750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:40.891586] Filtered data collected for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:40.891829] No data to write for rows 3700001 to 3750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:41.15072] Processing rows 3750001 to 3800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.649296] Filtered data collected for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.649533] No data to write for rows 3750001 to 3800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:43.94663] Processing rows 3800001 to 3850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.557982] Filtered data collected for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.558226] No data to write for rows 3800001 to 3850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:46.820975] Processing rows 3850001 to 3900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.479911] Filtered data collected for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.480158] No data to write for rows 3850001 to 3900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:49.74559] Processing rows 3900001 to 3950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.38654] Filtered data collected for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.386779] No data to write for rows 3900001 to 3950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:52.653228] Processing rows 3950001 to 4000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.177089] Filtered data collected for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.177329] No data to write for rows 3950001 to 4000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:55.443219] Processing rows 4000001 to 4050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:57.988245] Filtered data collected for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:57.988482] No data to write for rows 4000001 to 4050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:01:58.248976] Processing rows 4050001 to 4100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:00.798233] Filtered data collected for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:00.798461] No data to write for rows 4050001 to 4100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:01.057806] Processing rows 4100001 to 4150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.534392] Filtered data collected for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.534617] No data to write for rows 4100001 to 4150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:03.797488] Processing rows 4150001 to 4200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.189022] Filtered data collected for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.189256] No data to write for rows 4150001 to 4200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:06.479333] Processing rows 4200001 to 4250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:08.876655] Filtered data collected for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:08.876909] No data to write for rows 4200001 to 4250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:09.137885] Processing rows 4250001 to 4300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.600208] Filtered data collected for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.60058] No data to write for rows 4250001 to 4300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:11.861303] Processing rows 4300001 to 4350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.28963] Filtered data collected for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.289853] No data to write for rows 4300001 to 4350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:14.550174] Processing rows 4350001 to 4400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.121097] Filtered data collected for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.121341] No data to write for rows 4350001 to 4400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:17.382886] Processing rows 4400001 to 4450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:19.932045] Filtered data collected for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:19.932278] No data to write for rows 4400001 to 4450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:20.191363] Processing rows 4450001 to 4500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.687743] Filtered data collected for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.68798] No data to write for rows 4450001 to 4500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:22.953497] Processing rows 4500001 to 4550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.507224] Filtered data collected for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.507459] No data to write for rows 4500001 to 4550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:25.768815] Processing rows 4550001 to 4600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.389618] Filtered data collected for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.389847] No data to write for rows 4550001 to 4600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:28.69197] Processing rows 4600001 to 4650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.235887] Filtered data collected for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.23612] No data to write for rows 4600001 to 4650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:31.500246] Processing rows 4650001 to 4700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.078875] Filtered data collected for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.07911] No data to write for rows 4650001 to 4700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:34.344137] Processing rows 4700001 to 4750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:36.817146] Filtered data collected for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:36.817385] No data to write for rows 4700001 to 4750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:37.084546] Processing rows 4750001 to 4800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.581587] Filtered data collected for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.581834] No data to write for rows 4750001 to 4800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:39.842481] Processing rows 4800001 to 4850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.281135] Filtered data collected for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.28144] No data to write for rows 4800001 to 4850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:42.546987] Processing rows 4850001 to 4900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:44.957227] Filtered data collected for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:44.957548] No data to write for rows 4850001 to 4900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:45.225846] Processing rows 4900001 to 4950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.643085] Filtered data collected for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.643307] No data to write for rows 4900001 to 4950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:47.943909] Processing rows 4950001 to 5000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.302424] Filtered data collected for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.302648] No data to write for rows 4950001 to 5000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:50.5631] Processing rows 5000001 to 5050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:52.971501] Filtered data collected for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:52.971722] No data to write for rows 5000001 to 5050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:53.231383] Processing rows 5050001 to 5100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.636243] Filtered data collected for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.636464] No data to write for rows 5050001 to 5100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:55.89676] Processing rows 5100001 to 5150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.326498] Filtered data collected for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.326735] No data to write for rows 5100001 to 5150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:02:58.589203] Processing rows 5150001 to 5200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.061394] Filtered data collected for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.061623] No data to write for rows 5150001 to 5200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:01.322632] Processing rows 5200001 to 5250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:03.787833] Filtered data collected for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:03.788056] No data to write for rows 5200001 to 5250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:04.054421] Processing rows 5250001 to 5300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.442828] Filtered data collected for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.443064] No data to write for rows 5250001 to 5300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:06.733789] Processing rows 5300001 to 5350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.18226] Filtered data collected for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.182485] No data to write for rows 5300001 to 5350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:09.443307] Processing rows 5350001 to 5400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:11.842539] Filtered data collected for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:11.842779] No data to write for rows 5350001 to 5400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:12.107161] Processing rows 5400001 to 5450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.664536] Filtered data collected for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.664918] No data to write for rows 5400001 to 5450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:14.934234] Processing rows 5450001 to 5500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.498384] Filtered data collected for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.498619] No data to write for rows 5450001 to 5500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:17.763785] Processing rows 5500001 to 5550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.294787] Filtered data collected for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.295022] No data to write for rows 5500001 to 5550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:20.561449] Processing rows 5550001 to 5600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.06862] Filtered data collected for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.068848] No data to write for rows 5550001 to 5600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:23.329029] Processing rows 5600001 to 5650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.727765] Filtered data collected for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.728001] No data to write for rows 5600001 to 5650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:25.98832] Processing rows 5650001 to 5700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.529168] Filtered data collected for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.529397] No data to write for rows 5650001 to 5700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:28.788849] Processing rows 5700001 to 5750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.180571] Filtered data collected for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.180792] No data to write for rows 5700001 to 5750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:31.442181] Processing rows 5750001 to 5800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:33.931121] Filtered data collected for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:33.931351] No data to write for rows 5750001 to 5800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:34.191055] Processing rows 5800001 to 5850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.724481] Filtered data collected for rows 5800001 to 5850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.724706] No data to write for rows 5800001 to 5850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:36.987146] Processing rows 5850001 to 5900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.571637] Filtered data collected for rows 5850001 to 5900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.571876] No data to write for rows 5850001 to 5900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:39.835622] Processing rows 5900001 to 5950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.63346] Filtered data collected for rows 5900001 to 5950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.633713] No data to write for rows 5900001 to 5950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:42.898937] Processing rows 5950001 to 6000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.39422] Filtered data collected for rows 5950001 to 6000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.394472] No data to write for rows 5950001 to 6000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:45.653195] Processing rows 6000001 to 6050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.161039] Filtered data collected for rows 6000001 to 6050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.161291] No data to write for rows 6000001 to 6050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:48.453952] Processing rows 6050001 to 6100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:50.825356] Filtered data collected for rows 6050001 to 6100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:50.82558] No data to write for rows 6050001 to 6100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:51.466098] Processing rows 6100001 to 6150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:53.901961] Filtered data collected for rows 6100001 to 6150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:53.902193] No data to write for rows 6100001 to 6150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:54.158666] Processing rows 6150001 to 6200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.384405] Filtered data collected for rows 6150001 to 6200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.384677] No data to write for rows 6150001 to 6200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:56.642812] Processing rows 6200001 to 6250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:58.853581] Filtered data collected for rows 6200001 to 6250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:58.853807] No data to write for rows 6200001 to 6250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:03:59.111211] Processing rows 6250001 to 6300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.326167] Filtered data collected for rows 6250001 to 6300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.326397] No data to write for rows 6250001 to 6300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:01.585667] Processing rows 6300001 to 6350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:03.834212] Filtered data collected for rows 6300001 to 6350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:03.834444] No data to write for rows 6300001 to 6350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:04.089694] Processing rows 6350001 to 6400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.353563] Filtered data collected for rows 6350001 to 6400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.353797] No data to write for rows 6350001 to 6400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:06.612699] Processing rows 6400001 to 6450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:08.83107] Filtered data collected for rows 6400001 to 6450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:08.831307] No data to write for rows 6400001 to 6450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:09.083737] Processing rows 6450001 to 6500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.393956] Filtered data collected for rows 6450001 to 6500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.395072] No data to write for rows 6450001 to 6500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-06 20:04:11.653569] Processing rows 6500001 to 6550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.448646] Filtered data collected for rows 6500001 to 6550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.449474] No data to write for rows 6500001 to 6550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:38.837728] Processing rows 6550001 to 6600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.498041] Filtered data collected for rows 6550001 to 6600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.498396] No data to write for rows 6550001 to 6600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:43.811791] Processing rows 6600001 to 6650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.676279] Filtered data collected for rows 6600001 to 6650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.677482] No data to write for rows 6600001 to 6650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:48.99056] Processing rows 6650001 to 6700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:52.794072] Filtered data collected for rows 6650001 to 6700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:52.794396] No data to write for rows 6650001 to 6700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:53.165429] Processing rows 6700001 to 6750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:57.852296] Filtered data collected for rows 6700001 to 6750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:57.852637] No data to write for rows 6700001 to 6750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:41:58.2417] Processing rows 6750001 to 6800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.247403] Filtered data collected for rows 6750001 to 6800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.24777] No data to write for rows 6750001 to 6800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:02.597244] Processing rows 6800001 to 6850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.06223] Filtered data collected for rows 6800001 to 6850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.062531] No data to write for rows 6800001 to 6850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:06.431819] Processing rows 6850001 to 6900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:10.793265] Filtered data collected for rows 6850001 to 6900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:10.793545] No data to write for rows 6850001 to 6900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:11.157301] Processing rows 6900001 to 6950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:14.904761] Filtered data collected for rows 6900001 to 6950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:14.90498] No data to write for rows 6900001 to 6950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:15.198621] Processing rows 6950001 to 7000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.501108] Filtered data collected for rows 6950001 to 7000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.501356] No data to write for rows 6950001 to 7000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:18.789321] Processing rows 7000001 to 7050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.106136] Filtered data collected for rows 7000001 to 7050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.106372] No data to write for rows 7000001 to 7050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:22.380147] Processing rows 7050001 to 7100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.453639] Filtered data collected for rows 7050001 to 7100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.457893] No data to write for rows 7050001 to 7100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:25.740413] Processing rows 7100001 to 7150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:28.816216] Filtered data collected for rows 7100001 to 7150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:28.816438] No data to write for rows 7100001 to 7150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:29.118048] Processing rows 7150001 to 7200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.167528] Filtered data collected for rows 7150001 to 7200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.167792] No data to write for rows 7150001 to 7200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:32.443844] Processing rows 7200001 to 7250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.705371] Filtered data collected for rows 7200001 to 7250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.705609] No data to write for rows 7200001 to 7250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:35.975689] Processing rows 7250001 to 7300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.042345] Filtered data collected for rows 7250001 to 7300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.04268] No data to write for rows 7250001 to 7300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:39.328029] Processing rows 7300001 to 7350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.304096] Filtered data collected for rows 7300001 to 7350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.304307] No data to write for rows 7300001 to 7350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:42.582214] Processing rows 7350001 to 7400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.485503] Filtered data collected for rows 7350001 to 7400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.485856] No data to write for rows 7350001 to 7400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:45.751265] Processing rows 7400001 to 7450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.678113] Filtered data collected for rows 7400001 to 7450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.678328] No data to write for rows 7400001 to 7450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:48.948502] Processing rows 7450001 to 7500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.641464] Filtered data collected for rows 7450001 to 7500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.641776] No data to write for rows 7450001 to 7500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:51.918849] Processing rows 7500001 to 7550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:54.80162] Filtered data collected for rows 7500001 to 7550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:54.801847] No data to write for rows 7500001 to 7550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:55.061429] Processing rows 7550001 to 7600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:57.841741] Filtered data collected for rows 7550001 to 7600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:57.842019] No data to write for rows 7550001 to 7600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:42:58.106597] Processing rows 7600001 to 7650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:00.761695] Filtered data collected for rows 7600001 to 7650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:00.761913] No data to write for rows 7600001 to 7650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:01.022523] Processing rows 7650001 to 7700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.731331] Filtered data collected for rows 7650001 to 7700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.731605] No data to write for rows 7650001 to 7700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:03.997652] Processing rows 7700001 to 7750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.664577] Filtered data collected for rows 7700001 to 7750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.664802] No data to write for rows 7700001 to 7750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:06.926422] Processing rows 7750001 to 7800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.660168] Filtered data collected for rows 7750001 to 7800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.660447] No data to write for rows 7750001 to 7800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:09.922848] Processing rows 7800001 to 7850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.521521] Filtered data collected for rows 7800001 to 7850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.521738] No data to write for rows 7800001 to 7850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:12.82235] Processing rows 7850001 to 7900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.567262] Filtered data collected for rows 7850001 to 7900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.567483] No data to write for rows 7850001 to 7900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:15.828017] Processing rows 7900001 to 7950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.619345] Filtered data collected for rows 7900001 to 7950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.61957] No data to write for rows 7900001 to 7950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:18.885634] Processing rows 7950001 to 8000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.593661] Filtered data collected for rows 7950001 to 8000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.594] No data to write for rows 7950001 to 8000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:21.853186] Processing rows 8000001 to 8050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.475005] Filtered data collected for rows 8000001 to 8050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.475222] No data to write for rows 8000001 to 8050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:24.737007] Processing rows 8050001 to 8100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.473482] Filtered data collected for rows 8050001 to 8100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.473729] No data to write for rows 8050001 to 8100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:27.735858] Processing rows 8100001 to 8150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.239484] Filtered data collected for rows 8100001 to 8150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.239717] No data to write for rows 8100001 to 8150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:30.494137] Processing rows 8150001 to 8200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.105973] Filtered data collected for rows 8150001 to 8200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.106207] No data to write for rows 8150001 to 8200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:33.364896] Processing rows 8200001 to 8250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:35.870011] Filtered data collected for rows 8200001 to 8250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:35.87025] No data to write for rows 8200001 to 8250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:36.127617] Processing rows 8250001 to 8300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:38.738933] Filtered data collected for rows 8250001 to 8300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:38.739149] No data to write for rows 8250001 to 8300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:39.015261] Processing rows 8300001 to 8350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.069889] Filtered data collected for rows 8300001 to 8350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.070111] No data to write for rows 8300001 to 8350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:42.325792] Processing rows 8350001 to 8400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:44.844419] Filtered data collected for rows 8350001 to 8400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:44.8447] No data to write for rows 8350001 to 8400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:45.100586] Processing rows 8400001 to 8450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.613801] Filtered data collected for rows 8400001 to 8450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.614016] No data to write for rows 8400001 to 8450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:47.86877] Processing rows 8450001 to 8500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.415857] Filtered data collected for rows 8450001 to 8500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.416076] No data to write for rows 8450001 to 8500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:50.671575] Processing rows 8500001 to 8550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.177248] Filtered data collected for rows 8500001 to 8550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.177514] No data to write for rows 8500001 to 8550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:53.435972] Processing rows 8550001 to 8600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:55.937343] Filtered data collected for rows 8550001 to 8600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:55.937631] No data to write for rows 8550001 to 8600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:56.19927] Processing rows 8600001 to 8650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:58.910074] Filtered data collected for rows 8600001 to 8650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:58.910305] No data to write for rows 8600001 to 8650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:43:59.165329] Processing rows 8650001 to 8700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:01.743389] Filtered data collected for rows 8650001 to 8700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:01.744075] No data to write for rows 8650001 to 8700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:02.017748] Processing rows 8700001 to 8750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.63118] Filtered data collected for rows 8700001 to 8750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.631424] No data to write for rows 8700001 to 8750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:04.889258] Processing rows 8750001 to 8800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.404293] Filtered data collected for rows 8750001 to 8800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.404526] No data to write for rows 8750001 to 8800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:07.661279] Processing rows 8800001 to 8850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.110486] Filtered data collected for rows 8800001 to 8850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.1107] No data to write for rows 8800001 to 8850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:10.364493] Processing rows 8850001 to 8900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:12.903018] Filtered data collected for rows 8850001 to 8900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:12.903237] No data to write for rows 8850001 to 8900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:13.15864] Processing rows 8900001 to 8950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.600643] Filtered data collected for rows 8900001 to 8950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.600862] No data to write for rows 8900001 to 8950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:15.852644] Processing rows 8950001 to 9000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:18.835178] Filtered data collected for rows 8950001 to 9000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:18.83553] No data to write for rows 8950001 to 9000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:19.093573] Processing rows 9000001 to 9050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.512898] Filtered data collected for rows 9000001 to 9050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.513128] No data to write for rows 9000001 to 9050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:21.762887] Processing rows 9050001 to 9100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.049797] Filtered data collected for rows 9050001 to 9100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.050015] No data to write for rows 9050001 to 9100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:24.2987] Processing rows 9100001 to 9150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.585938] Filtered data collected for rows 9100001 to 9150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.58615] No data to write for rows 9100001 to 9150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:26.837427] Processing rows 9150001 to 9200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.093355] Filtered data collected for rows 9150001 to 9200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.093579] No data to write for rows 9150001 to 9200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:29.343654] Processing rows 9200001 to 9250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.673651] Filtered data collected for rows 9200001 to 9250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.673867] No data to write for rows 9200001 to 9250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:31.926033] Processing rows 9250001 to 9300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.217619] Filtered data collected for rows 9250001 to 9300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.217854] No data to write for rows 9250001 to 9300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 06:44:34.472224] Processing rows 9300001 to 9350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:25.638288] Filtered data collected for rows 9300001 to 9350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:25.638607] No data to write for rows 9300001 to 9350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:26.108764] Processing rows 9350001 to 9400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.355573] Filtered data collected for rows 9350001 to 9400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.355857] No data to write for rows 9350001 to 9400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:30.74703] Processing rows 9400001 to 9450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.311148] Filtered data collected for rows 9400001 to 9450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.31143] No data to write for rows 9400001 to 9450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:36.716459] Processing rows 9450001 to 9500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:41.888692] Filtered data collected for rows 9450001 to 9500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:41.889001] No data to write for rows 9450001 to 9500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:42.187112] Processing rows 9500001 to 9550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.418082] Filtered data collected for rows 9500001 to 9550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.418342] No data to write for rows 9500001 to 9550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:48.781777] Processing rows 9550001 to 9600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.179293] Filtered data collected for rows 9550001 to 9600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.179635] No data to write for rows 9550001 to 9600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:54.581625] Processing rows 9600001 to 9650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.494557] Filtered data collected for rows 9600001 to 9650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.494856] No data to write for rows 9600001 to 9650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:32:58.806106] Processing rows 9650001 to 9700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:02.958354] Filtered data collected for rows 9650001 to 9700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:02.95868] No data to write for rows 9650001 to 9700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:03.352408] Processing rows 9700001 to 9750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.134268] Filtered data collected for rows 9700001 to 9750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.134676] No data to write for rows 9700001 to 9750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:07.432275] Processing rows 9750001 to 9800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:10.718212] Filtered data collected for rows 9750001 to 9800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:10.718474] No data to write for rows 9750001 to 9800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:11.181314] Processing rows 9800001 to 9850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:14.79233] Filtered data collected for rows 9800001 to 9850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:14.792547] No data to write for rows 9800001 to 9850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:15.078969] Processing rows 9850001 to 9900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.56669] Filtered data collected for rows 9850001 to 9900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.566982] No data to write for rows 9850001 to 9900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:18.968327] Processing rows 9900001 to 9950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:21.986006] Filtered data collected for rows 9900001 to 9950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:21.986236] No data to write for rows 9900001 to 9950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:22.253652] Processing rows 9950001 to 10000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.214574] Filtered data collected for rows 9950001 to 10000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.214858] No data to write for rows 9950001 to 10000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:25.501288] Processing rows 10000001 to 10050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.292372] Filtered data collected for rows 10000001 to 10050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.292595] No data to write for rows 10000001 to 10050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:28.579205] Processing rows 10050001 to 10100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:31.795964] Filtered data collected for rows 10050001 to 10100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:31.796287] No data to write for rows 10050001 to 10100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:32.08214] Processing rows 10100001 to 10150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.308541] Filtered data collected for rows 10100001 to 10150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.309336] No data to write for rows 10100001 to 10150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:37.610509] Processing rows 10150001 to 10200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:41.729988] Filtered data collected for rows 10150001 to 10200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:41.731117] No data to write for rows 10150001 to 10200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:42.083177] Processing rows 10200001 to 10250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:45.653445] Filtered data collected for rows 10200001 to 10250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:45.655502] No data to write for rows 10200001 to 10250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:46.18825] Processing rows 10250001 to 10300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.018669] Filtered data collected for rows 10250001 to 10300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.018905] No data to write for rows 10250001 to 10300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:50.321174] Processing rows 10300001 to 10350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.720529] Filtered data collected for rows 10300001 to 10350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.720737] No data to write for rows 10300001 to 10350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:54.998903] Processing rows 10350001 to 10400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.259148] Filtered data collected for rows 10350001 to 10400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.259371] No data to write for rows 10350001 to 10400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:33:58.533853] Processing rows 10400001 to 10450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.45627] Filtered data collected for rows 10400001 to 10450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.456564] No data to write for rows 10400001 to 10450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:02.782147] Processing rows 10450001 to 10500000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.077958] Filtered data collected for rows 10450001 to 10500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.078202] No data to write for rows 10450001 to 10500000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:06.379858] Processing rows 10500001 to 10550000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.189278] Filtered data collected for rows 10500001 to 10550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.189499] No data to write for rows 10500001 to 10550000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:09.459211] Processing rows 10550001 to 10600000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:12.865001] Filtered data collected for rows 10550001 to 10600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:12.865273] No data to write for rows 10550001 to 10600000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:13.131536] Processing rows 10600001 to 10650000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.003379] Filtered data collected for rows 10600001 to 10650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.003594] No data to write for rows 10600001 to 10650000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:16.34437] Processing rows 10650001 to 10700000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.038427] Filtered data collected for rows 10650001 to 10700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.038765] No data to write for rows 10650001 to 10700000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:20.405102] Processing rows 10700001 to 10750000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.535904] Filtered data collected for rows 10700001 to 10750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.536271] No data to write for rows 10700001 to 10750000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:25.860672] Processing rows 10750001 to 10800000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.349281] Filtered data collected for rows 10750001 to 10800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.349517] No data to write for rows 10750001 to 10800000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:30.630375] Processing rows 10800001 to 10850000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:33.792253] Filtered data collected for rows 10800001 to 10850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:33.846302] No data to write for rows 10800001 to 10850000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:34.141972] Processing rows 10850001 to 10900000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:36.807315] Filtered data collected for rows 10850001 to 10900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:36.807541] No data to write for rows 10850001 to 10900000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:37.082464] Processing rows 10900001 to 10950000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.560642] Filtered data collected for rows 10900001 to 10950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.561178] No data to write for rows 10900001 to 10950000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:40.883208] Processing rows 10950001 to 11000000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.403303] Filtered data collected for rows 10950001 to 11000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.403534] No data to write for rows 10950001 to 11000000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:44.695449] Processing rows 11000001 to 11050000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.709253] Filtered data collected for rows 11000001 to 11050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.70955] No data to write for rows 11000001 to 11050000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:47.975699] Processing rows 11050001 to 11100000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:50.753806] Filtered data collected for rows 11050001 to 11100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:50.754165] No data to write for rows 11050001 to 11100000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:51.058473] Processing rows 11100001 to 11150000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.047721] Filtered data collected for rows 11100001 to 11150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.048001] No data to write for rows 11100001 to 11150000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:54.35854] Processing rows 11150001 to 11200000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.510817] Filtered data collected for rows 11150001 to 11200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.511082] No data to write for rows 11150001 to 11200000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:34:58.828075] Processing rows 11200001 to 11250000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:01.891811] Filtered data collected for rows 11200001 to 11250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:01.892053] No data to write for rows 11200001 to 11250000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:02.165716] Processing rows 11250001 to 11300000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.724306] Filtered data collected for rows 11250001 to 11300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.724526] No data to write for rows 11250001 to 11300000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:04.980146] Processing rows 11300001 to 11350000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.284282] Filtered data collected for rows 11300001 to 11350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.284544] No data to write for rows 11300001 to 11350000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:07.539788] Processing rows 11350001 to 11400000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:10.695716] Filtered data collected for rows 11350001 to 11400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:10.695963] No data to write for rows 11350001 to 11400000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:11.00426] Processing rows 11400001 to 11450000 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.588531] Filtered data collected for rows 11400001 to 11450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.588752] No data to write for rows 11400001 to 11450000 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:14.912262] Processing rows 11450001 to 11496362 for table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.404922] Filtered data collected for rows 11450001 to 11496362 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.405148] No data to write for rows 11450001 to 11496362 in table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:18.763311] All tables processed. #> [2024-10-07 07:35:18.900142] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv #> [2024-10-07 07:35:18.903189] The next function needed is `open_payments_collect_and_convert` #> $OP_DTL_GNRL_PGYR2020_P01182024 #> [1] \"Processed OP_DTL_GNRL_PGYR2020_P01182024\" #>  #> $OP_DTL_GNRL_PGYR2021_P01182024 #> [1] \"Processed OP_DTL_GNRL_PGYR2021_P01182024\" #>   # Example 2: Process multiple years of Open Payments data and filter by multiple OB-GYN specialties open_payments_specialty_cleaning(   con = DBI::dbConnect(duckdb::duckdb(), \"/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb\"),   table_names = c(\"OP_DTL_GNRL_PGYR2022_P01182024\", \"OP_DTL_GNRL_PGYR2023_P06282024_06122024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology\",                   \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine\"),   chunk_size = 100000 ) #> [2024-10-07 07:35:18.937755] Function inputs: #> [2024-10-07 07:35:18.938268] Tables: OP_DTL_GNRL_PGYR2022_P01182024, OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> [2024-10-07 07:35:18.938529] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv #> [2024-10-07 07:35:18.938699] Specialties: Allopathic & Osteopathic Physicians|Obstetrics & Gynecology, Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine #> Warning: cannot create dir '/Volumes/Video Projects Muffly 1', reason 'Permission denied' #> [2024-10-07 07:35:18.941362] Processing table: OP_DTL_GNRL_PGYR2022_P01182024 #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.054531] Error processing table: OP_DTL_GNRL_PGYR2022_P01182024 #> [2024-10-07 07:35:19.054712] Error message: cannot open the connection #> [2024-10-07 07:35:19.055706] Processing table: OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> Warning: restarting interrupted promise evaluation #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.107403] Error processing table: OP_DTL_GNRL_PGYR2023_P06282024_06122024 #> [2024-10-07 07:35:19.107644] Error message: cannot open the connection #> [2024-10-07 07:35:19.108283] All tables processed. #> [2024-10-07 07:35:19.225474] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv #> [2024-10-07 07:35:19.234962] The next function needed is `open_payments_collect_and_convert` #> list()  # Example 3: Process data with default connection and specialty filtering open_payments_specialty_cleaning(   table_names = c(\"OP_DTL_GNRL_PGYR2021_P01182024\"),   output_csv_path = \"/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv\",   specialties = c(\"Allopathic & Osteopathic Physicians|Gynecologic Oncology\",                   \"Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology\") ) #> [2024-10-07 07:35:19.237773] Function inputs: #> [2024-10-07 07:35:19.258656] Tables: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:19.259325] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv #> [2024-10-07 07:35:19.26026] Specialties: Allopathic & Osteopathic Physicians|Gynecologic Oncology, Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology #> Warning: cannot create dir '/Volumes/Video Projects Muffly 1', reason 'Permission denied' #> [2024-10-07 07:35:19.262182] Processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> Warning: cannot open file '/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb': No such file or directory #> [2024-10-07 07:35:19.29478] Error processing table: OP_DTL_GNRL_PGYR2021_P01182024 #> [2024-10-07 07:35:19.295025] Error message: cannot open the connection #> [2024-10-07 07:35:19.295195] All tables processed. #> [2024-10-07 07:35:19.388607] Output CSV Path: /Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv #> [2024-10-07 07:35:19.389133] The next function needed is `open_payments_collect_and_convert` #> list()"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":null,"dir":"Reference","previous_headings":"","what":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"phase0_city_state_assign_scenarios function designed assign cases professionals based specialty location (city state). function particularly useful managing scenarios professionals, physicians healthcare workers, need assigned cases administrative analytical purposes.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"","code":"phase0_city_state_assign_scenarios(   data,   generalist = \"General Dermatology\",   specialty = \"Pediatric Dermatology\",   case_names = c(\"Case Alpha\", \"Case Beta\", \"Case Gamma\"),   output_csv_path = \"Lizzy/data/city_state_assign_scenarios.csv\",   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. generalist character string specifying specialty name generalists. Default \"Generalist\". specialty character string specifying specialty name specialists. Default \"Specialist\". case_names character vector case names assign. Default c(\"Alpha\", \"Beta\", \"Gamma\"). output_csv_path character string specifying file path save output CSV. Default \"output/city_state_assign_scenarios.csv\". seed optional integer value set random seed reproducibility. Default NULL (seed set).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"data frame assigned cases.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"Generalists vs. Specialists: function differentiates generalists specialists, assigning cases accordingly. CSV Output: final output, including case assignments professional, saved CSV file using write_output_csv function.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"use-cases-","dir":"Reference","previous_headings":"","what":"Use Cases:","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"Healthcare Assignment: Assigning different types cases healthcare professionals based specialties cities/states practice. Research Studies: Managing scenarios research studies professionals need randomly assigned cases.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_assign_scenarios.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assign Cases to Professionals by City and State — phase0_city_state_assign_scenarios","text":"","code":"# Example 1: Using default parameters data <- data.frame(   city = c(\"CityA\", \"CityA\", \"CityB\", \"CityB\"),   state_code = c(\"State1\", \"State1\", \"State2\", \"State2\"),   specialty_primary = c(\"Generalist\", \"Specialist\", \"Generalist\", \"Specialist\"),   stringsAsFactors = FALSE ) result <- city_state_assign_scenarios(data) #> Error in city_state_assign_scenarios(data): could not find function \"city_state_assign_scenarios\" print(result) #> Error: object 'result' not found"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_check_specialty_generalist_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"function checks city-state combination required number generalists specialists. logs inputs, transformations, outputs, returns two data frames: one failing city-state-specialty combinations one successful combinations. Optionally, results can saved CSV files.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"","code":"phase0_city_state_check_specialty_generalist_counts(   data,   min_generalists,   min_specialists,   generalist_name = \"General Dermatology\",   specialist_name = \"Pediatric Dermatology\",   failing_csv_path = NULL,   successful_csv_path = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"data data frame containing professional information. Must include least columns city, state_code, specialty_primary. min_generalists integer specifying minimum number generalists required per city-state combination. min_specialists integer specifying minimum number specialists required per city-state combination. generalist_name string specifying specialty name generalists. Default \"General Dermatology\". specialist_name string specifying specialty name specialists. Default \"Pediatric Dermatology\". failing_csv_path optional string specifying file path save failing combinations CSV. Default NULL (file saved). successful_csv_path optional string specifying file path save successful combinations CSV. Default NULL (file saved).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"list containing two data frames: failing_combinations successful_combinations.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_check_specialty_generalist_counts.html","id":"key-features-","dir":"Reference","previous_headings":"","what":"Key Features:","title":"Check City-State Combinations for Minimum Generalists and Specialists with Logging and Summary — phase0_city_state_check_specialty_generalist_counts","text":"Generalists vs. Specialists: can specify names generalists specialists, function checks city-state combination required number . Logging: Extensive logging ensures inputs, transformations, results tracked. CSV Output: Optionally, function writes failing successful city-state-specialty combinations separate CSV files. Summary Logging: summary min_generalists, min_specialists, results logged end.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_sample_specialists.html","id":null,"dir":"Reference","previous_headings":"","what":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"function samples specialists generalists given dataset based city-state combinations. allows sampling three types specialists generalists customizable sample sizes . results can saved CSV file returned dataframe.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_sample_specialists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"","code":"phase0_city_state_sample_specialists(   data,   generalist = \"General Dermatology\",   specialist1 = \"Pediatric Dermatology\",   general_sample_size = 4,   specialist1_sample_size = 2,   specialist2 = NULL,   specialist2_sample_size = 0,   specialist3 = NULL,   specialist3_sample_size = 0,   same_phone_number = TRUE,   output_csv_path = NULL,   seed = 1978 )"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_sample_specialists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"data dataframe containing specialist information. must columns: city, state_code, specialty_primary, phone_number. generalist character string specifying generalist specialty sample. Default \"General Dermatology\". specialist1 character string specifying first specialist specialty sample. Default \"Pediatric Dermatology\". general_sample_size integer specifying many generalists sample city-state combination. Default 4. specialist1_sample_size integer specifying many first specialists sample city-state combination. Default 1. specialist2 character string specifying second specialist specialty sample. Optional. Default NULL. specialist2_sample_size integer specifying many second specialists sample. Default 0. specialist3 character string specifying third specialist specialty sample. Optional. Default NULL. specialist3_sample_size integer specifying many third specialists sample. Default 0. same_phone_number logical value indicating whether sample generalists specialists phone number (TRUE) different phone numbers (FALSE). Default TRUE. output_csv_path character string specifying path save output CSV. provided, result returned. seed integer setting seed reproducibility. Default 1978.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_sample_specialists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"dataframe containing sampled generalists specialists city-state combination.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_city_state_sample_specialists.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"#' Sample Generalists and Specialists by City-State Combination — phase0_city_state_sample_specialists","text":"","code":"# Example 1: Basic usage with default generalist and specialist data <- data.frame(   city = rep(c(\"New York\", \"Los Angeles\"), each = 6),   state_code = rep(c(\"NY\", \"CA\"), each = 6),   specialty_primary = c(     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",     \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\"   ),   phone_number = rep(c(\"123\", \"456\", \"789\"), 4) ) result <- city_state_sample_specialists(data) #> Error in city_state_sample_specialists(data): could not find function \"city_state_sample_specialists\" print(result) #> Error: object 'result' not found"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_create_sampled_by_region.html","id":null,"dir":"Reference","previous_headings":"","what":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"function processes taxonomy AAOS data, merging Census Bureau data create sampled dataset grouped region ortho spine. Outputs saved CSV.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_create_sampled_by_region.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"","code":"phase0_create_sampled_by_region(   taxonomy_and_aaos_tbl,   census_bureau_tbl,   output_csv_path = \"ortho_spine/phase_0/sampled_by_region_ortho_spine.csv\" )"},{"path":"https://mufflyt.github.io/tyler/reference/phase0_create_sampled_by_region.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"taxonomy_and_aaos_tbl tibble containing taxonomy AAOS data. census_bureau_tbl tibble containing Census Bureau data regional information. output_csv_path file path save sampled dataset. Defaults \"ortho_spine/phase_0/sampled_by_region_ortho_spine.csv\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_create_sampled_by_region.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"tibble sampled ortho spine records region.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/phase0_create_sampled_by_region.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Phase 0 Sample Creation by Region for Ortho Spine — phase0_create_sampled_by_region","text":"","code":"# Example 1: Basic usage with default output path sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl, census_bureau_tbl) #> INFO [2024-10-30 12:19:32] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             census_bureau_tbl), logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl), .topenv = `<env>`, namespace = \"tyler\"),         do.call(formatter, c(res$params, list(.logcall = substitute(.logcall),             .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl), .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 2: Custom output path sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl, census_bureau_tbl,                                                 output_csv_path = \"custom_path/sample.csv\") #> INFO [2024-10-30 12:19:32] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),         logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),             .topenv = `<env>`, namespace = \"tyler\"), do.call(formatter,             c(res$params, list(.logcall = substitute(.logcall),                 .topcall = substitute(.topcall), .topenv = .topenv))),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 census_bureau_tbl, output_csv_path = \"custom_path/sample.csv\"),             .topenv = `<env>`), withCallingHandlers(glue::glue(...,             .envir = .topenv), error = function(e) {            args <- paste0(capture.output(str(...)), collapse = \"\\n\")            stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                 args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                 \"\\n\\nPlease consider using another `log_formatter` or \",                 \"`skip_formatter` on strings with curly braces.\"))        }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces.  # Example 3: Using a subset of census_bureau_tbl for faster testing sampled_data <- phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                                                 dplyr::filter(census_bureau_tbl, Region == \"West\"),                                                 output_csv_path = \"test/sample.csv\") #> INFO [2024-10-30 12:19:32] Starting phase0_create_sampled_by_region. #> Error in (function (e) {    args <- paste0(capture.output(str(...)), collapse = \"\\n\")    stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",         args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),         \"\\n\\nPlease consider using another `log_formatter` or \",         \"`skip_formatter` on strings with curly braces.\"))})(structure(list(message = \"Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)}\",     trace = structure(list(call = list(base::tryCatch(base::withCallingHandlers({        NULL        base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE)        base::flush(base::stdout())        base::flush(base::stderr())        NULL        base::invisible()    }, error = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, interrupt = function(e) {        {            callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`            err <- callr_data$err            if (FALSE) {                base::assign(\".Traceback\", base::.traceback(4),                   envir = callr_data)                utils::dump.frames(\"__callr_dump__\")                base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                   envir = callr_data)                base::rm(\"__callr_dump__\", envir = .GlobalEnv)            }            e <- err$process_call(e)            e2 <- err$new_error(\"error in callr subprocess\")            class <- base::class            class(e2) <- base::c(\"callr_remote_error\", class(e2))            e2 <- err$add_trace_back(e2)            cut <- base::which(e2$trace$scope == \"global\")[1]            if (!base::is.na(cut)) {                e2$trace <- e2$trace[-(1:cut), ]            }            base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 \".error\"))        }    }, callr_message = function(e) {        base::try(base::signalCondition(e))    }), error = function(e) {        NULL        if (FALSE) {            base::try(base::stop(e))        }        else {            base::invisible()        }    }, interrupt = function(e) {        NULL        if (FALSE) {            e        }        else {            base::invisible()        }    }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(tryCatchList(expr,         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv,         handlers[[nh]]), doTryCatch(return(expr), name, parentenv,         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]),         tryCatchOne(expr, names, parentenv, handlers[[1L]]),         doTryCatch(return(expr), name, parentenv, handler), base::withCallingHandlers({            NULL            base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),                 base::list(envir = .GlobalEnv, quote = TRUE)),                 envir = .GlobalEnv, quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                 compress = FALSE)            base::flush(base::stdout())            base::flush(base::stderr())            NULL            base::invisible()        }, error = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, interrupt = function(e) {            {                callr_data <- base::as.environment(\"tools:callr\")$`__callr_data__`                err <- callr_data$err                if (FALSE) {                  base::assign(\".Traceback\", base::.traceback(4),                     envir = callr_data)                  utils::dump.frames(\"__callr_dump__\")                  base::assign(\".Last.dump\", .GlobalEnv$`__callr_dump__`,                     envir = callr_data)                  base::rm(\"__callr_dump__\", envir = .GlobalEnv)                }                e <- err$process_call(e)                e2 <- err$new_error(\"error in callr subprocess\")                class <- base::class                class(e2) <- base::c(\"callr_remote_error\", class(e2))                e2 <- err$add_trace_back(e2)                cut <- base::which(e2$trace$scope == \"global\")[1]                if (!base::is.na(cut)) {                  e2$trace <- e2$trace[-(1:cut), ]                }                base::saveRDS(base::list(\"error\", e2, e), file = base::paste0(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",                   \".error\"))            }        }, callr_message = function(e) {            base::try(base::signalCondition(e))        }), base::saveRDS(base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), file = \"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-res-5c893b54c4b3\",             compress = FALSE), base::do.call(base::do.call, base::c(base::readRDS(\"/var/folders/t6/jct906nn019fnrg7qr_b438w0000gn/T//RtmpDUgTex/callr-fun-5c891cd847d8\"),             base::list(envir = .GlobalEnv, quote = TRUE)), envir = .GlobalEnv,             quote = TRUE), `<fn>`(base::quote(`<fn>`), base::quote(`<named list>`),             envir = base::quote(`<env>`), quote = base::quote(TRUE)),         `<fn>`(pkg = base::quote(`<pkgdown>`), examples = base::quote(TRUE),             run_dont_run = base::quote(FALSE), seed = base::quote(1014L),             lazy = base::quote(FALSE), override = base::quote(`<list>`),             install = base::quote(FALSE), preview = base::quote(FALSE),             new_process = base::quote(FALSE), devel = base::quote(FALSE),             cli_colors = base::quote(256L), hyperlinks = base::quote(TRUE),             pkgdown_internet = base::quote(TRUE)), pkgdown::build_site(...),         build_site_local(pkg = pkg, examples = examples, run_dont_run = run_dont_run,             seed = seed, lazy = lazy, override = override, preview = preview,             devel = devel), build_reference(pkg, lazy = lazy,             examples = examples, run_dont_run = run_dont_run,             seed = seed, override = override, preview = FALSE,             devel = devel), unwrap_purrr_error(purrr::map(topics,             build_reference_topic, pkg = pkg, lazy = lazy, examples_env = examples_env,             run_dont_run = run_dont_run)), withCallingHandlers(code,             purrr_error_indexed = function(err) {                cnd_signal(err$parent)            }), purrr::map(topics, build_reference_topic, pkg = pkg,             lazy = lazy, examples_env = examples_env, run_dont_run = run_dont_run),         map_(\"list\", .x, .f, ..., .progress = .progress), with_indexed_errors(i = i,             names = names, error_call = .purrr_error_call, call_with_cleanup(map_impl,                 environment(), .type, .progress, n, names, i)),         withCallingHandlers(expr, error = function(cnd) {            if (i == 0L) {            }            else {                message <- c(i = \"In index: {i}.\")                if (!is.null(names) && !is.na(names[[i]]) &&                   names[[i]] != \"\") {                  name <- names[[i]]                  message <- c(message, i = \"With name: {name}.\")                }                else {                  name <- NULL                }                cli::cli_abort(message, location = i, name = name,                   parent = cnd, call = error_call, class = \"purrr_error_indexed\")            }        }), call_with_cleanup(map_impl, environment(), .type,             .progress, n, names, i), .f(.x[[i]], ...), withCallingHandlers(data_reference_topic(topic,             pkg, examples_env = examples_env, run_dont_run = run_dont_run),             error = function(err) {                cli::cli_abort(\"Failed to parse Rd in {.file {topic$file_in}}\",                   parent = err, call = quote(build_reference()))            }), data_reference_topic(topic, pkg, examples_env = examples_env,             run_dont_run = run_dont_run), run_examples(tags$tag_examples[[1]],             env = if (is.null(examples_env)) NULL else new.env(parent = examples_env),             topic = tools::file_path_sans_ext(topic$file_in),             run_dont_run = run_dont_run), highlight_examples(code,             topic, env = env), downlit::evaluate_and_highlight(code,             fig_save = fig_save_topic, env = eval_env, output_handler = handler),         evaluate::evaluate(code, child_env(env), new_device = TRUE,             output_handler = output_handler), withRestarts(with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE,             eval_error = function(cnd) {                signalCondition(cnd)                stop(cnd)            }), withRestartList(expr, restarts), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(withRestartList(expr,             restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr),             restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr,             restarts[[1L]]), doWithOneRestart(return(expr), restart),         with_handlers({            for (expr in tle$exprs) {                ev <- withVisible(eval(expr, envir))                watcher$capture_plot_and_output()                watcher$print_value(ev$value, ev$visible, envir)            }            TRUE        }, handlers), eval(call), eval(call), withCallingHandlers(code,             message = `<fn>`, warning = `<fn>`, error = `<fn>`),         withVisible(eval(expr, envir)), eval(expr, envir), eval(expr,             envir), phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,             dplyr::filter(census_bureau_tbl, Region == \"West\"),             output_csv_path = \"test/sample.csv\"), logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),         log_level(INFO, ..., namespace = namespace, .logcall = .logcall,             .topcall = .topcall, .topenv = .topenv), lapply(definitions,             function(definition) {                if (level > definition$threshold) {                  return(NULL)                }                log_fun <- do.call(logger, definition)                structure(do.call(log_fun, log_arg), class = \"logger\")            }), FUN(X[[i]], ...), structure(do.call(log_fun,             log_arg), class = \"logger\"), do.call(log_fun, log_arg),         `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             level = `<loglevel>`, .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 dplyr::filter(census_bureau_tbl, Region == \"West\"),                 output_csv_path = \"test/sample.csv\"), .topenv = `<env>`,             namespace = \"tyler\"), do.call(formatter, c(res$params,             list(.logcall = substitute(.logcall), .topcall = substitute(.topcall),                 .topenv = .topenv))), `<fn>`(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\",             .logcall = logger::log_info(\"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\"),             .topcall = phase0_create_sampled_by_region(taxonomy_and_aaos_tbl,                 dplyr::filter(census_bureau_tbl, Region == \"West\"),                 output_csv_path = \"test/sample.csv\"), .topenv = `<env>`),         withCallingHandlers(glue::glue(..., .envir = .topenv),             error = function(e) {                args <- paste0(capture.output(str(...)), collapse = \"\\n\")                stop(paste0(\"`glue` failed in `formatter_glue` on:\\n\\n\",                   args, \"\\n\\nRaw error message:\\n\\n\", conditionMessage(e),                   \"\\n\\nPlease consider using another `log_formatter` or \",                   \"`skip_formatter` on strings with curly braces.\"))            }), glue::glue(..., .envir = .topenv), glue_data(.x = NULL,             ..., .sep = .sep, .envir = .envir, .open = .open,             .close = .close, .na = .na, .null = .null, .comment = .comment,             .literal = .literal, .transformer = .transformer,             .trim = .trim), `<fn>`(\"nrow(taxonomy_and_aaos_tbl)\"),         .transformer(expr, env) %||% .null, .transformer(expr,             env), with_glue_error(eval(expr, envir), paste0(\"Failed to evaluate glue component {\",             text, \"}\")), withCallingHandlers(expr, error = function(cnd) {            rlang::abort(message, parent = cnd, call = NULL)        }), eval(expr, envir), eval(expr, envir), nrow(taxonomy_and_aaos_tbl),         .handleSimpleError(`<fn>`, \"object 'taxonomy_and_aaos_tbl' not found\",             base::quote(eval(expr, envir))), h(simpleError(msg,             call)), rlang::abort(message, parent = cnd, call = NULL)),         parent = c(0L, 1L, 2L, 3L, 2L, 5L, 6L, 0L, 0L, 0L, 0L,         0L, 12L, 13L, 14L, 15L, 16L, 15L, 18L, 19L, 20L, 19L,         19L, 23L, 23L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 32L,         31L, 34L, 35L, 34L, 37L, 38L, 29L, 40L, 41L, 40L, 29L,         29L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 51L, 51L, 54L,         54L, 56L, 56L, 58L, 0L, 60L, 60L, 62L, 63L, 62L, 65L,         47L, 0L, 68L, 69L), visible = c(TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,         FALSE, FALSE, FALSE), namespace = c(\"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", NA, \"pkgdown\", \"pkgdown\", \"pkgdown\", \"pkgdown\",         \"base\", \"purrr\", \"purrr\", \"purrr\", \"base\", \"purrr\", \"pkgdown\",         \"base\", \"pkgdown\", \"pkgdown\", \"pkgdown\", \"downlit\", \"evaluate\",         \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"evaluate\", \"base\", \"base\", \"base\",         \"base\", \"base\", \"base\", \"tyler\", \"logger\", \"logger\",         \"base\", \"logger\", \"base\", \"base\", \"logger\", \"base\", \"logger\",         \"base\", \"glue\", \"glue\", \"glue\", NA, \"glue\", \"glue\", \"base\",         \"base\", \"base\", \"base\", \"base\", \"glue\", \"rlang\"), scope = c(\"::\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"::\", \"::\", \"::\", \"local\", \"global\", \"::\", \":::\", \"::\",         \":::\", \"::\", \"::\", \":::\", \":::\", \"::\", \":::\", \"local\",         \"::\", \":::\", \":::\", \":::\", \"::\", \"::\", \"::\", \"local\",         \"local\", \"local\", \"local\", \"local\", \"local\", \"local\",         \"local\", \"local\", \":::\", \"::\", \"::\", \"::\", \"::\", \"::\",         \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\", \"::\", \"local\",         \"::\", \"local\", \"::\", \"::\", \"::\", \"local\", NA, \"local\",         \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"local\", \"::\")), row.names = c(NA,     -70L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\",     \"tbl\", \"data.frame\")), parent = structure(list(message = \"object 'taxonomy_and_aaos_tbl' not found\",         call = eval(expr, envir)), class = c(\"simpleError\", \"error\",     \"condition\")), rlang = list(inherit = TRUE), call = NULL), class = c(\"rlang_error\", \"error\", \"condition\"))): `glue` failed in `formatter_glue` on: #>  #>  chr \"Inputs provided: taxonomy_and_aaos_tbl with {nrow(taxonomy_and_aaos_tbl)} rows and {ncol(taxonomy_and_aaos_tbl)} columns.\" #>  #> Raw error message: #>  #> Failed to evaluate glue component {nrow(taxonomy_and_aaos_tbl)} #> Caused by error: #> ! object 'taxonomy_and_aaos_tbl' not found #>  #> Please consider using another `log_formatter` or `skip_formatter` on strings with curly braces."},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate and Summarize Physician Age — physician_age","title":"Calculate and Summarize Physician Age — physician_age","text":"function calculates median age, well 25th 75th percentiles (Interquartile Range, IQR) specified age column data frame. returns sentence summarizing statistics.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate and Summarize Physician Age — physician_age","text":"","code":"physician_age(df, age_column)"},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate and Summarize Physician Age — physician_age","text":"df data frame containing age data. age_column character string representing name column df contains age data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate and Summarize Physician Age — physician_age","text":"character string summarizing median age IQR specified age column dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate and Summarize Physician Age — physician_age","text":"function calculates median, 25th percentile (Q1), 75th percentile (Q3) age data, rounding results two decimal places median one decimal place percentiles. constructs summary sentence describing statistics.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/physician_age.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate and Summarize Physician Age — physician_age","text":"","code":"# Example 1: Basic usage with a small dataset df <- data.frame(age = c(30, 40, 50, 60, 35, 45, 55, 65)) summary_sentence <- physician_age(df, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 47.5 (IQR 25th percentile 38.8 to 75th percentile 56.2).\"  # Example 2: Handling missing data df_with_na <- data.frame(age = c(30, 40, NA, 60, 35, NA, 55, 65)) summary_sentence <- physician_age(df_with_na, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 47.5 (IQR 25th percentile 36.2 to 75th percentile 58.8).\"  # Example 3: Different age distribution df_large <- data.frame(age = c(rep(30, 70), rep(40, 30), rep(50, 20), rep(60, 10))) summary_sentence <- physician_age(df_large, \"age\") print(summary_sentence) #> [1] \"The median age of the dataset was 30 (IQR 25th percentile 30 to 75th percentile 40).\""},{"path":"https://mufflyt.github.io/tyler/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://mufflyt.github.io/tyler/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_and_save_emmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"function computes estimated marginal means (EMMs) model object, creates plot EMMs confidence intervals, saves plot specified directory. includes robust error handling, detailed logging, default behaviors work box.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_and_save_emmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"","code":"plot_and_save_emmeans(   model_object,   specs,   variable_of_interest,   color_by,   output_dir = \"Ari/Figures\",   y_min = NULL,   y_max = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/plot_and_save_emmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"model_object fitted model object EMMs computed. can generalized linear model (GLM), linear model, suitable models. specs character string formula specifying predictor variable(s) EMMs computed. example, treatment groups, scenarios, demographic variables. variable_of_interest character string specifying variable plotted x-axis. Typically, specs parameter. color_by character string specifying variable used color points error bars. categorical variable gender, insurance type, academic affiliation. output_dir character string specifying directory plot saved. Defaults \"Ari/Figures\". y_min Minimum value y-axis. Defaults NULL, means calculated automatically data. y_max Maximum value y-axis. Defaults NULL, means calculated automatically data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_and_save_emmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"list containing estimated marginal means data (data) ggplot object (plot).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_and_save_emmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot and Save Estimated Marginal Means (EMMs) with Error Handling — plot_and_save_emmeans","text":"function logs data transformation, inputs, outputs, output files saved. uses emmeans package compute EMMs creates plot using ggplot2. function ensures logging key steps assist debugging auditing.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_region_counts.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"function creates choropleth map physician counts region, options different regional groupings. merges state counts regional data, calculates centroids text labels, generates plot using ggplot2. Extensive logging provided console major step, including data transformations, plot generation, file saving.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_region_counts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"","code":"plot_region_counts(state_counts, region_df, region_col, save_path = NULL)"},{"path":"https://mufflyt.github.io/tyler/reference/plot_region_counts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"state_counts dataframe containing state-level physician counts. Must columns state_code total_available. region_df dataframe columns State region grouping column. row maps state region. region_col string specifying name column region_df used group states region. save_path optional string specifying file path save generated plot. NULL, plot displays onscreen .","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_region_counts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"ggplot object representing choropleth map.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/plot_region_counts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Regional Physician Counts by Specified Grouping — plot_region_counts","text":"","code":"# Example 1: Plot region counts for ENT_BOG regions, saving output to a file ent_bog_regions <- data.frame(   State = c(\"Alabama\", \"Alaska\", \"Arizona\"),   ENT_BOG_Region = c(\"Region 4\", \"Region 9\", \"Region 9\") ) state_counts <- data.frame(   state_code = c(\"alabama\", \"alaska\", \"arizona\"),   total_available = c(10, 15, 8) ) plot_region_counts(   state_counts = state_counts,   region_df = ent_bog_regions,   region_col = \"ENT_BOG_Region\",   save_path = \"ent_bog_region_map.png\" ) #> Error in plot_region_counts(state_counts = state_counts, region_df = ent_bog_regions,     region_col = \"ENT_BOG_Region\", save_path = \"ent_bog_region_map.png\"): could not find function \"plot_region_counts\"  # Example 2: Plot without saving, grouping by ACOG districts acog_districts_df <- data.frame(   State = c(\"California\", \"Nevada\", \"Utah\"),   ACOG_District = c(\"District IX\", \"District IX\", \"District VIII\") ) state_counts <- data.frame(   state_code = c(\"california\", \"nevada\", \"utah\"),   total_available = c(20, 12, 5) ) plot_region_counts(   state_counts = state_counts,   region_df = acog_districts_df,   region_col = \"ACOG_District\" ) #> Error in plot_region_counts(state_counts = state_counts, region_df = acog_districts_df,     region_col = \"ACOG_District\"): could not find function \"plot_region_counts\"  # Example 3: Grouping by custom region data, saving output to file custom_regions <- data.frame(   State = c(\"Texas\", \"New York\", \"Florida\"),   Custom_Region = c(\"South\", \"Northeast\", \"South\") ) state_counts <- data.frame(   state_code = c(\"texas\", \"new york\", \"florida\"),   total_available = c(25, 17, 13) ) plot_region_counts(   state_counts = state_counts,   region_df = custom_regions,   region_col = \"Custom_Region\",   save_path = \"custom_region_map.png\" ) #> Error in plot_region_counts(state_counts = state_counts, region_df = custom_regions,     region_col = \"Custom_Region\", save_path = \"custom_region_map.png\"): could not find function \"plot_region_counts\""},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"function reads dataset, fits Poisson regression model predict waiting times (business_days_until_appointment) based specified grouping variable, prints incidence rate ratios (IRR) along confidence intervals p-values. function returns fitted Poisson model.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"","code":"poisson_wait_time_stats(data_dir, file_name, group_var = \"insurance\")"},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"data_dir character string specifying directory data file located. file_name character string specifying name RDS file read. group_var character string specifying grouping variable use model (e.g., \"insurance\" \"scenario\").","code":""},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"fitted Poisson regression model object (glm).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"function fits Poisson regression model log link function. model estimates incidence rate ratios (IRRs) level specified grouping variable relative reference level (first level dataset). function prints IRRs, confidence intervals, p-values comparison. fitted model returned use.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/poisson_wait_time_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a Poisson Regression Model for Waiting Times by Group — poisson_wait_time_stats","text":"","code":"if (FALSE) { # \\dontrun{ # Example usage: poisson_model <- poisson_wait_time_stats(data_dir = \"your_data_directory\",                                          file_name = \"Phase_2.rds\",                                          group_var = \"scenario\") } # }"},{"path":"https://mufflyt.github.io/tyler/reference/prepare_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"function prepares dataset excluding specified columns predictor variables. logs process, including inputs outputs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/prepare_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"","code":"prepare_dataset(df, target_variable, excluded_columns)"},{"path":"https://mufflyt.github.io/tyler/reference/prepare_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"df data frame containing dataset. target_variable string representing name target variable. excluded_columns vector strings representing names columns exclude predictors.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/prepare_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare the Dataset by Excluding Certain Columns with Logging — prepare_dataset","text":"vector strings representing names predictor variables.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Process and Save Isochrones — process_and_save_isochrones","title":"Process and Save Isochrones — process_and_save_isochrones","text":"function takes input file locations, retrieves isochrones location, saves shapefiles. processes data chunks 25 rows time prevent data loss case errors.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process and Save Isochrones — process_and_save_isochrones","text":"","code":"process_and_save_isochrones(input_file, chunk_size = 25)"},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process and Save Isochrones — process_and_save_isochrones","text":"input_file data frame containing location data columns \"lat\" \"long.\" input file represent geographic coordinates isochrones calculated. chunk_size number rows process chunk. Default 25.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process and Save Isochrones — process_and_save_isochrones","text":"sf (simple features) data frame containing isochrone polygons.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process and Save Isochrones — process_and_save_isochrones","text":"function uses hereR package calculate isochrones based provided geographic coordinates. retrieves isochrones location input file, processes data chunks minimize risk data loss case errors, saves isochrones shapefiles analysis.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/process_and_save_isochrones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process and Save Isochrones — process_and_save_isochrones","text":"","code":"# Load the input file (e.g., from a CSV) input_file <- read_csv(\"data/locations.csv\") #> Error in read_csv(\"data/locations.csv\"): could not find function \"read_csv\"  # Process and save isochrones for the input file (chunk size set to 25) isochrones_data <- process_and_save_isochrones(input_file, chunk_size = 25) #> Error in process_and_save_isochrones(input_file, chunk_size = 25): could not find function \"process_and_save_isochrones\"  # Optionally, write the combined isochrones to a shapefile sf::st_write(isochrones_data, dsn = \"data/isochrones/isochrones_all_combined\",              layer = \"isochrones\", driver = \"ESRI Shapefile\", quiet = FALSE) #> Error: object 'isochrones_data' not found"},{"path":"https://mufflyt.github.io/tyler/reference/qualitycheck.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Quality Check Table — qualitycheck","title":"Save Quality Check Table — qualitycheck","text":"function takes data frame containing 'npi' 'name' columns creates quality check table. table includes count observations 'npi' 'name' combination count greater 2. resulting table saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/qualitycheck.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Quality Check Table — qualitycheck","text":"","code":"qualitycheck(df, filepath)"},{"path":"https://mufflyt.github.io/tyler/reference/qualitycheck.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Quality Check Table — qualitycheck","text":"df data frame containing columns 'npi' 'name'. filepath path CSV file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/qualitycheck.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save Quality Check Table — qualitycheck","text":"Prints message console indicating CSV file saved successfully.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/race_drive_time_generate_summary_sentence.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"function generates summary sentence indicating level access gynecologic oncologists specified race drive time. can run individual races races dataset.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/race_drive_time_generate_summary_sentence.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"","code":"race_drive_time_generate_summary_sentence(   tabulated_data,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" )"},{"path":"https://mufflyt.github.io/tyler/reference/race_drive_time_generate_summary_sentence.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"tabulated_data data frame containing data analyze. Must include columns Driving Time (minutes), Year, total_female_026, columns race proportions like White_prop, Black_prop, etc. driving_time_minutes numeric value specifying driving time minutes filter data. Default 180. race character string specifying race generate summary sentence. Supported values \"White\", \"Black\", \"American Indian/Alaska Native\", \"Asian\", \"Native Hawaiian Pacific Islander\", \"\" generate sentences supported races. Default \"American Indian/Alaska Native\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/race_drive_time_generate_summary_sentence.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"character string containing summary sentence, list summary sentences race = \"\".","code":""},{"path":"https://mufflyt.github.io/tyler/reference/race_drive_time_generate_summary_sentence.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Summary Sentence for Race and Drive Time — race_drive_time_generate_summary_sentence","text":"","code":"# Example usage summary_sentence <- race_drive_time_generate_summary_sentence(   tabulated_data = tabulated_all_years_numeric,   driving_time_minutes = 180,   race = \"American Indian/Alaska Native\" ) #> Function race_drive_time_generate_summary_sentence called with inputs: #> Driving Time (minutes): 180 #> Race: American Indian/Alaska Native #> Race column used for analysis: AIAN_prop #> Error: object 'tabulated_all_years_numeric' not found print(summary_sentence) #> Error: object 'summary_sentence' not found"},{"path":"https://mufflyt.github.io/tyler/reference/remove_constant_vars.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Constant Variables from a Data Frame — remove_constant_vars","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"function takes data frame returns new data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_constant_vars.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"","code":"remove_constant_vars(data_frame)"},{"path":"https://mufflyt.github.io/tyler/reference/remove_constant_vars.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"data_frame data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_constant_vars.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"data frame constant variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_constant_vars.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove Constant Variables from a Data Frame — remove_constant_vars","text":"","code":"if (FALSE) { # \\dontrun{ new_data <- remove_constant_vars(data_frame) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/remove_near_zero_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"function takes data frame returns new data frame near-zero variance variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_near_zero_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"","code":"remove_near_zero_var(data_frame, freqCut = 19, uniqueCut = 10)"},{"path":"https://mufflyt.github.io/tyler/reference/remove_near_zero_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"data_frame data frame near-zero variance variables removed. freqCut ratio common value second common value. Defaults 19. uniqueCut percentage distinct values number total samples. Defaults 10.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_near_zero_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"data frame near-zero variance variables removed.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/remove_near_zero_var.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Remove Near-Zero Variance Variables from a Data Frame — remove_near_zero_var","text":"","code":"if (FALSE) { # \\dontrun{ new_data <- remove_near_zero_var(data_frame) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/rename_columns_by_substring.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename columns based on substring matches — rename_columns_by_substring","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"function searches column names data frame specified substrings renames first matching column new name provided user. logs detailed information operation, including columns found renaming actions taken. multiple columns match substring, first renamed, warning issued.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/rename_columns_by_substring.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"","code":"rename_columns_by_substring(data, target_strings, new_names)"},{"path":"https://mufflyt.github.io/tyler/reference/rename_columns_by_substring.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"data data frame whose columns need renaming. target_strings vector substrings search within column names. new_names vector new names corresponding target strings.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/rename_columns_by_substring.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"data frame renamed columns.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/rename_columns_by_substring.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rename columns based on substring matches — rename_columns_by_substring","text":"","code":"# Example 1: Simple renaming of a single column df1 <- data.frame(   doctor_info = 1:5,   patient_contact_data = 6:10 ) df1 <- rename_columns_by_substring(df1,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician_info\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 1 columns matching 'doctor': doctor_info #>  #>  #> --- Column renaming complete. Updated column names:  physician_info, patient_contact_data --- print(df1)  # Should show 'physician_info' instead of 'doctor_info' #>   physician_info patient_contact_data #> 1              1                    6 #> 2              2                    7 #> 3              3                    8 #> 4              4                    9 #> 5              5                   10  # Example 2: Renaming multiple columns with a single target string df2 <- data.frame(   doctor_notes = 1:5,   doctor_info = 6:10,   patient_data = 11:15 ) df2 <- rename_columns_by_substring(df2,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 2 columns matching 'doctor': doctor_notes, doctor_info #> Warning: Multiple columns match 'doctor'. Only the first (doctor_notes) will be renamed to 'physician'. #>  #>  #> --- Column renaming complete. Updated column names:  physician, doctor_info, patient_data --- print(df2)  # Should show 'physician' instead of 'doctor_info' and 'doctor_notes' #>   physician doctor_info patient_data #> 1         1           6           11 #> 2         2           7           12 #> 3         3           8           13 #> 4         4           9           14 #> 5         5          10           15  # Example 3: Renaming with a warning for multiple matches df3 <- data.frame(   doctor_details = 1:5,   doctor_notes = 6:10,   doctor_data = 11:15 ) df3 <- rename_columns_by_substring(df3,                                     target_strings = c(\"doctor\"),                                     new_names = c(\"physician\")) #>  #> --- Starting to search and rename columns based on target substrings --- #> Found 3 columns matching 'doctor': doctor_details, doctor_notes, doctor_data #> Warning: Multiple columns match 'doctor'. Only the first (doctor_details) will be renamed to 'physician'. #>  #>  #> --- Column renaming complete. Updated column names:  physician, doctor_notes, doctor_data --- print(df3)  # Should show 'physician' as the first match and issue a warning #>   physician doctor_notes doctor_data #> 1         1            6          11 #> 2         2            7          12 #> 3         3            8          13 #> 4         4            9          14 #> 5         5           10          15"},{"path":"https://mufflyt.github.io/tyler/reference/results_section_medicaid_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"function generates summary sentence results section based significant predictor variables Medicaid acceptance rates. logs process, performs error checking, includes default behavior robust execution.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_medicaid_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"","code":"results_section_medicaid_summary(   significant_predictors,   medicaid_acceptance_rate,   accepted_medicaid_count,   total_medicaid_physicians_count )"},{"path":"https://mufflyt.github.io/tyler/reference/results_section_medicaid_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"significant_predictors data frame containing significant predictor variables, directions, formatted p-values. columns: \"Variable\", \"Direction\", \"Formatted_P_Value\". medicaid_acceptance_rate numeric value representing Medicaid acceptance rate (percentage). accepted_medicaid_count integer representing count physicians accepting Medicaid. total_medicaid_physicians_count integer representing total count physicians considered Medicaid.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_medicaid_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"character string representing summary sentence.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_medicaid_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Results Section: Medicaid Summary with Logging and Error Handling — results_section_medicaid_summary","text":"","code":"# Example 1: Basic usage with two significant predictors significant_vars <- data.frame(   Variable = c(\"Specialty\", \"Region\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.01\", \"0.02\") ) results_section_medicaid_summary(significant_vars, 45.6, 100, 200) #> Starting to create the summary sentence... #> Logging inputs... #> Input validated successfully. #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #>  Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 )  #> Step 2: Constructing the final summary sentence... #> Summary sentence constructed: #>  Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).  #> Final summary sentence constructed: #>  Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).  #> [1] \"Physicians who accepted Medicaid were Specialty positively associated (p = <0.01 ) and Region negatively associated (p = 0.02 ). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).\" # Output: Physicians who accepted Medicaid were Specialty positively associated (p = <0.01) and Region negatively associated (p = 0.02). The Medicaid acceptance rate was 45.6% (n = 100/N = 200).  # Example 2: Using larger physician counts and different acceptance rate significant_vars <- data.frame(   Variable = c(\"Experience\", \"Training Level\"),   Direction = c(\"positively associated\", \"negatively associated\"),   Formatted_P_Value = c(\"<0.05\", \"0.01\") ) results_section_medicaid_summary(significant_vars, 35.2, 500, 1200) #> Starting to create the summary sentence... #> Logging inputs... #> Input validated successfully. #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #>  Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 )  #> Step 2: Constructing the final summary sentence... #> Summary sentence constructed: #>  Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).  #> Final summary sentence constructed: #>  Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).  #> [1] \"Physicians who accepted Medicaid were Experience positively associated (p = <0.05 ) and Training Level negatively associated (p = 0.01 ). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).\" # Output: Physicians who accepted Medicaid were Experience positively associated (p = <0.05) and Training Level negatively associated (p = 0.01). The Medicaid acceptance rate was 35.2% (n = 500/N = 1,200).  # Example 3: Using a single significant predictor and a high acceptance rate significant_vars <- data.frame(   Variable = c(\"Age\"),   Direction = c(\"positively associated\"),   Formatted_P_Value = c(\"<0.001\") ) results_section_medicaid_summary(significant_vars, 60.0, 750, 1250) #> Starting to create the summary sentence... #> Logging inputs... #> Input validated successfully. #> Step 1: Constructing the sentence for significant predictors... #> Significant predictors sentence part constructed: #>  Age positively associated (p = <0.001 )  #> Step 2: Constructing the final summary sentence... #> Summary sentence constructed: #>  Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).  #> Final summary sentence constructed: #>  Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).  #> [1] \"Physicians who accepted Medicaid were Age positively associated (p = <0.001 ). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250).\" # Output: Physicians who accepted Medicaid were Age positively associated (p = <0.001). The Medicaid acceptance rate was 60.0% (n = 750/N = 1,250)."},{"path":"https://mufflyt.github.io/tyler/reference/results_section_summarize_common_provider_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"function calculates returns summary sentence describing common gender, specialty, training, academic affiliation provided dataset. filters missing values column determining common value, calculates proportion common value relative total non-missing values column.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_summarize_common_provider_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"","code":"results_section_summarize_common_provider_info(   provider_info,   gender_col,   specialty_col,   training_col,   academic_affiliation_col )"},{"path":"https://mufflyt.github.io/tyler/reference/results_section_summarize_common_provider_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"provider_info data frame containing provider information. gender_col Name column representing gender. specialty_col Name column representing specialty. training_col Name column representing training credentials. academic_affiliation_col Name column representing academic affiliation.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_summarize_common_provider_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"character string summarizing common gender, specialty, training, academic affiliation along respective proportions.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_summarize_common_provider_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize the Most Common Gender, Specialty, Training, and Academic Affiliation in Provider Data — results_section_summarize_common_provider_info","text":"","code":"# Example usage with specified columns provider_info <- data.frame(   gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"),   specialty = c(\"Cardiology\", \"Cardiology\", \"Neurology\", \"Cardiology\", \"Neurology\"),   Provider.Credential.Text = c(\"MD\", \"MD\", \"DO\", \"MD\", \"DO\"),   academic_affiliation = c(\"Yes\", \"No\", \"Yes\", \"No\", \"Yes\") ) summarize_common_provider_info(   provider_info,   gender_col = \"gender\",   specialty_col = \"specialty\",   training_col = \"Provider.Credential.Text\",   academic_affiliation_col = \"academic_affiliation\" ) #> Error in summarize_common_provider_info(provider_info, gender_col = \"gender\",     specialty_col = \"specialty\", training_col = \"Provider.Credential.Text\",     academic_affiliation_col = \"academic_affiliation\"): could not find function \"summarize_common_provider_info\""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_wait_time_median_stats.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"function calculates median wait time, 25th percentile (Q1), 75th percentile (Q3) business_days_until_appointment column overall grouped specified variable. logs step console includes error handling ensure robustness.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_wait_time_median_stats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"","code":"results_section_wait_time_median_stats(   appointment_data,   wait_time_col = \"business_days_until_appointment\",   group_var = NULL,   round_digits = 1 )"},{"path":"https://mufflyt.github.io/tyler/reference/results_section_wait_time_median_stats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"appointment_data data frame containing appointment wait time data. wait_time_col Character string; name column representing wait time business days. group_var Character string; name column group grouped statistics (e.g., \"Subspecialty\"). NULL, overall statistics calculated. round_digits Integer; number decimal places round Q1 Q3 values.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_wait_time_median_stats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"data frame calculated statistics summary sentences.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/results_section_wait_time_median_stats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Wait Time Statistics and Summary Sentences with Logging and Error Handling — results_section_wait_time_median_stats","text":"","code":"# Example 1: Basic usage with overall statistics only appointment_data <- data.frame(   business_days_until_appointment = c(5, 10, 15, 20, 25, 30, 35, 40),   Subspecialty = c(\"Pediatric Dermatology\", \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",                    \"Pediatric Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\") ) results_section_wait_time_median_stats(   appointment_data = appointment_data,   wait_time_col = \"business_days_until_appointment\",   round_digits = 1 ) #> Error in results_section_wait_time_median_stats(appointment_data = appointment_data,     wait_time_col = \"business_days_until_appointment\", round_digits = 1): could not find function \"results_section_wait_time_median_stats\" # Expected output: #   Group    median_wait_time  wait_time_q1  wait_time_q3 #  \"Overall\"    22.5              13.75        31.25 #  summary_sentence: \"The overall median wait time was 22.5 business days (IQR: 13.75–31.25).\"  # Example 2: Grouped statistics by Subspecialty with specific rounding results_section_wait_time_median_stats(   appointment_data = appointment_data,   wait_time_col = \"business_days_until_appointment\",   group_var = \"Subspecialty\",   round_digits = 1 ) #> Error in results_section_wait_time_median_stats(appointment_data = appointment_data,     wait_time_col = \"business_days_until_appointment\", group_var = \"Subspecialty\",     round_digits = 1): could not find function \"results_section_wait_time_median_stats\" # Expected output: #   Group                 median_wait_time  wait_time_q1  wait_time_q3 #  \"Overall\"                 22.5              13.75        31.25 #  \"Pediatric Dermatology\"    20.0              11.25        28.75 #  \"General Dermatology\"      25.0              17.5         32.5 #  summary_sentence for each group: \"For Pediatric Dermatology, the median wait time was 20 business days (IQR: 11.25–28.75).\"  # Example 3: Handling missing data and non-numeric columns gracefully appointment_data_missing <- data.frame(   business_days_until_appointment = c(5, NA, 15, 20, NA, 30, 35, 40),   Subspecialty = c(\"Pediatric Dermatology\", \"General Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\",                    \"Pediatric Dermatology\", \"General Dermatology\", \"Pediatric Dermatology\", \"General Dermatology\") ) results_section_wait_time_median_stats(   appointment_data = appointment_data_missing,   wait_time_col = \"business_days_until_appointment\",   group_var = \"Subspecialty\",   round_digits = 1 ) #> Error in results_section_wait_time_median_stats(appointment_data = appointment_data_missing,     wait_time_col = \"business_days_until_appointment\", group_var = \"Subspecialty\",     round_digits = 1): could not find function \"results_section_wait_time_median_stats\" # Expected output: #   Group                 median_wait_time  wait_time_q1  wait_time_q3 #  \"Overall\"                 22.5              15            32.5 #  \"Pediatric Dermatology\"    27.5              20            35 #  \"General Dermatology\"      25.0              15            30 #  summary_sentence for each group: \"For General Dermatology, the median wait time was 25 business days (IQR: 15–30).\""},{"path":"https://mufflyt.github.io/tyler/reference/retrieve_clinician_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Clinician Data — retrieve_clinician_data","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"Retrieves clinician data valid NPI dataframe CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/retrieve_clinician_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"","code":"retrieve_clinician_data(input_data)"},{"path":"https://mufflyt.github.io/tyler/reference/retrieve_clinician_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"input_data Either dataframe containing NPI numbers path CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/retrieve_clinician_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"tibble clinician data valid NPI.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/retrieve_clinician_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Clinician Data — retrieve_clinician_data","text":"","code":"sample_data <- tibble::tibble(npi = c(1689603763)) retrieve_clinician_data(sample_data) #> INFO [2024-10-30 12:19:37] Starting retrieve_clinician_data with input_data of class tbl_df. #> INFO [2024-10-30 12:19:37] Starting retrieve_clinician_data with input_data of class tbl. #> INFO [2024-10-30 12:19:37] Starting retrieve_clinician_data with input_data of class data.frame. #> INFO [2024-10-30 12:19:37] Input data provided as a dataframe with 1 rows. #> INFO [2024-10-30 12:19:37] Mapping clinician data retrieval over each NPI. #> INFO [2024-10-30 12:19:47] Retrieved clinician data for NPI: 1689603763 #> Error in tidyr::unnest_wider(., clinician_data): Can't duplicate names between the affected columns and the original #> data. #> ✖ These names are duplicated: #>   ℹ `npi`, from `clinician_data`. #> ℹ Use `names_sep` to disambiguate using the column name. #> ℹ Or use `names_repair` to specify a repair strategy."},{"path":"https://mufflyt.github.io/tyler/reference/save_quality_check_table.html","id":null,"dir":"Reference","previous_headings":"","what":"Save Quality Check Table — save_quality_check_table","title":"Save Quality Check Table — save_quality_check_table","text":"function takes data frame containing 'npi' 'name' columns creates quality check table. table includes count observations 'npi' 'name' combination count greater 2. resulting table saved CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/save_quality_check_table.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Save Quality Check Table — save_quality_check_table","text":"","code":"save_quality_check_table(df, filepath)"},{"path":"https://mufflyt.github.io/tyler/reference/save_quality_check_table.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Save Quality Check Table — save_quality_check_table","text":"df data frame containing columns 'npi' 'name'. filepath path CSV file saved.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/save_quality_check_table.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Save Quality Check Table — save_quality_check_table","text":"Prints message console indicating CSV file saved successfully.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape Physicians' Data — scrape_physicians_data","title":"Scrape Physicians' Data — scrape_physicians_data","text":"function scrapes data physicians within specified ID range, excluding wrong IDs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape Physicians' Data — scrape_physicians_data","text":"","code":"scrape_physicians_data(startID, endID)"},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape Physicians' Data — scrape_physicians_data","text":"startID starting ID scraping. endID ending ID scraping.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape Physicians' Data — scrape_physicians_data","text":"dataframe containing scraped physicians' data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape Physicians' Data — scrape_physicians_data","text":"","code":"# Call the function scrape_result <- scrape_physicians_data(startID = 9045999, endID = 9046000) #> Error in scrape_physicians_data(startID = 9045999, endID = 9046000): could not find function \"scrape_physicians_data\""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data_with_tor.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"function scrapes data physicians within specified ID range, excluding wrong IDs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data_with_tor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"","code":"scrape_physicians_data_with_tor(startID, endID, torPort)"},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data_with_tor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"startID starting ID scraping. endID ending ID scraping. torPort port number Tor SOCKS proxy.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data_with_tor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"dataframe containing scraped physicians' data.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/scrape_physicians_data_with_tor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape Physicians' Data with Tor — scrape_physicians_data_with_tor","text":"","code":"# Call the function scrape_result <- scrape_physicians_data_with_tor(startID = 9045999, endID = 9046000, torPort = 9150) #> Error in scrape_physicians_data_with_tor(startID = 9045999, endID = 9046000,     torPort = 9150): could not find function \"scrape_physicians_data_with_tor\""},{"path":"https://mufflyt.github.io/tyler/reference/search_and_process_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Search and Process NPI Numbers with Logging — search_and_process_npi","title":"Search and Process NPI Numbers with Logging — search_and_process_npi","text":"function searches NPI numbers based provided first last names, filters results taxonomy, writes chunked results CSV files. supports retrying failed searches ensures logging inputs, outputs, errors.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_and_process_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search and Process NPI Numbers with Logging — search_and_process_npi","text":"","code":"search_and_process_npi(   name_data,   npi_type = \"ind\",   max_results_limit = 5L,   valid_country = \"US\",   allowed_credentials = c(\"MD\", \"DO\"),   searchable_taxonomies = c(\"Allergy & Immunology\", \"Anesthesiology\", \"Dermatology\",     \"Family Medicine\", \"Internal Medicine\", \"Obstetrics & Gynecology\", \"Pediatrics\",     \"Psychiatry & Neurology\", \"Radiology\", \"Surgery\", \"Urology\"),   chunk_size = 10,   output_directory = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/search_and_process_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search and Process NPI Numbers with Logging — search_and_process_npi","text":"name_data data frame columns 'first' 'last' containing names search. npi_type enumeration type NPI search (e.g., \"ind\", \"org\", \"\"). Default \"ind\". max_results_limit maximum number search results request name pair. Default 5. valid_country Filter valid country, default \"US\". allowed_credentials character vector containing credentials filter NPI results. Default c(\"MD\", \"\"). searchable_taxonomies character vector taxonomies filter NPI results. Default set clinical taxonomies.  searchable list can found Medicare Provider Supplier Taxonomy Crosswalk: https://data.cms.gov/provider-characteristics/medicare-provider-supplier-enrollment/medicare-provider--supplier-taxonomy-crosswalk/data. chunk_size number results save per chunk. Default 10. output_directory Directory save chunked results. Default NULL (current working directory).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_and_process_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search and Process NPI Numbers with Logging — search_and_process_npi","text":"combined data frame NPI search results.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_batch_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch NPI Search Function — search_batch_npi","title":"Batch NPI Search Function — search_batch_npi","text":"function performs batch search first last names using NPI registry API returns flattened dataframe relevant results. Optionally, saves results CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_batch_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch NPI Search Function — search_batch_npi","text":"","code":"search_batch_npi(df, limit = 5, write_csv_path = NULL)"},{"path":"https://mufflyt.github.io/tyler/reference/search_batch_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch NPI Search Function — search_batch_npi","text":"df dataframe two columns: first (first names) last (last names). limit number results request query (default 5). write_csv_path character string specifying file path save results CSV file. NULL, CSV saved. (default NULL).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_batch_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch NPI Search Function — search_batch_npi","text":"flattened dataframe NPI results, including custom columns tracking first last names searched, option save results CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_batch_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Batch NPI Search Function — search_batch_npi","text":"","code":"# Example 1: Basic batch NPI search with two names df <- data.frame(   first = c(\"Tyler\", \"Matthew\"),   last = c(\"Muffly\", \"Muffly\") ) npi_results <- search_batch_npi(df, limit = 5) #> Querying NPI for: Tyler Muffly #> 5 records requested #> Requesting records 0-5... #> Querying NPI for: Matthew Muffly #> 5 records requested #> Requesting records 0-5...  # Example 2: Batch NPI search with different name combinations df <- data.frame(   first = c(\"John\", \"Jane\"),   last = c(\"Smith\", \"Doe\") ) npi_results <- search_batch_npi(df, limit = 10) #> Querying NPI for: John Smith #> 10 records requested #> Requesting records 0-10... #> Querying NPI for: Jane Doe #> 10 records requested #> Requesting records 0-10...  # Example 3: Batch NPI search with more names and a CSV output df <- data.frame(   first = c(\"Anna\", \"Tom\", \"Emily\"),   last = c(\"Brown\", \"White\", \"Black\") ) npi_results <- search_batch_npi(df, limit = 15, write_csv_path = \"npi_results.csv\") #> Querying NPI for: Anna Brown #> 15 records requested #> Requesting records 0-15... #> Querying NPI for: Tom White #> 15 records requested #> Requesting records 0-15... #> Querying NPI for: Emily Black #> 15 records requested #> Requesting records 0-15... #> Data saved to file: npi_results.csv"},{"path":"https://mufflyt.github.io/tyler/reference/search_by_taxonomy.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NPI by Taxonomy — search_by_taxonomy","title":"Search NPI by Taxonomy — search_by_taxonomy","text":"function performs search NPI registry list taxonomy descriptions returns combined dataframe results. processes returned data cleans unnecessary fields.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_by_taxonomy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NPI by Taxonomy — search_by_taxonomy","text":"","code":"search_by_taxonomy(taxonomy_to_search)"},{"path":"https://mufflyt.github.io/tyler/reference/search_by_taxonomy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NPI by Taxonomy — search_by_taxonomy","text":"taxonomy_to_search character vector taxonomy descriptions search NPI registry.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_by_taxonomy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NPI by Taxonomy — search_by_taxonomy","text":"dataframe cleaned NPI data, filtered taxonomy criteria.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NPI Numbers for Given Names — search_npi","title":"Search NPI Numbers for Given Names — search_npi","text":"function searches NPI (National Provider Identifier) numbers based given first last names. can accept input data form dataframe, CSV file, RDS file columns named 'first' 'last' representing first names last names, respectively.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NPI Numbers for Given Names — search_npi","text":"","code":"search_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/reference/search_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NPI Numbers for Given Names — search_npi","text":"input_data dataframe, file path CSV, RDS file containing first last names.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NPI Numbers for Given Names — search_npi","text":"dataframe containing NPI numbers provided names match specified taxonomies.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/search_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search NPI Numbers for Given Names — search_npi","text":"","code":"# Input as a dataframe input_df <- data.frame(   first = c(\"John\", \"Jane\", \"Alice\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") ) npi_results <- search_npi(input_df) #> Error in validate_wildcard_rules(x): x must be a character vector with length 1  # Input as a CSV file input_csv <- \"path/to/input.csv\" npi_results <- search_npi(input_csv) #> Error: 'path/to/input.csv' does not exist in current working directory ('/Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference')."},{"path":"https://mufflyt.github.io/tyler/reference/split_and_save.html","id":null,"dir":"Reference","previous_headings":"","what":"Split data into multiple parts and save each part as separate Excel files — split_and_save","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"function splits data based provided lab assistant names saves part separate Excel file. allows arrangement calls insurance type prioritize Medicaid first two days Blue Cross/Blue Shield last two days.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/split_and_save.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"","code":"split_and_save(   data_or_path,   output_directory,   lab_assistant_names,   seed = 1978,   complete_file_prefix = \"complete_non_split_version_\",   split_file_prefix = \"\",   recursive_create = TRUE,   insurance_order = c(\"Medicaid\", \"Blue Cross/Blue Shield\") )"},{"path":"https://mufflyt.github.io/tyler/reference/split_and_save.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"data_or_path Either dataframe containing input data path input data file (RDS, CSV, XLS/XLSX). output_directory Directory output Excel files saved. lab_assistant_names Vector lab assistant names name output files. seed Seed value randomization (default 1978). complete_file_prefix Prefix complete output file name (default \"complete_non_split_version_\"). split_file_prefix Prefix split output file name (default empty). recursive_create Logical indicating directories created recursively (default TRUE). insurance_order Vector insurance types ordered priority call scheduling (default c(\"Medicaid\", \"Blue Cross/Blue Shield\")).","code":""},{"path":"https://mufflyt.github.io/tyler/reference/split_and_save.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split data into multiple parts and save each part as separate Excel files — split_and_save","text":"","code":"if (FALSE) { # \\dontrun{ library(tyler) input_data <- readr::read_csv(\"/path/to/your/input/file.csv\") output_directory <- \"/path/to/your/output/directory\" lab_assistant_names <- c(\"Label1\", \"Label2\", \"Label3\") insurance_order <- c(\"Medicaid\", \"Blue Cross/Blue Shield\") split_and_save(data_or_path = input_data, output_directory, lab_assistant_names, insurance_order = insurance_order) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/split_calls_to_lab_assistants_and_save_by_priority.html","id":null,"dir":"Reference","previous_headings":"","what":"Split data into multiple parts and save each part as separate Excel files — split_calls_to_lab_assistants_and_save_by_priority","title":"Split data into multiple parts and save each part as separate Excel files — split_calls_to_lab_assistants_and_save_by_priority","text":"function splits data based specified column (e.g., insurance specialty), saves part separate Excel file. allows prioritize certain values column, ensures physician appears consecutive rows.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/split_calls_to_lab_assistants_and_save_by_priority.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split data into multiple parts and save each part as separate Excel files — split_calls_to_lab_assistants_and_save_by_priority","text":"","code":"split_calls_to_lab_assistants_and_save_by_priority(   input_data_or_path,   output_folder = getwd(),   assistant_names,   seed_value = 1978,   complete_file_prefix = \"complete_version_\",   split_file_prefix = \"\",   recursive_create = TRUE,   split_column = \"insurance\",   priority_values = NULL )"},{"path":"https://mufflyt.github.io/tyler/reference/split_calls_to_lab_assistants_and_save_by_priority.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split data into multiple parts and save each part as separate Excel files — split_calls_to_lab_assistants_and_save_by_priority","text":"input_data_or_path Either dataframe containing input data path input data file (RDS, CSV, XLS/XLSX). output_folder Directory output Excel files saved. Default current working directory. assistant_names Vector lab assistant names name output files. seed_value Seed value randomization (default 1978). complete_file_prefix Prefix complete output file name (default \"complete_version_\"). split_file_prefix Prefix split output file name (default empty). recursive_create Logical indicating directories created recursively (default TRUE). split_column column base split (e.g., \"insurance\", \"specialty_primary\"). Default \"insurance\". priority_values Vector values prioritize within split_column (e.g., c(\"Pediatric Dermatologists\", \"Medicaid\")). Default NULL.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/split_calls_to_lab_assistants_and_save_by_priority.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split data into multiple parts and save each part as separate Excel files — split_calls_to_lab_assistants_and_save_by_priority","text":"","code":"if (FALSE) { # \\dontrun{ split_calls_to_lab_assistants_and_save_by_priority(input_data_or_path = \"/path/to/file.csv\",                                                   output_folder = \"/path/to/output/\",                                                   assistant_names = c(\"Assistant1\", \"Assistant2\"),                                                   split_column = \"specialty_primary\",                                                   priority_values = c(\"Pediatric Dermatologists\")) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/states_where_physicians_were_NOT_contacted.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"function summarizes demographic details identifying states physicians successfully contacted included.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/states_where_physicians_were_NOT_contacted.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"","code":"states_where_physicians_were_NOT_contacted(filtered_data, all_states = NULL)"},{"path":"https://mufflyt.github.io/tyler/reference/states_where_physicians_were_NOT_contacted.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"filtered_data data frame containing filtered data contacted physicians. all_states character vector possible states including Washington, DC. provided, default set states used.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/states_where_physicians_were_NOT_contacted.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"character string summarizing inclusion exclusion states.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/states_where_physicians_were_NOT_contacted.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize States Where Physicians Were NOT Contacted — states_where_physicians_were_NOT_contacted","text":"","code":"# Example with provided all_states filtered_data <- data.frame(state = c(\"California\", \"New York\", \"Texas\")) all_states <- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\",                  \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\",                  \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\",                  \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\",                  \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\",                  \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\",                  \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\",                  \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",                  \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\",                  \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\",                  \"District of Columbia\") states_where_physicians_were_NOT_contacted(filtered_data, all_states) #> [1] \"Physicians were successfully contacted in 3 states including the District of Columbia. The excluded states include Alabama, Alaska, Arizona, Arkansas, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming and District of Columbia.\"  # Example with default all_states filtered_data <- data.frame(state = c(\"California\", \"New York\", \"Texas\", \"Nevada\")) states_where_physicians_were_NOT_contacted(filtered_data) #> [1] \"Physicians were successfully contacted in 4 states including the District of Columbia. The excluded states include Alabama, Alaska, Arizona, Arkansas, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming and District of Columbia.\""},{"path":"https://mufflyt.github.io/tyler/reference/taxonomy.html","id":null,"dir":"Reference","previous_headings":"","what":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","title":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","text":"dataset contains taxonomy codes Obstetricians Gynecologists among healthcare providers.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/taxonomy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","text":"","code":"taxonomy"},{"path":"https://mufflyt.github.io/tyler/reference/taxonomy.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","text":"data frame two columns: NUCC Code NUCC (National Uniform Claim Committee) code healthcare providers. Provider Type type healthcare provider corresponding NUCC code.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/taxonomy.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","text":"https://www.nucc.org/images/stories/PDF/taxonomy_23_0.pdf","code":""},{"path":"https://mufflyt.github.io/tyler/reference/taxonomy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Taxonomy Codes for Obstetricians and Gynecologists — taxonomy","text":"","code":"if (FALSE) { # \\dontrun{ # Load the taxonomy dataset data(taxonomy)  # Explore the dataset head(taxonomy) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":null,"dir":"Reference","previous_headings":"","what":"Test and Process Isochrones — test_and_process_isochrones","title":"Test and Process Isochrones — test_and_process_isochrones","text":"function tests processes isochrones location input file. identifies reports errors encountered isochrone retrieval process.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test and Process Isochrones — test_and_process_isochrones","text":"","code":"test_and_process_isochrones(input_file)"},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test and Process Isochrones — test_and_process_isochrones","text":"input_file data frame containing location data columns \"lat\" \"long.\" input file represent geographic coordinates isochrones calculated.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test and Process Isochrones — test_and_process_isochrones","text":"Prints messages indicating errors, , isochrone retrieval.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test and Process Isochrones — test_and_process_isochrones","text":"function uses hereR package calculate isochrones based provided geographic coordinates. retrieves isochrones location input file, identifies errors retrieval process, reports errors. function designed used input data meets specific requirements, including valid latitude longitude values.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/test_and_process_isochrones.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test and Process Isochrones — test_and_process_isochrones","text":"","code":"# Validate the file of geocoded data. input_file <- readr::read_csv(\"data/isochrones/inner_join_postmastr_clinician_data.csv\") %>%   dplyr::mutate(id = dplyr::row_number()) %>%   dplyr::filter(postmastr.name.x != \"Hye In Park, MD\") #> Error: 'data/isochrones/inner_join_postmastr_clinician_data.csv' does not exist in current working directory ('/Users/tylermuffly/Dropbox (Personal)/tyler/docs/reference').  test_and_process_isochrones(input_file = input_file) #> Error in test_and_process_isochrones(input_file = input_file): could not find function \"test_and_process_isochrones\"  # Filter out the rows that are going to error out after using the test_and_process_isochrones function. # error_rows <- c(265, 431, 816, 922, 1605, 2049, 2212, 2284, 2308, 2409, 2482, 2735, 2875, 2880, 3150, 3552, 3718) # input_file_no_error_rows <- input_file %>% #   dplyr::filter(!id %in% error_rows)"},{"path":"https://mufflyt.github.io/tyler/reference/tm_write2pdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","title":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","text":"function generates overall table summarizing demographics Table 1 data. logs key steps, including inputs, outputs, data transformations, file paths. function supports RDS, CSV, XLS file formats creates overall summary table saved PDF file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/tm_write2pdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","text":"","code":"tm_write2pdf(object, filename)"},{"path":"https://mufflyt.github.io/tyler/reference/tm_write2pdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","text":"input_file_path string representing path data file (RDS, CSV, XLS format). output_directory string representing directory output table file saved. title string specifying title overall table summary (default \"Overall Table Summary\"). selected_columns optional vector selected columns include table. Default NULL. label_translations optional named list label translations use table summary.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/tm_write2pdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","text":"Nothing returned. function saves output table PDF file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/tm_write2pdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Overall Table with Error Handling and Logging — tm_write2pdf","text":"","code":"if (FALSE) { # \\dontrun{ # Example 1: Generate the overall table with default settings generate_overall_table(\"data/Table1.rds\", \"output_tables\")  # Example 2: Generate the overall table with specific selected columns generate_overall_table(\"data/Table1.csv\", \"output_tables\", selected_columns = c(\"age\", \"gender\"))  # Example 3: Generate the overall table with label translations and title customization label_translations <- list(age = \"Age (years)\", gender = \"Gender\") generate_overall_table(\"data/Table1.xlsx\", \"output_tables\", title = \"Demographic Summary\",                        label_translations = label_translations)  # Example 4: Generate the table from a CSV file with all columns used generate_overall_table(\"data/Table1.csv\", \"output_tables\")  # Example 5: Save the table to a custom directory with a specific title and selected columns generate_overall_table(\"data/Table1.rds\", \"custom_output\", title = \"Custom Summary\",                        selected_columns = c(\"age\", \"income\")) } # }"},{"path":"https://mufflyt.github.io/tyler/reference/tyler-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care — tyler-package","title":"tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care — tyler-package","text":"'tyler' package provides collection functions designed facilitate mystery caller studies, often used evaluating patient access healthcare. includes tools searching processing National Provider Identifier (NPI) numbers based names analyzing demographic data associated NPIs. package simplifies handling NPI data creation informative tables analysis reporting.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/validate_and_remove_invalid_npi.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"function reads CSV file containing NPI numbers, validates format using npi package, removes rows missing invalid NPIs. function reads CSV file containing NPI numbers, validates format using npi package, removes rows missing invalid NPIs.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/validate_and_remove_invalid_npi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"","code":"validate_and_remove_invalid_npi(input_data)  validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/reference/validate_and_remove_invalid_npi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"input_data Either dataframe containing NPI numbers path CSV file.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/validate_and_remove_invalid_npi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"dataframe containing valid NPI numbers. dataframe containing valid NPI numbers.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/validate_and_remove_invalid_npi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate and Remove Invalid NPI Numbers — validate_and_remove_invalid_npi","text":"","code":"# Example usage: # input_data <- \"~/path/to/your/NPI/file.csv\" # valid_df <- validate_and_remove_invalid_npi(input_data)"},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":null,"dir":"Reference","previous_headings":"","what":"Search NPI Numbers for Given Names — vc","title":"Search NPI Numbers for Given Names — vc","text":"function searches NPI (National Provider Identifier) numbers based given first last names. can accept input data form dataframe, CSV file, RDS file columns named 'first' 'last' representing first names last names, respectively.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search NPI Numbers for Given Names — vc","text":"","code":"vc"},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Search NPI Numbers for Given Names — vc","text":"object class character length 97.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search NPI Numbers for Given Names — vc","text":"input_data dataframe, file path CSV, RDS file containing first last names.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search NPI Numbers for Given Names — vc","text":"dataframe containing NPI numbers provided names match specified taxonomies.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/vc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search NPI Numbers for Given Names — vc","text":"","code":"# Input as a dataframe input_df <- data.frame(   first = c(\"John\", \"Jane\", \"Alice\"),   last = c(\"Doe\", \"Smith\", \"Johnson\") ) npi_results <- search_npi(input_df) #> Error in search_npi(input_df): could not find function \"search_npi\"  # Input as a CSV file input_csv <- \"path/to/input.csv\" npi_results <- search_npi(input_csv) #> Error in search_npi(input_csv): could not find function \"search_npi\""},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"function writes data frame CSV file specified output directory. includes detailed logging inform user function's progress potential issues.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"","code":"write_output_csv(   df,   filename,   output_dir = \"ortho_sports_med/Figures\",   verbose = TRUE )"},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"df data frame written CSV file. filename string specifying name output file (.csv extension). output_dir string specifying directory CSV file saved. Default \"ortho_sports_med/Figures\". verbose boolean indicating whether print detailed logs. Default TRUE.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"NULL. function saves CSV file specified location.","code":""},{"path":[]},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":"key-features-of-the-function-","dir":"Reference","previous_headings":"","what":"Key Features of the Function:","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"Directory Creation: specified output directory exist, function attempts create . Logging: Verbose logging informs user progress potential errors writing process. Error Handling: function handles errors invalid input types directory creation failures.","code":""},{"path":"https://mufflyt.github.io/tyler/reference/write_output_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write a Data Frame to CSV with Robust Logging and Error Handling — write_output_csv","text":"","code":"# Example 1: Save a data frame to the default directory with detailed logging df <- data.frame(Name = c(\"John\", \"Jane\"), Age = c(30, 25)) write_output_csv(df, \"output.csv\") #> File successfully saved to: ortho_sports_med/Figures/output.csv   # Example 2: Save a data frame to a custom directory without logging df <- data.frame(Product = c(\"Apple\", \"Banana\"), Price = c(1.2, 0.5)) write_output_csv(df, \"output.csv\", output_dir = \"custom/directory\", verbose = FALSE)  # Example 3: Save a large data frame with detailed logging df_large <- data.frame(ID = 1:1000, Value = rnorm(1000)) write_output_csv(df_large, \"large_output.csv\", output_dir = \"data/outputs\", verbose = TRUE) #> File successfully saved to: data/outputs/large_output.csv"},{"path":"https://mufflyt.github.io/tyler/news/index.html","id":"tyler-0009000","dir":"Changelog","previous_headings":"","what":"tyler 0.0.0.9000","title":"tyler 0.0.0.9000","text":"Added NEWS.md file track changes package.","code":""}]
