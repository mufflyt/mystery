\nonstopmode{}
\documentclass[a4paper]{book}
\usepackage[times,inconsolata,hyper]{Rd}
\usepackage{makeidx}
\makeatletter\@ifl@t@r\fmtversion{2018/04/01}{}{\usepackage[utf8]{inputenc}}\makeatother
% \usepackage{graphicx} % @USE GRAPHICX@
\makeindex{}
\begin{document}
\chapter*{}
\begin{center}
{\textbf{\huge Package `tyler'}}
\par\bigskip{\large \today}
\end{center}
\ifthenelse{\boolean{Rd@use@hyper}}{\hypersetup{pdftitle = {tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care}}}{}
\begin{description}
\raggedright{}
\item[Title]\AsIs{Common Functions for Mystery Caller or Audit Studies Evaluating
Patient Access to Care}
\item[Version]\AsIs{1.2.0}
\item[Author]\AsIs{Tyler Muffly }\email{tyler.muffly@dhha.org}\AsIs{}
\item[Description]\AsIs{The 'tyler' package provides a collection of functions designed to facilitate mystery caller studies, often used in evaluating patient access to healthcare. It includes tools for searching and processing National Provider Identifier (NPI) numbers based on names and analyzing demographic data associated with these NPIs. The package simplifies the handling of NPI data and the creation of informative tables for analysis and reporting.}
\item[Encoding]\AsIs{UTF-8}
\item[Roxygen]\AsIs{list(markdown = TRUE)}
\item[RoxygenNote]\AsIs{7.3.2}
\item[License]\AsIs{MIT + file LICENSE}
\item[URL]\AsIs{}\url{https://mufflyt.github.io/tyler/}\AsIs{}
\item[Date]\AsIs{2023-09-11}
\item[Keywords]\AsIs{healthcare, patient access, audit study, NPI, mystery caller}
\item[Maintainer]\AsIs{Tyler Muffly }\email{tyler.muffly@dhha.org}\AsIs{}
\item[Depends]\AsIs{R (>= 3.5.0)}
\item[Imports]\AsIs{dplyr, DBI, duckdb, easyr, glue, janitor, lifecycle,
lubridate, mockery, npi, progress, purrr, stringr, tibble,
tidyr, logger, arsenal, beepr, censusapi, caret, devtools,
effects, exploratory, emmeans, fs, gender, genderdata, ggmap,
ggplot2, ggspatial, here, hereR, htmlwidgets, imager,
gridExtra, knitr, leaflet, leaflet.extras, memoise, openxlsx,
provider, rainbow, rnaturalearth, remotes, rmarkdown, scales,
sf, stats, testthat, tigris, zipcodeR, viridis, broom,
conflicted, data.table, httr, humaniformat, jsonlite, lme4,
magrittr, readr, readxl, rlang, webshot}
\item[Remotes]\AsIs{andrewallenbruce/provider, lmullen/genderdata}
\item[LazyData]\AsIs{true}
\item[Files]\AsIs{R/}
\item[VignetteBuilder]\AsIs{knitr}
\item[VignetteEngine]\AsIs{knitr::rmarkdown}
\item[Config/testthat/edition]\AsIs{3}
\end{description}
\Rdcontents{Contents}
\HeaderA{tyler-package}{tyler: Common Functions for Mystery Caller or Audit Studies Evaluating Patient Access to Care}{tyler.Rdash.package}
\aliasA{tyler}{tyler-package}{tyler}
\keyword{NPI}{tyler-package}
\keyword{NPPES}{tyler-package}
\keyword{healthcare}{tyler-package}
%
\begin{Description}
The 'tyler' package provides a collection of functions designed to facilitate mystery caller studies, often used in evaluating patient access to healthcare. It includes tools for searching and processing National Provider Identifier (NPI) numbers based on names and analyzing demographic data associated with these NPIs. The package simplifies the handling of NPI data and the creation of informative tables for analysis and reporting.
\end{Description}
%
\begin{Author}
\strong{Maintainer}: Tyler Muffly \email{tyler.muffly@dhha.org} (ORCID: 0000-0002-2044-1693)

\end{Author}
%
\begin{SeeAlso}
Useful links:
\begin{itemize}

\item{} \url{https://your.package.url}
\item{} \url{https://mufflyt.github.io/tyler/}

\end{itemize}


\end{SeeAlso}
\HeaderA{acgme}{ACGME OBGYN Residency Data}{acgme}
\keyword{dataset}{acgme}
%
\begin{Description}
This dataset provides information about Obstetricians and Gynecologists (OBGYN) residency programs accredited by the Accreditation Council for Graduate Medical Education (ACGME).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
acgme
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with the following columns:
\begin{description}

\item[Program directors] The names of program directors for OBGYN residency programs.
\item[name] The names of OBGYN residency programs.

\end{description}

\end{Format}
%
\begin{Source}
Data was obtained from the ACGME website: \url{https://apps.acgme.org/ads/Public/Programs/Search}
\end{Source}
%
\begin{Examples}
\begin{ExampleCode}
# Load the ACGME OBGYN Residency Data
data(acgme)

# View the first few rows of the dataset
head(acgme)

# Get a summary of the dataset
summary(acgme)

# Perform data analysis and exploration

\end{ExampleCode}
\end{Examples}
\HeaderA{ACOG\_Districts}{ACOG Districts Data}{ACOG.Rul.Districts}
\keyword{dataset}{ACOG\_Districts}
%
\begin{Description}
This dataset contains information about American College of Obstetricians and Gynecologists (ACOG) districts, including their two-letter state abbreviations and full state names.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
ACOG_Districts
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with the following columns:
\begin{description}

\item[ACOG\_District] Two-letter state abbreviations representing ACOG districts.
\item[name] Full names of the states corresponding to the ACOG districts.

\end{description}

\end{Format}
%
\begin{Source}
Data was obtained from the official ACOG website: \url{https://www.acog.org/community/districts-and-sections}
\end{Source}
%
\begin{Examples}
\begin{ExampleCode}
# Load the ACOG Districts Data
data(ACOG_Districts)

# View the first few rows of the dataset
head(ACOG_Districts)

# Get a summary of the dataset
summary(ACOG_Districts)

# Perform data analysis and exploration

\end{ExampleCode}
\end{Examples}
\HeaderA{arsenal\_tables\_write2word}{Writes an Arsenal table object to a Word document.}{arsenal.Rul.tables.Rul.write2word}
%
\begin{Description}
Writes an Arsenal table object to a Word document.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
arsenal_tables_write2word(object, filename)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{object}] An object to be written to Word, typically an Arsenal table.

\item[\code{filename}] The filename (without extension) for the Word document.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
arsenal_tables_write2word(my_table, "output_table")

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{calcpercentages}{Calculate the Percentage of the Most Common Value in a Categorical Variable}{calcpercentages}
%
\begin{Description}
This function calculates the percentage of the most common value in a specified categorical variable from a data frame.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
calcpercentages(df, variable)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the categorical variable.

\item[\code{variable}] A character string representing the name of the categorical variable within \code{df}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function converts the specified variable to character type if it's a factor, then counts the occurrences of each unique value. It identifies the most common value and returns the count and percentage.
\end{Details}
%
\begin{Value}
A data frame containing the most common value and its count, along with the percentage of the total count that it represents.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with a simple dataset
df <- data.frame(category = c("A", "B", "A", "C", "A", "B", "B", "A"))
result <- calcpercentages(df, "category")
print(result)

# Example 2: Using a dataset with multiple most common values
df_tie <- data.frame(category = c("A", "B", "A", "B", "C", "C", "C", "A", "B"))
result <- calcpercentages(df_tie, "category")
print(result)

# Example 3: Handling a dataset with missing values
df_na <- data.frame(category = c("A", NA, "A", "C", "A", "B", "B", NA))
result <- calcpercentages(df_na, "category")
print(result)

\end{ExampleCode}
\end{Examples}
\HeaderA{calculate\_descriptive\_stats}{Calculate Descriptive Statistics with Robust Logging}{calculate.Rul.descriptive.Rul.stats}
%
\begin{Description}
This function calculates the median, 25th percentile (Q1), and 75th percentile (Q3) for a specified column in a dataframe. The function includes detailed logging of inputs, outputs, and each data transformation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
calculate_descriptive_stats(df, column, verbose = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A dataframe containing the data.

\item[\code{column}] A string representing the column name for which to calculate the descriptive statistics.

\item[\code{verbose}] A boolean indicating whether to print detailed logs. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the median, 25th percentile (Q1), and 75th percentile (Q3) of the specified column.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example: Calculate descriptive statistics for a column with logging
stats <- calculate_descriptive_stats(df, "business_days_until_appointment", verbose = TRUE)

\end{ExampleCode}
\end{Examples}
\HeaderA{calculate\_distribution}{Calculate Demographic Distribution with Logging}{calculate.Rul.distribution}
%
\begin{Description}
This function calculates the distribution of a categorical variable within a data frame,
including counts and percentages. It also logs inputs, outputs, and all transformations
for transparency and debugging purposes.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
calculate_distribution(df, column)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the data.

\item[\code{column}] A string representing the name of the column for which the distribution is calculated.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the count, total, and percentage for each level of the specified column.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
df <- data.frame(gender = c("Male", "Female", "Female", "Male", "Male", "Female", NA))
result <- calculate_distribution(df, "gender")
print(result)
\end{ExampleCode}
\end{Examples}
\HeaderA{calculate\_intersection\_overlap\_and\_save}{Calculate intersection overlap and save results to shapefiles.}{calculate.Rul.intersection.Rul.overlap.Rul.and.Rul.save}
%
\begin{Description}
This function calculates the intersection between block groups and isochrones for a specific drive time and saves the results to a shapefile.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
calculate_intersection_overlap_and_save(
  block_groups,
  isochrones_joined,
  drive_time,
  output_dir
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{block\_groups}] An sf object representing block groups.

\item[\code{isochrones\_joined}] An sf object representing isochrones.

\item[\code{drive\_time}] The drive time value for which to calculate the intersection.

\item[\code{output\_dir}] The directory where the intersection shapefile will be saved.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None. The function saves the intersection shapefile and provides logging.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
calculate_intersection_overlap_and_save(block_groups, isochrones_joined, 30L, "data/shp/")

\end{ExampleCode}
\end{Examples}
\HeaderA{calculate\_proportion}{Calculate the Proportion of Each Level in a Categorical Variable}{calculate.Rul.proportion}
%
\begin{Description}
This function calculates the proportion of each level in a specified categorical variable within a data frame.
It returns a data frame with the counts and percentages of each level.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
calculate_proportion(df, variable_name)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the categorical variable.

\item[\code{variable\_name}] The name of the categorical variable for which proportions are calculated, passed as an unquoted expression.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function counts the occurrences of each unique value in the specified variable and calculates the percentage each value represents of the total count.
The percentages are rounded to two decimal places.
\end{Details}
%
\begin{Value}
A data frame with two columns: \code{n} (the count of each level) and \code{percent} (the percentage of the total count represented by each level).
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with a simple dataset
df <- data.frame(gender = c("Male", "Female", "Female", "Male", "Male", "Female"))
result <- calculate_proportion(df, gender)
print(result)

# Example 2: Handling a dataset with missing values
df_na <- data.frame(gender = c("Male", NA, "Female", "Female", "Male", "Female", NA))
result <- calculate_proportion(df_na, gender)
print(result)

# Example 3: Using a variable with multiple levels
df_multi <- data.frame(grade = c("A", "B", "A", "C", "B", "A", "C", "B"))
result <- calculate_proportion(df_multi, grade)
print(result)

\end{ExampleCode}
\end{Examples}
\HeaderA{check\_normality}{Check Normality and Summarize Data}{check.Rul.normality}
%
\begin{Description}
This function checks the normality of a specified variable in a dataframe using the Shapiro-Wilk test
and provides summary statistics (mean and standard deviation if normal, median and IQR if not normal).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
check_normality(data, variable)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing the data.

\item[\code{variable}] A string specifying the column name of the variable to be checked and summarized.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the summary statistics (mean and standard deviation if normal, median and IQR if not normal).
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage with a dataframe 'df' and outcome variable 'business_days_until_appointment'
check_normality(df, "business_days_until_appointment")
\end{ExampleCode}
\end{Examples}
\HeaderA{city\_state\_assign\_scenarios}{Assign Cases to Professionals by City and State}{city.Rul.state.Rul.assign.Rul.scenarios}
%
\begin{Description}
The \code{city\_state\_assign\_scenarios} function assigns cases to professionals based on their specialty and location. Generalists are assigned cases randomly, while specialists receive all cases. The function handles logging, error checking, and outputs the results to a CSV file. An optional seed parameter allows for reproducible random assignments.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
city_state_assign_scenarios(
  data,
  generalist = "General Dermatology",
  specialty = "Pediatric Dermatology",
  case_names = c("Alpha", "Beta", "Gamma"),
  output_csv_path = "Lizzy/data/city_state_assign_scenarios.csv",
  seed = 1978
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing professional information. Must include at least the columns \code{city}, \code{state\_code}, and \code{specialty\_primary}.

\item[\code{generalist}] A character string specifying the specialty name for generalists. Default is \code{"Generalist"}.

\item[\code{specialty}] A character string specifying the specialty name for specialists. Default is \code{"Specialist"}.

\item[\code{case\_names}] A character vector of case names to assign. Default is \code{c("Alpha", "Beta", "Gamma")}.

\item[\code{output\_csv\_path}] A character string specifying the file path to save the output CSV. Default is \code{"output/city\_state\_assign\_scenarios.csv"}.

\item[\code{seed}] An optional integer value to set the random seed for reproducibility. Default is \code{NULL} (no seed set).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the assigned cases.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Using default parameters
data <- data.frame(
  city = c("CityA", "CityA", "CityB", "CityB"),
  state_code = c("State1", "State1", "State2", "State2"),
  specialty_primary = c("Generalist", "Specialist", "Generalist", "Specialist"),
  stringsAsFactors = FALSE
)
result <- city_state_assign_scenarios(data)
print(result)

# Example 2: Specifying custom specialties and cases
data <- data.frame(
  city = c("CityA", "CityA", "CityB", "CityB", "CityC"),
  state_code = c("State1", "State1", "State2", "State2", "State3"),
  specialty_primary = c("General Worker", "Specialist Worker", "General Worker", "Specialist Worker", "Other Specialty"),
  stringsAsFactors = FALSE
)
result <- city_state_assign_scenarios(
  data = data,
  generalist = "General Worker",
  specialty = "Specialist Worker",
  case_names = c("Case1", "Case2", "Case3"),
  output_csv_path = "output/custom_assignments.csv",
  seed = 1234
)
print(result)

# Example 3: Testing with more generalists than cases and a set seed
data <- data.frame(
  city = rep("CityA", 6),
  state_code = rep("State1", 6),
  specialty_primary = c("Generalist", "Generalist", "Generalist", "Generalist", "Generalist", "Specialist"),
  stringsAsFactors = FALSE
)
result <- city_state_assign_scenarios(
  data = data,
  case_names = c("Alpha", "Beta"),
  output_csv_path = "output/more_generalists.csv",
  seed = 42
)
print(result)

\end{ExampleCode}
\end{Examples}
\HeaderA{city\_state\_sample\_specialists}{Sample Generalists and Specialists by City-State Combination}{city.Rul.state.Rul.sample.Rul.specialists}
%
\begin{Description}
This function samples specialists and generalists from a given dataset based on city-state combinations.
It allows sampling up to three types of specialists and generalists with customizable sample sizes for each.
The results can be saved as a CSV file or returned as a dataframe.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
city_state_sample_specialists(
  data,
  generalist = "General Dermatology",
  specialist1 = "Pediatric Dermatology",
  general_sample_size = 4,
  specialist1_sample_size = 2,
  specialist2 = NULL,
  specialist2_sample_size = 0,
  specialist3 = NULL,
  specialist3_sample_size = 0,
  output_csv_path = NULL,
  seed = 1978
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing specialist information. It must have columns: \code{city}, \code{state\_code}, and \code{specialty\_primary}.

\item[\code{generalist}] A character string specifying the generalist specialty to sample. Default is "General Dermatology".

\item[\code{specialist1}] A character string specifying the first specialist specialty to sample. Default is "Pediatric Dermatology".

\item[\code{general\_sample\_size}] An integer specifying how many generalists to sample for each city-state combination. Default is 4.

\item[\code{specialist1\_sample\_size}] An integer specifying how many of the first specialists to sample for each city-state combination. Default is 1.

\item[\code{specialist2}] A character string specifying a second specialist specialty to sample. Optional. Default is NULL.

\item[\code{specialist2\_sample\_size}] An integer specifying how many of the second specialists to sample. Default is 0.

\item[\code{specialist3}] A character string specifying a third specialist specialty to sample. Optional. Default is NULL.

\item[\code{specialist3\_sample\_size}] An integer specifying how many of the third specialists to sample. Default is 0.

\item[\code{output\_csv\_path}] A character string specifying the path to save the output CSV. If not provided, the result will only be returned.

\item[\code{seed}] An integer for setting a seed for reproducibility. Default is 1978.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing the sampled generalists and specialists for each city-state combination.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with default generalist and specialist
data <- data.frame(
  city = rep(c("New York", "Los Angeles"), each = 6),
  state_code = rep(c("NY", "CA"), each = 6),
  specialty_primary = c(
    "General Dermatology", "Pediatric Dermatology", "General Dermatology",
    "General Dermatology", "Pediatric Dermatology", "General Dermatology",
    "General Dermatology", "General Dermatology", "Pediatric Dermatology",
    "General Dermatology", "General Dermatology", "Pediatric Dermatology"
  )
)
result <- city_state_sample_specialists(data)

# Example 2: Custom sampling with different generalist and specialist, saving to a CSV
result <- city_state_sample_specialists(
  data,
  generalist = "General Dermatology",
  specialist1 = "Pediatric Dermatology",
  general_sample_size = 3,
  specialist1_sample_size = 2,
  output_csv_path = "sampled_data.csv"
)

# Example 3: Sampling multiple specialists with custom sample sizes
result <- city_state_sample_specialists(
  data,
  generalist = "General Dermatology",
  specialist1 = "Pediatric Dermatology",
  specialist2 = "Cosmetic Dermatology",
  general_sample_size = 2,
  specialist1_sample_size = 1,
  specialist2_sample_size = 1
)

\end{ExampleCode}
\end{Examples}
\HeaderA{clean\_phase\_1\_results}{Clean Phase 1 Results Data}{clean.Rul.phase.Rul.1.Rul.results}
%
\begin{Description}
This function reads the Phase 1 results data file, performs various cleaning
and transformation operations, and prepares the data for further analysis.
It ensures all required fields are present and formats column names. Missing
NPI numbers are handled by generating a unique \code{random\_id}.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
clean_phase_1_results(df)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the Phase 1 results data. Ensure that it
includes columns like 'for\_redcap', 'id', 'names', 'practice\_name', 'phone\_number',
'state\_name', and optionally 'npi'. If 'npi' is missing or any of its values are NA,
a \code{random\_id} is generated as a fallback.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Invisible NULL; the function is used for its side effects of cleaning data
and outputting a CSV file with cleaned data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
library(tyler)
file_path <- "/path/to/your/input/file.xls"
df <- readxl::read_excel(file_path)  # Assuming use of readxl for Excel files
clean_phase_1_results(df)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{clean\_phase\_2\_data}{Clean and process Phase 2 data}{clean.Rul.phase.Rul.2.Rul.data}
%
\begin{Description}
This function reads data from a file or data frame, cleans column names, and applies renaming based on specified criteria to facilitate data analysis. The function logs each step of the process, including data loading, column cleaning, and renaming for transparency.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
clean_phase_2_data(data_or_path, required_strings, standard_names)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_or\_path}] Path to the data file or a data frame.

\item[\code{required\_strings}] Vector of substrings for which to search in column names.

\item[\code{standard\_names}] Vector of new names to apply to the matched columns.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with processed data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming an input path to a CSV file
input_path <- "path_to_your_data.csv"
required_strings <- c("physician_information", "able_to_contact_office")
standard_names <- c("physician_info", "contact_office")
cleaned_data <- clean_phase_2_data(input_path, required_strings, standard_names)

# Directly using a data frame
df <- data.frame(
  doc_info = 1:5,
  contact_data = 6:10
)
required_strings <- c("doc_info", "contact_data")
standard_names <- c("doctor_info", "patient_contact_info")
cleaned_df <- clean_phase_2_data(df, required_strings, standard_names)
print(cleaned_df)
\end{ExampleCode}
\end{Examples}
\HeaderA{convert\_list\_to\_df\_expanded}{Convert a List of Column Names to an Expanded Data Frame}{convert.Rul.list.Rul.to.Rul.df.Rul.expanded}
%
\begin{Description}
This helper function converts a list of column names, grouped by table, into an expanded data frame
where each column name is placed into a separate column.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
convert_list_to_df_expanded(column_list)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{column\_list}] A named list where each element contains the column names of a table.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with each table name and its corresponding columns.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Convert a list of column names to an expanded data frame
test_list <- list(table1 = c("col1", "col2"), table2 = c("col1", "col2", "col3"))
convert_list_to_df_expanded(test_list)

# Example 2: Handling missing columns in some tables
test_list <- list(table1 = c("col1", "col2"), table2 = c("col1"))
convert_list_to_df_expanded(test_list)
\end{ExampleCode}
\end{Examples}
\HeaderA{count\_unique\_physicians}{Count Unique Physicians Based on Insurance Type and Exclusion Reason}{count.Rul.unique.Rul.physicians}
%
\begin{Description}
This function filters a dataframe of physician data based on insurance type, reason for exclusion,
and appointment availability, then counts the number of unique physicians who meet the criteria.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
count_unique_physicians(
  df,
  insurance_type,
  reason_for_exclusion = NULL,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A dataframe containing physician data. Must include columns 'insurance', 'reason\_for\_exclusions', 'business\_days\_until\_appointment', and 'phone'.

\item[\code{insurance\_type}] A string specifying the insurance type to filter by (e.g., "Medicaid").

\item[\code{reason\_for\_exclusion}] A string specifying the reason for exclusion to filter by. Default is NULL, which includes all rows regardless of the exclusion reason.

\item[\code{verbose}] A boolean indicating whether to print detailed logs. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An integer representing the number of unique physicians who meet the specified criteria.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage:
count_unique_physicians(df, insurance_type = "Medicaid", reason_for_exclusion = "Able to contact")
count_unique_physicians(df, insurance_type = "Blue Cross/Blue Shield", verbose = TRUE)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_and\_plot\_interaction}{Create and Plot Interaction Effects in GLMM}{create.Rul.and.Rul.plot.Rul.interaction}
%
\begin{Description}
This function reads data from a specified file, fits a generalized linear mixed model (GLMM) with a specified interaction term, and creates a plot to visualize the interaction effects. The plot is saved to a specified directory. This is particularly useful for analyzing how two categorical variables interact to affect a response variable, controlling for a random intercept.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_and_plot_interaction(
  data_path,
  response_variable,
  variable_of_interest,
  interaction_variable,
  random_intercept,
  output_path,
  resolution = 100
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_path}] A character string specifying the path to the .rds file containing the dataset.

\item[\code{response\_variable}] A character string specifying the name of the response variable in the dataset.

\item[\code{variable\_of\_interest}] A character string specifying the first categorical predictor variable in the interaction.

\item[\code{interaction\_variable}] A character string specifying the second categorical predictor variable in the interaction.

\item[\code{random\_intercept}] A character string specifying the variable to be used as the random intercept in the model (e.g., "city").

\item[\code{output\_path}] A character string specifying the directory where the interaction plot will be saved.

\item[\code{resolution}] An integer specifying the resolution (in DPI) for saving the plot. Defaults to 100.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function performs the following steps:
\begin{itemize}

\item{} Reads the data from the provided file path.
\item{} Renames and converts the specified columns to appropriate types.
\item{} Fits a Poisson GLMM with an interaction term and a random intercept.
\item{} Summarizes the interaction effects and creates a plot.
\item{} Saves the plot to the specified directory.

\end{itemize}


This function is useful in scenarios where you want to examine the interaction between two categorical variables and their combined effect on a count-based response variable (e.g., the number of business days until an appointment). It can handle complex data structures and controls for variability across groups using random intercepts.
\end{Details}
%
\begin{Value}
A list containing the fitted GLMM (\code{model}) and the summarized data used for the effects plot (\code{effects\_plot\_data}).
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example 1: Analyzing the effect of gender and appointment center on wait times
result <- create_and_plot_interaction(
  data_path = "Ari/data/Phase2/late_phase_2_ENT_analysis_3.rds",
  response_variable = "business_days_until_appointment",
  variable_of_interest = "central_number_e_g_appointment_center",
  interaction_variable = "gender",
  random_intercept = "city",
  output_path = "Ari/data/figures",
  resolution = 100
)

# Example 2: Examining the interaction between insurance type and scenario on appointment delays
result <- create_and_plot_interaction(
  data_path = "data/healthcare_calls.rds",
  response_variable = "business_days_until_appointment",
  variable_of_interest = "insurance_type",
  interaction_variable = "scenario",
  random_intercept = "state",
  output_path = "figures/insurance_scenario_interaction",
  resolution = 150
)

# Example 3: Studying the interaction between gender and subspecialty in wait times
result <- create_and_plot_interaction(
  data_path = "data/mystery_caller_study.rds",
  response_variable = "waiting_time_days",
  variable_of_interest = "subspecialty",
  interaction_variable = "gender",
  random_intercept = "clinic_id",
  output_path = "results/waiting_times",
  resolution = 300
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_and\_save\_physician\_dot\_map}{Create and Save a Leaflet Dot Map of Physicians}{create.Rul.and.Rul.save.Rul.physician.Rul.dot.Rul.map}
%
\begin{Description}
This function creates a Leaflet dot map of physicians using their longitude and latitude
coordinates. It also adds ACOG district boundaries to the map and saves it as an HTML file
with an accompanying PNG screenshot.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_and_save_physician_dot_map(
  physician_data,
  jitter_range = 0.05,
  color_palette = "magma",
  popup_var = "name"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{physician\_data}] An sf object containing physician data with "long" and "lat" columns.

\item[\code{jitter\_range}] The range for adding jitter to latitude and longitude coordinates.

\item[\code{color\_palette}] The color palette for ACOG district colors.

\item[\code{popup\_var}] The variable to use for popup text.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A Leaflet map object.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Load required libraries
library(viridis)
library(leaflet)

# Generate physician data (replace with your own data)
physician_data <- data.frame(
  long = c(-95.363271, -97.743061, -98.493628, -96.900115, -95.369803),
  lat = c(29.763283, 30.267153, 29.424349, 32.779167, 29.751808),
  name = c("Physician 1", "Physician 2", "Physician 3", "Physician 4", "Physician 5"),
  ACOG_District = c("District I", "District II", "District III", "District IV", "District V")
)

# Create and save the dot map
create_and_save_physician_dot_map(physician_data)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_block\_group\_overlap\_map}{Function to create and export a map showing block group overlap with isochrones}{create.Rul.block.Rul.group.Rul.overlap.Rul.map}
%
\begin{Description}
This function creates a map that displays block groups and their overlap with isochrones.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_block_group_overlap_map(
  bg_data,
  isochrones_data,
  output_dir = "figures/"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{bg\_data}] A SpatialPolygonsDataFrame representing block group data.

\item[\code{isochrones\_data}] A SpatialPolygonsDataFrame representing isochrone data.

\item[\code{output\_dir}] Directory path for exporting the map files. Default is "figures/".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Create and export the map with the default output directory
create_block_group_overlap_map(block_groups, isochrones_joined_map)

# Create and export the map with a custom output directory
create_block_group_overlap_map(block_groups, isochrones_joined_map, "custom_output/")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_density\_plot}{Create a Density Plot for Mystery Caller Studies with Optional Transformations}{create.Rul.density.Rul.plot}
%
\begin{Description}
This function generates a density plot designed for mystery caller studies, allowing for the visualization of waiting times or similar outcomes across different categories, such as insurance types. The function supports transformations on the x-axis and custom labels.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_density_plot(
  data,
  x_var,
  fill_var,
  x_transform = "none",
  dpi = 100,
  output_dir = "output",
  file_prefix = "density_plot",
  x_label = NULL,
  y_label = "Density",
  plot_title = NULL,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing the data to be plotted. Must contain the variables specified in \code{x\_var} and \code{fill\_var}.

\item[\code{x\_var}] A string representing the column name for the x-axis variable. This should be a numeric variable (e.g., waiting time in days).

\item[\code{fill\_var}] A string representing the column name for the fill variable. This should be a categorical or factor variable (e.g., insurance type).

\item[\code{x\_transform}] A string specifying the transformation for the x-axis: "log" for log transformation (log1p), "sqrt" for square root transformation, or "none" for no transformation. Default is "none".

\item[\code{dpi}] An integer specifying the resolution of the saved plot in dots per inch (DPI). Default is 100.

\item[\code{output\_dir}] A string representing the directory where the plot files will be saved. Default is "output".

\item[\code{file\_prefix}] A string used as the prefix for the generated plot filenames. The filenames will have a timestamp appended to ensure uniqueness. Default is "density\_plot".

\item[\code{x\_label}] A string specifying the label for the x-axis. Default is \code{NULL} (uses x\_var).

\item[\code{y\_label}] A string specifying the label for the y-axis. Default is "Density".

\item[\code{plot\_title}] A string specifying the title of the plot. Default is \code{NULL} (no title).

\item[\code{verbose}] A boolean indicating whether to print messages about the saved plot locations. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
This function displays the plot and saves it to the specified directory.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic density plot with log transformation
create_density_plot(
    data = df3,
    x_var = "business_days_until_appointment",
    fill_var = "insurance",
    x_transform = "log",  # Log transformation
    dpi = 100,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_density",
    x_label = "Log (Waiting Times in Days)",
    y_label = "Density",
    plot_title = "Density Plot of Waiting Times by Insurance"
)

# Example 2: Density plot with square root transformation
create_density_plot(
    data = df3,
    x_var = "business_days_until_appointment",
    fill_var = "insurance",
    x_transform = "sqrt",  # Square root transformation
    dpi = 150,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_density_sqrt",
    x_label = "Sqrt (Waiting Times in Days)",
    y_label = "Density",
    plot_title = "Square Root Transformed Density Plot"
)

# Example 3: Density plot without any transformation
create_density_plot(
    data = df3,
    x_var = "business_days_until_appointment",
    fill_var = "insurance",
    x_transform = "none",  # No transformation
    dpi = 200,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_density_none",
    x_label = "Waiting Times in Days",
    y_label = "Density",
    plot_title = "Density Plot Without Transformation"
)
\end{ExampleCode}
\end{Examples}
\HeaderA{create\_forest\_plot}{Create a Forest Plot for Significant Predictors with Logging}{create.Rul.forest.Rul.plot}
%
\begin{Description}
This function creates a forest plot for significant predictors with their coefficients and confidence intervals.
It logs the process, including inputs and outputs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_forest_plot(df, target_variable, significant_vars)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the dataset.

\item[\code{target\_variable}] A string representing the name of the target variable.

\item[\code{significant\_vars}] A data frame containing significant predictors and their coefficients.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object representing the forest plot.
\end{Value}
\HeaderA{create\_formula}{Create a Formula for Poisson Model}{create.Rul.formula}
%
\begin{Description}
This function creates a formula for a Poisson model based on the provided data,
response variable, and optional random effect.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_formula(data, response_var, random_effect = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing the predictor and response variables.

\item[\code{response\_var}] The name of the response variable in the dataframe.

\item[\code{random\_effect}] Optional. The name of the random effect variable for the formula.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A formula object suitable for modeling in R.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage:
response_variable <- "days"
random_effect_term <- "name"  # Change this to the desired random effect variable
df3_filtered <- data.frame(days = c(5, 10, 15), age = c(30, 40, 50), name = c("A", "B", "C"))
formula <- create_formula(df3_filtered, response_variable, random_effect_term)
formula

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_individual\_isochrone\_plots}{Create Individual Isochrone Maps and Shapefiles}{create.Rul.individual.Rul.isochrone.Rul.plots}
%
\begin{Description}
This function creates individual Leaflet maps and shapefiles for specified drive times
based on isochrone data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_individual_isochrone_plots(isochrones, drive_times)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{isochrones}] An sf object containing isochrone data.

\item[\code{drive\_times}] A vector of unique drive times (in minutes) for which maps and shapefiles will be created.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
None. The function creates and saves individual maps and shapefiles.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Load required libraries
library(sf)
library(leaflet)
library(tyler)

# Load isochrone data
isochrones <- readRDS("path_to_isochrones.rds")

# List of unique drive times for which you want to create plots and shapefiles
drive_times <- unique(isochrones$drive_time)

# Create individual isochrone maps and shapefiles
create_individual_isochrone_plots(isochrones, drive_times)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_insurance\_by\_insurance\_scatter\_plot}{Create a Scatter Plot Comparing Waiting Times Between Two Insurance Types}{create.Rul.insurance.Rul.by.Rul.insurance.Rul.scatter.Rul.plot}
%
\begin{Description}
This function creates a scatter plot comparing waiting times (in days) to an appointment between two different insurance types.
The plot is saved as both a TIFF and PNG file in the specified output directory. The function allows customization of plot aesthetics,
including axis labels, point size, and alpha transparency.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_insurance_by_insurance_scatter_plot(
  df,
  unique_variable,
  insurance1 = "medicaid",
  insurance2 = "blue cross/blue shield",
  output_directory = "ortho_sports_med/figures/",
  dpi = 100,
  height = 8,
  width = 11,
  x_label = "Time in days to appointment\nBlue Cross Blue Shield (Log Scale)",
  y_label = "Time in days to appointment\nMedicaid (Log Scale)",
  plot_title = "Comparison of Waiting Times: Medicaid vs Blue Cross Blue Shield",
  point_size = 3,
  point_alpha = 0.6,
  add_confidence_interval = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the data to be plotted.

\item[\code{unique\_variable}] A string representing the column name that uniquely identifies each entity in the data (e.g., "phone" or "npi").

\item[\code{insurance1}] A string representing the first insurance type to be compared (default is "medicaid").

\item[\code{insurance2}] A string representing the second insurance type to be compared (default is "blue cross/blue shield").

\item[\code{output\_directory}] A string specifying the directory where the plot files will be saved. The default is "ortho\_sports\_med/figures/".

\item[\code{dpi}] An integer specifying the resolution of the saved plot files in dots per inch (DPI). The default is 100.

\item[\code{height}] A numeric value specifying the height of the saved plot files in inches. The default is 8 inches.

\item[\code{width}] A numeric value specifying the width of the saved plot files in inches. The default is 11 inches.

\item[\code{x\_label}] A string representing the label for the x-axis. The default is "Time in days to appointment Blue Cross Blue Shield (Log Scale)".

\item[\code{y\_label}] A string representing the label for the y-axis. The default is "Time in days to appointment Medicaid (Log Scale)".

\item[\code{plot\_title}] A string representing the title of the plot. The default is "Comparison of Waiting Times: Medicaid vs Blue Cross Blue Shield".

\item[\code{point\_size}] A numeric value specifying the size of the points in the scatter plot. The default is 3.

\item[\code{point\_alpha}] A numeric value between 0 and 1 specifying the transparency level of the points in the scatter plot. The default is 0.6.

\item[\code{add\_confidence\_interval}] A logical value indicating whether to add a confidence interval around the linear fit line. The default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot2 scatter plot object.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Create and save a scatter plot with all arguments filled in
scatterplot <- create_insurance_by_insurance_scatter_plot(
  df = df3,  # The data frame to be used
  unique_variable = "phone",  # The unique identifier variable
  insurance1 = "medicaid",  # The first insurance type
  insurance2 = "blue cross/blue shield",  # The second insurance type
  output_directory = "ortho_sports_med/figures/custom_directory/",  # Custom output directory
  dpi = 300,  # Higher DPI for better resolution
  height = 10,  # Custom height for the plot
  width = 15,  # Custom width for the plot
  x_label = "Appointment Time (days) - Blue Cross Blue Shield (Log Scale)",  # Custom x-axis label
  y_label = "Appointment Time (days) - Medicaid (Log Scale)",  # Custom y-axis label
  plot_title = "Custom Plot Title: Waiting Times Comparison",  # Custom plot title
  point_size = 4,  # Larger points
  point_alpha = 0.8,  # Less transparent points
  add_confidence_interval = FALSE  # Do not add a confidence interval around the fit line
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_isochrones}{Memoized function to try a location with isoline calculations}{create.Rul.isochrones}
%
\begin{Description}
This function calculates isolines for a given location using the hereR package.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_isochrones(
  location,
  range,
  posix_time = as.POSIXct("2023-10-20 08:00:00", format = "%Y-%m-%d %H:%M:%S")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{location}] An sf object representing the location for which isolines will be calculated.

\item[\code{range}] A numeric vector of time ranges in seconds.

\item[\code{posix\_time}] A POSIXct object representing the date and time of calculation. Default is "2023-10-20 08:00:00".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list of isolines for different time ranges, or an error message if the calculation fails.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Set your HERE API key in your Renviron file using the following steps:
# 1. Add key to .Renviron
Sys.setenv(HERE_API_KEY = "your_api_key_here")
# 2. Reload .Renviron
readRenviron("~/.Renviron")

# Define a sf object for the location
location <- sf::st_point(c(-73.987, 40.757))

# Calculate isolines for the location with a 30-minute, 60-minute, 120-minute, and 180-minute range
isolines <- create_isochrones(location = location, range = c(1800, 3600, 7200, 10800))

# Print the isolines
print(isolines)


## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{create\_isochrones\_for\_dataframe}{Get isochrones for each point in a dataframe}{create.Rul.isochrones.Rul.for.Rul.dataframe}
%
\begin{Description}
This function retrieves isochrones for each point in a given dataframe by looping
over the rows and calling the create\_isochrones function for each point.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_isochrones_for_dataframe(
  input_file,
  breaks = c(1800, 3600, 7200, 10800)
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_file}] A path to the input file containing points for which isochrones are to be retrieved.

\item[\code{breaks}] A numeric vector specifying the breaks for categorizing drive times (default is c(1800, 3600, 7200, 10800)).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing the isochrones data with added 'name' column.
\end{Value}
\HeaderA{create\_line\_plot}{Create a Line Plot with Optional Transformations and Grouping}{create.Rul.line.Rul.plot}
%
\begin{Description}
This function creates a line plot using ggplot2 with options for transforming the y-axis, grouping lines, and saving the plot with a specified resolution. The plot can be saved in both TIFF and PNG formats with automatic filename generation.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_line_plot(
  data,
  x_var,
  y_var,
  y_transform = "none",
  dpi = 100,
  output_dir = "output",
  file_prefix = "line_plot",
  use_geom_line = FALSE,
  geom_line_group = NULL,
  point_color = "viridis",
  line_color = "red",
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing the data to be plotted. Must contain the variables specified in \code{x\_var} and \code{y\_var}.

\item[\code{x\_var}] A string representing the column name for the x-axis variable. This should be a categorical or factor variable.

\item[\code{y\_var}] A string representing the column name for the y-axis variable. This should be a numeric variable.

\item[\code{y\_transform}] A string specifying the transformation for the y-axis: "log" for log transformation (log1p), "sqrt" for square root transformation, or "none" for no transformation. Default is "none".

\item[\code{dpi}] An integer specifying the resolution of the saved plot in dots per inch (DPI). Default is 100.

\item[\code{output\_dir}] A string representing the directory where the plot files will be saved. Default is "output".

\item[\code{file\_prefix}] A string used as the prefix for the generated plot filenames. The filenames will have a timestamp appended to ensure uniqueness. Default is "line\_plot".

\item[\code{use\_geom\_line}] A boolean indicating whether to include lines connecting points for grouped data. Default is FALSE.

\item[\code{geom\_line\_group}] A string representing the column name to group the lines by when \code{use\_geom\_line} is TRUE. This should be a categorical or factor variable.

\item[\code{point\_color}] A string specifying the color of the points. Default is "viridis", which uses the viridis color palette.

\item[\code{line\_color}] A string specifying the color of the summary line (median). Default is "red".

\item[\code{verbose}] A boolean indicating whether to print messages about the saved plot locations. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
This function saves the plot to the specified directory and returns no value.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic plot with log transformation
create_line_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "log",  # Log transformation
    dpi = 100,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance"
)

# Example 2: Plot with square root transformation and lines grouped by 'last'
create_line_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "sqrt",  # Square root transformation
    dpi = 150,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_sqrt",
    use_geom_line = TRUE,  # Include lines
    geom_line_group = "last"  # Group lines by 'last' column
)

# Example 3: Plot without any transformation and without lines
create_line_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "none",  # No transformation
    dpi = 200,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_none"
)
\end{ExampleCode}
\end{Examples}
\HeaderA{create\_scatter\_plot}{Create a Scatter Plot for Mystery Caller Studies with Optional Transformations, Jitter, and Custom Labels}{create.Rul.scatter.Rul.plot}
%
\begin{Description}
This function generates a scatter plot designed for mystery caller studies, allowing for the visualization of waiting times or similar outcomes across different categories, such as insurance types. The function supports transformations on the y-axis, custom jitter, and colors each category in the x-axis using the \code{viridis} color palette. The plot is automatically displayed and saved with a specified resolution.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_scatter_plot(
  data,
  x_var,
  y_var,
  y_transform = "none",
  dpi = 100,
  output_dir = "output",
  file_prefix = "scatter_plot",
  jitter_width = 0.2,
  jitter_height = 0,
  point_alpha = 0.6,
  x_label = NULL,
  y_label = NULL,
  plot_title = NULL,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A dataframe containing the data to be plotted. Must contain the variables specified in \code{x\_var} and \code{y\_var}.

\item[\code{x\_var}] A string representing the column name for the x-axis variable. This should be a categorical or factor variable (e.g., insurance type).

\item[\code{y\_var}] A string representing the column name for the y-axis variable. This should be a numeric variable (e.g., waiting time in days).

\item[\code{y\_transform}] A string specifying the transformation for the y-axis: "log" for log transformation (log1p), "sqrt" for square root transformation, or "none" for no transformation. Default is "none".

\item[\code{dpi}] An integer specifying the resolution of the saved plot in dots per inch (DPI). Default is 100.

\item[\code{output\_dir}] A string representing the directory where the plot files will be saved. Default is "output".

\item[\code{file\_prefix}] A string used as the prefix for the generated plot filenames. The filenames will have a timestamp appended to ensure uniqueness. Default is "scatter\_plot".

\item[\code{jitter\_width}] A numeric value specifying the width of the jitter along the x-axis. Default is 0.2.

\item[\code{jitter\_height}] A numeric value specifying the height of the jitter along the y-axis. Default is 0.

\item[\code{point\_alpha}] A numeric value specifying the transparency level of the points. Default is 0.6.

\item[\code{x\_label}] A string specifying the label for the x-axis. Default is \code{NULL} (uses x\_var).

\item[\code{y\_label}] A string specifying the label for the y-axis. Default is \code{NULL} (uses y\_var or transformed version).

\item[\code{plot\_title}] A string specifying the title of the plot. Default is \code{NULL} (no title).

\item[\code{verbose}] A boolean indicating whether to print messages about the saved plot locations. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
This function displays the plot and saves it to the specified directory.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic scatter plot with log transformation
create_scatter_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "log",  # Log transformation
    dpi = 100,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance",
    x_label = "Insurance",
    y_label = "Log (Waiting Times in Days)",
    plot_title = "Scatter Plot of Waiting Times by Insurance"
)

# Example 2: Scatter plot with square root transformation and custom jitter
create_scatter_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "sqrt",  # Square root transformation
    dpi = 150,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_sqrt",
    jitter_width = 0.3,
    jitter_height = 0.1,
    x_label = "Insurance",
    y_label = "Square Root (Waiting Times in Days)",
    plot_title = "Square Root Transformed Scatter Plot"
)

# Example 3: Scatter plot without any transformation and increased transparency
create_scatter_plot(
    data = df3,
    x_var = "insurance",
    y_var = "business_days_until_appointment",
    y_transform = "none",  # No transformation
    dpi = 200,
    output_dir = "ortho_sports_med/Figures",
    file_prefix = "ortho_sports_vs_insurance_none",
    point_alpha = 0.8,
    x_label = "Insurance",
    y_label = "Waiting Times in Days",
    plot_title = "Scatter Plot Without Transformation"
)
\end{ExampleCode}
\end{Examples}
\HeaderA{create\_summary\_sentence}{Create a Summary Sentence with Logging}{create.Rul.summary.Rul.sentence}
%
\begin{Description}
This function generates a summary sentence based on significant variables and Medicaid acceptance rates.
It logs the process, including inputs, outputs, and the construction of the summary sentence.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
create_summary_sentence(
  significant_vars,
  medicaid_acceptance_rate,
  count_accepts_medicaid,
  count_medicaid_insurance
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{significant\_vars}] A data frame containing significant predictor variables, their directions, and formatted p-values.

\item[\code{medicaid\_acceptance\_rate}] A numeric value representing the Medicaid acceptance rate.

\item[\code{count\_accepts\_medicaid}] An integer representing the count of physicians accepting Medicaid.

\item[\code{count\_medicaid\_insurance}] An integer representing the total count of physicians considered for Medicaid.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character string representing the summary sentence.
\end{Value}
\HeaderA{determine\_direction}{Determine the Direction of Effects for Significant Variables with Logging}{determine.Rul.direction}
%
\begin{Description}
This function determines whether the effect of each significant predictor is positive or negative.
It logs the process, including inputs and outputs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
determine_direction(df, target_variable, significant_vars)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the dataset.

\item[\code{target\_variable}] A string representing the name of the target variable.

\item[\code{significant\_vars}] A data frame containing significant predictors and their p-values.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with an additional column indicating the direction ("Higher" or "Lower") of each significant predictor.
\end{Value}
\HeaderA{find\_common\_columns\_in\_years\_of\_open\_payments\_data}{Find Common Columns in Years of Open Payments Data}{find.Rul.common.Rul.columns.Rul.in.Rul.years.Rul.of.Rul.open.Rul.payments.Rul.data}
%
\begin{Description}
This function identifies and compares the column names from multiple years of
Open Payments data stored in DuckDB. It synchronizes the column names across
different tables, writes the result to an Excel file, and provides detailed logging.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
find_common_columns_in_years_of_open_payments_data(
  con,
  output_excel_path,
  table_names = c("OP_DTL_GNRL_PGYR2014_P06302021", "OP_DTL_GNRL_PGYR2015_P06302021",
    "OP_DTL_GNRL_PGYR2016_P01182024", "OP_DTL_GNRL_PGYR2017_P01182024",
    "OP_DTL_GNRL_PGYR2018_P01182024", "OP_DTL_GNRL_PGYR2019_P01182024",
    "OP_DTL_GNRL_PGYR2020_P01182024", "OP_DTL_GNRL_PGYR2021_P01182024",
    "OP_DTL_GNRL_PGYR2022_P01182024", "OP_DTL_GNRL_PGYR2023_P06282024_06122024"),
  beep_on_complete = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] A DuckDB connection object.

\item[\code{output\_excel\_path}] A string specifying the path to save the output Excel file.

\item[\code{table\_names}] A character vector specifying the table names to process.
Defaults to Open Payments data from 2014 to 2023.

\item[\code{beep\_on\_complete}] Logical value indicating if a beep sound should be played when the function completes. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A tibble with the column names from each table.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example with default table names:
con <- DBI::dbConnect(duckdb::duckdb(), "path_to_duckdb.duckdb")
find_common_columns_in_years_of_open_payments_data(con, "output.xlsx")

# Example with custom table names:
custom_tables <- c("OP_DTL_GNRL_PGYR2014_P06302021", "OP_DTL_GNRL_PGYR2015_P06302021")
find_common_columns_in_years_of_open_payments_data(con, "output_custom.xlsx", custom_tables)
\end{ExampleCode}
\end{Examples}
\HeaderA{fips}{Data of FIPS codes}{fips}
\keyword{datasets}{fips}
%
\begin{Description}
3 columns with 51 observations.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fips
\end{verbatim}
\end{Usage}
%
\begin{Format}
A tibble with 243 rows and 10 variables:
\begin{description}


\end{description}

\end{Format}
%
\begin{Source}
\url{https://github.com/kjhealy/fips-codes/blob/master/state_and_county_fips_master.csv}
\end{Source}
\HeaderA{fit\_mixed\_model\_with\_logging}{Fit a Mixed-Effects Model with Logging and Robust Error Handling}{fit.Rul.mixed.Rul.model.Rul.with.Rul.logging}
%
\begin{Description}
This function fits either a linear mixed-effects model (\code{lmer}) or a robust linear mixed-effects model (\code{rlmer})
and logs every step. It is designed to handle errors robustly, provide progress and completion signals through system beeps, and
optionally save results to a file. The function is highly configurable but works out of the box with default settings.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_mixed_model_with_logging(
  data,
  response_variable = "log_business_days_until_appointment",
  random_effect = "(1 | NPI)",
  excluded_columns = c(response_variable, "last", "business_days_until_appointment",
    "cleaned_does_the_physician_accept_medicaid", "record_id", "ID", "middle",
    "physician_information", "address", "offered_a_clinic_appointment_to_be_seen",
    "reason_for_exclusions", "state", "Grd_yr", "age_category", "notes", "first",
    "does_the_physician_accept_medicaid", "insurance_type", "zip", "Subspecialty", "NPI",
    "lng", "lat", "including_this_physician_in_the_study",
    "told_to_go_to_the_emergency_department"),
  model_type = "lmer",
  significance_level = 0.2,
  output_path = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame containing the dataset.

\item[\code{response\_variable}] A character string specifying the response variable. Default is \code{"log\_business\_days\_until\_appointment"}.

\item[\code{random\_effect}] A character string specifying the random effect. Default is \code{"(1 | NPI)"}.

\item[\code{excluded\_columns}] A character vector specifying the columns to exclude from the predictor variables.
Default is \code{c(response\_variable, "last", "business\_days\_until\_appointment",  "cleaned\_does\_the\_physician\_accept\_medicaid", "record\_id", "ID", "middle", "physician\_information", "address", "offered\_a\_clinic\_appointment\_to\_be\_seen", "reason\_for\_exclusions", "state", "Grd\_yr", "age\_category", "notes", "first", "does\_the\_physician\_accept\_medicaid", "insurance\_type", "zip", "Subspecialty", "NPI", "lng", "lat", "including\_this\_physician\_in\_the\_study", "told\_to\_go\_to\_the\_emergency\_department")}.

\item[\code{model\_type}] A character string indicating the model type: \code{"lmer"} for linear mixed-effects or \code{"rlmer"} for robust linear mixed-effects. Default is \code{"lmer"}.

\item[\code{significance\_level}] A numeric value for the p-value threshold for filtering significant predictors. Default is \code{0.2}.

\item[\code{output\_path}] Optional. A character string specifying the file path where the results should be saved as a CSV. If not provided, results will not be saved.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A cleaned-up tibble with the significant predictors, p-values, IRR (Incident Rate Ratios), confidence intervals, and whether the effect is associated with "longer" or "shorter" wait times.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with default settings
df <- my_data_frame
result <- fit_mixed_model_with_logging(data = df)

# Example 2: Using a robust linear mixed-effects model (rlmer) and saving the results
result <- fit_mixed_model_with_logging(data = df, model_type = "rlmer", output_path = "results_rlmer.csv")

# Example 3: Custom response variable and random effect, with significance level 0.05
result <- fit_mixed_model_with_logging(data = df,
                                       response_variable = "some_other_response",
                                       random_effect = "(1 | group_id)",
                                       significance_level = 0.05)

\end{ExampleCode}
\end{Examples}
\HeaderA{fit\_poisson\_models}{Fit Poisson Models and Extract P-Values with Logging}{fit.Rul.poisson.Rul.models}
%
\begin{Description}
This function fits Poisson models for each predictor variable against a target variable.
It logs the process, including inputs, outputs, and any variables that are skipped.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
fit_poisson_models(df, target_variable, predictor_vars)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the dataset.

\item[\code{target\_variable}] A string representing the name of the target variable.

\item[\code{predictor\_vars}] A vector of strings representing the names of predictor variables.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame containing the predictor variables and their corresponding p-values.
\end{Value}
\HeaderA{format\_pct}{Format a Numeric Value as a Percentage}{format.Rul.pct}
%
\begin{Description}
This function formats a numeric value as a percentage with a specified number of decimal places.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
format_pct(x, my_digits = 1)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{x}] A numeric value or vector that you want to format as a percentage.

\item[\code{my\_digits}] An integer specifying the number of decimal places to include in the formatted percentage. The default is 1.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function converts a numeric value to a percentage format with the specified number of decimal places.
This is useful for consistent display of percentage values in reports or visualizations.
\end{Details}
%
\begin{Value}
A character vector representing the formatted percentage(s) with the specified number of decimal places.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Format a single numeric value
result <- format_pct(0.12345)
print(result)  # Output: "12.3%"

# Example 2: Format a vector of numeric values with 2 decimal places
values <- c(0.12345, 0.6789, 0.54321)
formatted_values <- format_pct(values, my_digits = 2)
print(formatted_values)  # Output: "12.35%", "67.89%", "54.32%"

# Example 3: Format a value with no decimal places
no_decimal <- format_pct(0.5, my_digits = 0)
print(no_decimal)  # Output: "50%"

\end{ExampleCode}
\end{Examples}
\HeaderA{genderize\_physicians}{Genderize Physicians Data}{genderize.Rul.physicians}
%
\begin{Description}
This function reads a CSV file containing physician data, genderizes the first names,
and joins the gender information back to the original data. It then saves the
result to a new CSV file with a timestamp.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
genderize_physicians(input_csv, output_dir = getwd())
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_csv}] The path to the input CSV file containing physician data.

\item[\code{output\_dir}] The directory where the output CSV file will be saved. Default is the current working directory.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with genderized information joined to the original data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
result <- genderize_physicians("sample.csv")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{generate\_latex\_equation}{Generate LaTeX Equation with Logging}{generate.Rul.latex.Rul.equation}
%
\begin{Description}
This function generates a LaTeX equation that incorporates the provided "patient\_scenario\_label".
The function logs the input, processes the input string by escaping LaTeX characters,
and constructs a LaTeX equation. It logs all operations and can output to both the console
and a log file (if provided).
\end{Description}
%
\begin{Usage}
\begin{verbatim}
generate_latex_equation(
  patient_scenario_label = "Default Patient Scenario",
  log_file = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{patient\_scenario\_label}] A string representing the text (typically a "Patient Scenario")
to be inserted into the LaTeX equation. Default is "Default Patient Scenario".

\item[\code{log\_file}] A string representing the full path to a file where logs will be written.
If NULL (default), logs are printed only to the console.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function generates dynamic LaTeX code using the provided \code{patient\_scenario\_label}, ensuring
special LaTeX characters (such as underscores) are escaped properly. It logs the entire process,
including the input validation, transformations, and final output. If no \code{patient\_scenario\_label}
is provided, a default value is used.
\end{Details}
%
\begin{Value}
The LaTeX code as a string, ready to be inserted into an RMarkdown or LaTeX document.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with logging to the console
## Not run: 
generate_latex_equation("Patient Scenario")

## End(Not run)

# Example 2: Handle underscores in the patient_scenario_label
## Not run: 
generate_latex_equation("Patient_Scenario_With_Underscores")

## End(Not run)

# Example 3: Logging the process to a file
## Not run: 
log_file_path <- "latex_generation_log.txt"
generate_latex_equation("Patient Scenario", log_file = log_file_path)

## End(Not run)

# Example 4: Using default patient_scenario_label when none is provided
## Not run: 
latex_code <- generate_latex_equation()
cat(latex_code)

## End(Not run)

# Example 5: Input validation error - non-character input
## Not run: 
generate_latex_equation(12345)  # This will throw an error

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{generate\_leaflet\_base\_map}{Generate Leaflet Base Map}{generate.Rul.leaflet.Rul.base.Rul.map}
%
\begin{Description}
This function creates a Leaflet BASE map with specific configurations, including the base tile layer, scale bar,
default view settings, and layers control.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
generate_leaflet_base_map()
\end{verbatim}
\end{Usage}
%
\begin{Value}
A Leaflet map object.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
map <- generate_leaflet_base_map()
\end{ExampleCode}
\end{Examples}
\HeaderA{geocode\_unique\_addresses}{Geocode Unique Addresses}{geocode.Rul.unique.Rul.addresses}
%
\begin{Description}
This function geocodes unique addresses using the Google Maps API and appends the
latitude and longitude to the original dataset. Please ensure that every dataset
must have a column named 'address'.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
geocode_unique_addresses(
  file_path,
  google_maps_api_key,
  output_file_path = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{file\_path}] Path to the input file (CSV, RDS, or XLSX) containing address data.

\item[\code{google\_maps\_api\_key}] Your Google Maps API key.

\item[\code{output\_file\_path}] Path to the output CSV file where the geocoded data will be saved. (Optional)
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing geocoded address data with latitude and longitude.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Define the input file path, Google Maps API key, and output file path (optional)
file_path <- "input_data.csv"
google_maps_api_key <- "your_api_key"
output_file_path <- "output_data.csv"  # Optional

# Call the geocode_unique_addresses function with or without specifying output_file_path
geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key)
# or
geocoded_data <- geocode_unique_addresses(file_path, google_maps_api_key, output_file_path)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{get\_census\_data}{Get Census data of all state block groups}{get.Rul.census.Rul.data}
%
\begin{Description}
This function retrieves Census data of all state block groups by looping
over the specified list of state FIPS codes.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
get_census_data(us_fips_list, vintage = 2022)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{us\_fips\_list}] A vector of state FIPS codes which Census data is to be retrieved.

\item[\code{vintage}] The vintage year is Census data (default is 2022).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing Census data all state block groups.
\end{Value}
\HeaderA{hrr}{Get Hospital Referral Region Shapefile}{hrr}
%
\begin{Description}
This function loads the hospital referral region shapefile and optionally removes Hawaii and Alaska.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
hrr(remove_HI_AK = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{remove\_HI\_AK}] Logical, should Hawaii and Alaska be removed? Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
An sf object containing the hospital referral region data.
\end{Value}
\HeaderA{hrr\_generate\_maps}{Generate Hexagon Maps for Hospital Referral Regions (HRR)}{hrr.Rul.generate.Rul.maps}
%
\begin{Description}
This function generates hexagon maps for hospital referral regions.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
hrr_generate_maps(physician_sf, trait_map = "all", honey_map = "all")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{physician\_sf}] An sf object containing physician data with coordinates.

\item[\code{trait\_map}] A string specifying the trait map (default is "all").

\item[\code{honey\_map}] A string specifying the honey map (default is "all").
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A ggplot object of the generated map.
\end{Value}
\HeaderA{install\_missing\_packages}{Install Missing CRAN Packages}{install.Rul.missing.Rul.packages}
%
\begin{Description}
This function checks if the specified CRAN packages are installed. If a package is not installed, it will be installed automatically. For packages that are already installed, a message will be printed indicating that they are installed along with their version.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
install_missing_packages(cran_pkgs)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{cran\_pkgs}] A character vector of CRAN package names to check and install if missing.
\end{ldescription}
\end{Arguments}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Install ggplot2 and dplyr if missing
install_missing_packages(c("ggplot2", "dplyr"))

# Example 2: Install multiple packages including data.table and tidyr
install_missing_packages(c("data.table", "tidyr", "readr"))

# Example 3: Install a large set of packages commonly used in data science
install_missing_packages(c("ggplot2", "dplyr", "tidyverse", "caret", "randomForest"))

\end{ExampleCode}
\end{Examples}
\HeaderA{linear\_regression\_race\_drive\_time\_generate\_summary\_sentence}{Generate Summary Sentence for Linear Regression on Race and Drive Time with Raw Proportions}{linear.Rul.regression.Rul.race.Rul.drive.Rul.time.Rul.generate.Rul.summary.Rul.sentence}
%
\begin{Description}
This function generates a summary sentence using linear regression to analyze the trend in the proportion of women without access to a gynecologic oncologist over time for a specified race and driving time. It includes the raw proportions for the first and last years in the dataset.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
linear_regression_race_drive_time_generate_summary_sentence(
  tabulated_data,
  driving_time_minutes = 180,
  race = "American Indian/Alaska Native"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tabulated\_data}] A data frame containing the data to analyze. Must include columns \AsIs{\texttt{Driving Time (minutes)}}, \code{Year}, and columns for race proportions like \code{White\_prop}, \code{Black\_prop}, etc.

\item[\code{driving\_time\_minutes}] A numeric value specifying the driving time in minutes to filter the data. Default is 180.

\item[\code{race}] A character string specifying the race for which to generate the summary sentence. Supported values are "White", "Black", "American Indian/Alaska Native", "Asian", "Native Hawaiian or Pacific Islander", or "all" to generate sentences for all supported races. Default is "American Indian/Alaska Native".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character string containing the summary sentence, or a list of summary sentences if \code{race = "all"}.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage
summary_sentence <- linear_regression_race_drive_time_generate_summary_sentence(
  tabulated_data = tabulated_all_years_numeric,
  driving_time_minutes = 180,
  race = "White"
)
print(summary_sentence)
\end{ExampleCode}
\end{Examples}
\HeaderA{load\_data}{Load and Process Data from an RDS File with Robust Logging}{load.Rul.data}
%
\begin{Description}
This function loads data from an RDS file, renames the 'ID' column to 'id\_number', and logs every step of the process.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
load_data(data_dir, file_name, verbose = TRUE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_dir}] A string specifying the directory where the RDS file is located.

\item[\code{file\_name}] A string specifying the name of the RDS file to load.

\item[\code{verbose}] A boolean indicating whether to print detailed logs. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with the 'ID' column renamed to 'id\_number'.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example: Load data from a specified directory with logging
df <- load_data(data_dir = "data", file_name = "Phase_2.rds", verbose = TRUE)

\end{ExampleCode}
\end{Examples}
\HeaderA{logistic\_regression}{Perform Logistic Regression on Multiple Predictors with Logging}{logistic.Rul.regression}
%
\begin{Description}
This function performs logistic regression for multiple predictor variables
against a specified target variable. It logs the process, including inputs,
data transformations, and results. The function filters predictors based on
a given significance level.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
logistic_regression(df, target_variable, predictor_vars, significance_level)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the target and predictor variables.

\item[\code{target\_variable}] A string representing the name of the target variable.

\item[\code{predictor\_vars}] A vector of strings representing the names of predictor variables.

\item[\code{significance\_level}] A numeric value specifying the significance level for filtering predictors.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame containing significant predictors with their p-values and formatted p-values.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Assuming df is your data frame
target_variable <- "cleaned_does_the_physician_accept_medicaid_numeric"
predictor_vars <- setdiff(names(df), c(target_variable, "does_the_physician_accept_medicaid", "cleaned_does_the_physician_accept_medicaid"))
significance_logistic_regression <- 0.2

significant_vars <- logistic_regression(df, target_variable, predictor_vars, significance_level = significance_logistic_regression)
print(significant_vars)
\end{ExampleCode}
\end{Examples}
\HeaderA{MaxTable}{Calculate the Maximum Value(s) and Corresponding Level(s) of a Factor Variable}{MaxTable}
%
\begin{Description}
This function returns the level(s) corresponding to the maximum value(s) of a factor variable.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
MaxTable(InVec, mult = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{InVec}] Input vector, expected to be a factor variable or convertible to a factor.

\item[\code{mult}] Logical value indicating whether to return multiple maximum values or just the first one. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
If \code{mult} is FALSE, returns the level corresponding to the maximum value of the factor variable.
If \code{mult} is TRUE, returns a character vector containing all the levels with the maximum value.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
vec <- factor(c("A", "B", "A", "C", "B", "B"))
MaxTable(vec) # Returns "A"
MaxTable(vec, mult = TRUE) # Returns c("A", "B")
\end{ExampleCode}
\end{Examples}
\HeaderA{MinTable}{Calculate the Minimum Value(s) and Corresponding Level(s) of a Factor Variable}{MinTable}
%
\begin{Description}
This function returns the level(s) corresponding to the minimum value(s) of a factor variable.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
MinTable(InVec, mult = FALSE)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{InVec}] Input vector, expected to be a factor variable or convertible to a factor.

\item[\code{mult}] Logical value indicating whether to return multiple minimum values or just the first one. Default is FALSE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
If \code{mult} is FALSE, returns the level corresponding to the minimum value of the factor variable.
If \code{mult} is TRUE, returns a character vector containing all the levels with the minimum value.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
vec <- factor(c("A", "B", "A", "C", "B", "B"))
MinTable(vec) # Returns "C"
MinTable(vec, mult = TRUE) # Returns "C"
\end{ExampleCode}
\end{Examples}
\HeaderA{most\_common\_gender\_training\_academic}{Generate a Summary Sentence for the Most Common Gender, Specialty, Training, and Academic Affiliation}{most.Rul.common.Rul.gender.Rul.training.Rul.academic}
%
\begin{Description}
This function calculates and returns a sentence that describes the most common gender, specialty, training, and academic affiliation in the provided dataset.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
most_common_gender_training_academic(df)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the columns \code{gender}, \code{specialty}, \code{Provider.Credential.Text}, and \code{academic\_affiliation}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function filters out missing values in each column before determining the most common value. It then calculates the proportion of this most common value relative to the total non-missing values in that column.
\end{Details}
%
\begin{Value}
A character string summarizing the most common gender, specialty, training, and academic affiliation along with their respective proportions.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with a small dataset
df <- data.frame(
  gender = c("Male", "Female", "Female", "Male", "Male"),
  specialty = c("Cardiology", "Cardiology", "Neurology", "Cardiology", "Neurology"),
  Provider.Credential.Text = c("MD", "MD", "DO", "MD", "DO"),
  academic_affiliation = c("Yes", "No", "Yes", "No", "Yes")
)
result <- most_common_gender_training_academic(df)
print(result)

# Example 2: Handling missing data
df_with_na <- data.frame(
  gender = c("Male", NA, "Female", "Male", "Male"),
  specialty = c("Cardiology", "Cardiology", "Neurology", NA, "Neurology"),
  Provider.Credential.Text = c("MD", "MD", "DO", "MD", "DO"),
  academic_affiliation = c("Yes", "No", "Yes", "No", NA)
)
result <- most_common_gender_training_academic(df_with_na)
print(result)

# Example 3: Different proportions with a larger dataset
df_large <- data.frame(
  gender = c(rep("Male", 70), rep("Female", 30)),
  specialty = c(rep("Cardiology", 50), rep("Neurology", 30), rep("Orthopedics", 20)),
  Provider.Credential.Text = c(rep("MD", 60), rep("DO", 40)),
  academic_affiliation = c(rep("Yes", 40), rep("No", 60))
)
result <- most_common_gender_training_academic(df_large)
print(result)

\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_collect\_and\_clean\_data}{Collect and clean the processed data}{nppes.Rul.collect.Rul.and.Rul.clean.Rul.data}
%
\begin{Description}
This function collects and cleans the processed data from DuckDB.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_collect_and_clean_data(processed_data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{processed\_data}] A \code{tbl} object containing the processed data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
After collecting and cleaning the data, the next step is to save the cleaned data to a CSV file using \code{nppes\_save\_data\_to\_csv()}.
\end{Details}
%
\begin{Value}
A cleaned data frame.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
cleaned_data <- nppes_collect_and_clean_data(processed_data)
# Next step: nppes_save_data_to_csv()
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_connect\_to\_duckdb}{Connect to DuckDB}{nppes.Rul.connect.Rul.to.Rul.duckdb}
%
\begin{Description}
This function establishes a connection to a DuckDB database.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_connect_to_duckdb(duckdb_file_path)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{duckdb\_file\_path}] The file path to the DuckDB database file.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A DuckDB connection object.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
con <- nppes_connect_to_duckdb("/path/to/your/duckdb_file.duckdb")
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_create\_table\_from\_csv}{Create a table in DuckDB from a CSV file}{nppes.Rul.create.Rul.table.Rul.from.Rul.csv}
%
\begin{Description}
This function creates a table in DuckDB by reading data from a CSV file.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_create_table_from_csv(con, file_path, table_name = "npi_2024")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] A DuckDB connection object.

\item[\code{file\_path}] A character string specifying the path to the CSV file.

\item[\code{table\_name}] A character string specifying the name of the table to be created. Default is "npi\_2024".
\end{ldescription}
\end{Arguments}
%
\begin{Details}
After creating the table, the next step is to query sample data using \code{nppes\_query\_sample\_data()}.
\end{Details}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
con <- nppes_connect_to_duckdb("/path/to/your/duckdb_file.duckdb")
nppes_create_table_from_csv(con, "/path/to/your/csv_file.csv", "npi_2024")
# Next step: nppes_query_sample_data()

con <- nppes_connect_to_duckdb("/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb")
nppes_create_table_from_csv(con, "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data.csv", "npi_2024")
# Next step: nppes_query_sample_data()
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_disconnect\_from\_duckdb}{Disconnect from DuckDB}{nppes.Rul.disconnect.Rul.from.Rul.duckdb}
%
\begin{Description}
This function disconnects from a DuckDB database connection after all operations are complete.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_disconnect_from_duckdb(con)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] A DuckDB connection object.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Use this function after you have completed all operations involving the DuckDB connection.
\end{Details}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
con <- nppes_connect_to_duckdb("/path/to/your/duckdb_file.duckdb")
# After all operations, disconnect from the database
nppes_disconnect_from_duckdb(con)

con <- nppes_connect_to_duckdb("/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb")
# After all operations, disconnect from the database
nppes_disconnect_from_duckdb(con)
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_get\_data\_for\_one\_year}{Get NPPES Data for One Year with Chunked Processing}{nppes.Rul.get.Rul.data.Rul.for.Rul.one.Rul.year}
%
\begin{Description}
This function processes NPPES data for one year by reading a CSV file, filtering based on taxonomy codes,
and writing the results to an output CSV file. It processes the data in chunks to improve memory efficiency
and includes system beeps for progress and completion. Logs are provided at each step using the \code{log\_message}
function.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_get_data_for_one_year(
  npi_file_path,
  output_csv_path,
  duckdb_file_path =
    "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb",
  taxonomy_codes_1 = c("207V00000X", "207VB0002X", "207VC0300X", "207VC0200X",
    "207VX0201X", "207VG0400X", "207VH0002X", "207VM0101X", "207VX0000X", "207VE0102X",
    "207VF0040X"),
  taxonomy_codes_2 = c("207V00000X", "207VB0002X", "207VC0300X", "207VC0200X",
    "207VX0201X", "207VG0400X", "207VH0002X", "207VM0101X", "207VX0000X", "207VE0102X",
    "207VF0040X"),
  save_column_in_each_nppes_year = FALSE,
  excel_file_path = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{npi\_file\_path}] A character string specifying the path to the NPI CSV file. This file is expected to
contain the raw NPPES data for one year.

\item[\code{output\_csv\_path}] A character string specifying the path to save the output cleaned data as a CSV file.
The file will be created or overwritten during the process.

\item[\code{duckdb\_file\_path}] A character string specifying the path to the DuckDB database file. Default is a
predefined file path.

\item[\code{taxonomy\_codes\_1}] A character vector specifying the taxonomy codes to filter in
\AsIs{\texttt{Healthcare Provider Taxonomy Code\_1}}. Default is a predefined set of taxonomy codes.

\item[\code{taxonomy\_codes\_2}] A character vector specifying the taxonomy codes to filter in
\AsIs{\texttt{Healthcare Provider Taxonomy Code\_2}}. Default is the same set as \code{taxonomy\_codes\_1}.

\item[\code{save\_column\_in\_each\_nppes\_year}] A logical value indicating whether to save a sample of the data to
an Excel file for each year. Default is \code{FALSE}.

\item[\code{excel\_file\_path}] A character string specifying the path to save the Excel file if
\code{save\_column\_in\_each\_nppes\_year} is \code{TRUE}. Default is \code{NULL}.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This function processes large datasets in chunks to reduce memory usage, filtering the data based on
specified taxonomy codes. It logs progress using the \code{log\_message} helper function and gives auditory feedback with system beeps.

The function saves the cleaned data to the specified \code{output\_csv\_path} and can optionally save sample data to Excel.
After completion, a message is displayed indicating that \code{nppes\_save\_summary\_statistics} is the next function to run.
\end{Details}
%
\begin{Value}
A cleaned data frame containing the NPPES data for one year. The data is saved to a CSV file
specified by the \code{output\_csv\_path} argument.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with default taxonomy codes
result <- nppes_get_data_for_one_year(
  npi_file_path = "/path/to/npi_file.csv",
  output_csv_path = "/path/to/output.csv"
)

# Example 2: Using custom taxonomy codes for filtering
result <- nppes_get_data_for_one_year(
  npi_file_path = "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/NPPES_Data_Disseminat_September_2024/npidata_pfile_20050523-20240811.csv",
  output_csv_path = "/path/to/output.csv",
  taxonomy_codes_1 = c("207X00000X", "207Y00000X"),
  taxonomy_codes_2 = c("207X00000X", "207Y00000X")
)

# Example 3: Saving sample data to an Excel file
result <- nppes_get_data_for_one_year(
  npi_file_path = "/path/to/npi_file.csv",
  output_csv_path = "/path/to/output.csv",
  save_column_in_each_nppes_year = TRUE,
  excel_file_path = "/path/to/sample_data.xlsx"
)

\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_initialize\_environment}{Initialize the environment by loading necessary libraries and resolving conflicts}{nppes.Rul.initialize.Rul.environment}
%
\begin{Description}
This function sets up the environment by sourcing the setup script and resolving package conflicts.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_initialize_environment(setup_file)
\end{verbatim}
\end{Usage}
%
\begin{Details}
After initializing the environment, the next step is to connect to DuckDB using \code{connect\_to\_duckdb()}.
\end{Details}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
nppes_initialize_environment()
# Next step: tyler::connect_to_duckdb()
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_process\_npi\_table\_chunk}{Process an NPI table in DuckDB}{nppes.Rul.process.Rul.npi.Rul.table.Rul.chunk}
%
\begin{Description}
This function processes an NPI table in DuckDB, filtering by taxonomy codes and cleaning the data.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_process_npi_table_chunk(
  con,
  table_name = "npi_2024",
  taxonomy_codes_1 = c("390200000X", "174400000X", "207V00000X", "207VB0002X",
    "207VC0300X", "207VC0200X", "207VX0201X", "207VG0400X", "207VH0002X", "207VM0101X",
    "207VX0000X", "207VE0102X", "207VF0040X"),
  taxonomy_codes_2 = c("390200000X", "174400000X", "207V00000X", "207VB0002X",
    "207VC0300X", "207VC0200X", "207VX0201X", "207VG0400X", "207VH0002X", "207VM0101X",
    "207VX0000X", "207VE0102X", "207VF0040X")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] A DuckDB connection object.

\item[\code{table\_name}] A character string specifying the name of the table to process. Default is "npi\_2024".

\item[\code{taxonomy\_codes\_1}] A character vector specifying the taxonomy codes to filter in \AsIs{\texttt{Healthcare Provider Taxonomy Code\_1}}. Default is a predefined set of codes.

\item[\code{taxonomy\_codes\_2}] A character vector specifying the taxonomy codes to filter in \AsIs{\texttt{Healthcare Provider Taxonomy Code\_2}}. Default is a predefined set of codes.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
After processing the NPI table, the next step is to collect and clean the data using \code{nppes\_collect\_and\_clean\_data()}.
\end{Details}
%
\begin{Value}
A \code{tbl} object containing the processed data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
con <- nppes_connect_to_duckdb("/path/to/your/duckdb_file.duckdb")
processed_data <- nppes_process_npi_table_chunk(con, "npi_2024", c("207V00000X"), c("207V00000X"))
# Next step: nppes_collect_and_clean_data()

con <- nppes_connect_to_duckdb("/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb")
processed_data <- nppes_process_npi_table_chunk(con)
# Next step: nppes_collect_and_clean_data()
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_query\_sample\_data}{Query sample data from a DuckDB table}{nppes.Rul.query.Rul.sample.Rul.data}
%
\begin{Description}
This function queries a sample of data from a specified DuckDB table.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_query_sample_data(
  con,
  table_name = "npi_2024",
  limit = 5,
  save_column_in_each_nppes_year = TRUE,
  excel_file_path = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] A DuckDB connection object.

\item[\code{table\_name}] A character string specifying the name of the table to query. Default is "npi\_2024".

\item[\code{limit}] An integer specifying the number of rows to retrieve. Default is 5.

\item[\code{save\_column\_in\_each\_nppes\_year}] A logical value indicating whether to save the sample data to an Excel file. Default is FALSE.

\item[\code{excel\_file\_path}] A character string specifying the path to the Excel file if \code{save\_column\_in\_each\_nppes\_year} is TRUE. Default is NULL.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
After querying the sample data, the next step is to process the NPI table using \code{nppes\_process\_npi\_table\_chunk()}.
\end{Details}
%
\begin{Value}
A data frame containing the queried sample data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
con <- nppes_connect_to_duckdb("/path/to/your/duckdb_file.duckdb")
nppes_query_sample_data(con, "npi_2024", 5)
# Next step: nppes_process_npi_table_chunk()

con <- nppes_connect_to_duckdb("/Volumes/Video Projects Muffly 1/nppes_historical_downloads/my_duckdb.duckdb")
nppes_query_sample_data(con, "npi_2024", 10, TRUE, "/path/to/excel_file.xlsx")
# Next step: nppes_process_npi_table_chunk()
\end{ExampleCode}
\end{Examples}
\HeaderA{nppes\_save\_data\_to\_csv}{Save the final data to CSV}{nppes.Rul.save.Rul.data.Rul.to.Rul.csv}
%
\begin{Description}
This function saves the cleaned data to a CSV file.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
nppes_save_data_to_csv(data, file_path)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A cleaned data frame.

\item[\code{file\_path}] A character string specifying the path to the CSV file.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
Use this function to save the final cleaned data to a specified file path.
\end{Details}
%
\begin{Value}
None
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
nppes_save_data_to_csv(cleaned_data, "/path/to/your/file.csv")
\end{ExampleCode}
\end{Examples}
\HeaderA{open\_payments\_collect\_and\_convert}{Collect and Convert Open Payments Data with Crosswalk Merging}{open.Rul.payments.Rul.collect.Rul.and.Rul.convert}
%
\begin{Description}
This function reads the filtered OBGYN data from a CSV file, merges it with crosswalk data from an RDS file on the NPI column, adds prefixes to distinguish the columns from each dataset, and writes the merged data to a CSV file. Detailed logging is performed for each step of the process.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
open_payments_collect_and_convert(
  duckdb_file_path,
  specialty_csv_path,
  crosswalk_rds_path,
  log_path,
  output_csv =
    "/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/end_csv_output_open_payments_collect_and_convert.csv"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{duckdb\_file\_path}] A string representing the full path to the DuckDB database file. This is used for logging purposes, although no data is read directly from the DuckDB in this function.

\item[\code{specialty\_csv\_path}] A string representing the full path to the CSV file containing the filtered OBGYN data from the Open Payments dataset. This file should contain a "Covered\_Recipient\_NPI" column.

\item[\code{crosswalk\_rds\_path}] A string representing the full path to the RDS file containing the crosswalk data. The crosswalk data should contain an "NPI" column that matches the "Covered\_Recipient\_NPI" in the OBGYN data.

\item[\code{log\_path}] A string representing the full path to a text file where logging details about the execution of the function will be written.

\item[\code{output\_csv}] A string representing the full path to the output CSV file where the merged data will be written. The default is "/Users/tylermuffly/Dropbox (Personal)/Lo\_Fellowship\_Directors/academic\_hand\_search/data/end\_csv\_output\_open\_payments\_collect\_and\_convert.csv".
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function follows these steps:
\begin{enumerate}

\item{} Reads the OBGYN data from the provided CSV file and renames the "Covered\_Recipient\_NPI" column to "NPI".
\item{} Adds the prefix "OP\_" to all columns from the OBGYN data to avoid column name conflicts in the final merged data.
\item{} Reads the crosswalk data from the provided RDS file, ensures that only unique NPI values are retained using \code{distinct()}, and adds the prefix "lo\_data\_" to its columns.
\item{} Performs a left join between the OBGYN data and crosswalk data on the "NPI" column (now prefixed as "OP\_NPI" and "lo\_data\_NPI").
\item{} Logs the number of rows in the merged data and writes the merged data to the specified output CSV file.
\item{} Logs all steps, including DuckDB connection, data loading, column renaming, merging, and output writing.

\end{enumerate}

\end{Details}
%
\begin{Value}
A data frame containing the merged data from the OBGYN and crosswalk datasets. The columns from the OBGYN dataset will be prefixed with "OP\_" and the columns from the crosswalk dataset will be prefixed with "lo\_data\_".
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Using default output path and logging to a file
## Not run: 
open_payments_collect_and_convert(
  duckdb_file_path = "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb",
  specialty_csv_path = "/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv",
  crosswalk_rds_path = "/Users/tylermuffly/Dropbox (Personal)/Lo_Fellowship_Directors/academic_hand_search/data/merged_into_a_crosswalk.rds",
  log_path = "/path/to/log_file.txt"
)

## End(Not run)

# Example 2: Custom output path for CSV and detailed logging
## Not run: 
open_payments_collect_and_convert(
  duckdb_file_path = "/path/to/duckdb_file.duckdb",
  specialty_csv_path = "/path/to/specialty_data.csv",
  crosswalk_rds_path = "/path/to/crosswalk_data.rds",
  log_path = "/path/to/detailed_log.txt",
  output_csv = "/path/to/merged_output_data.csv"
)

## End(Not run)

# Example 3: Using different file locations for each argument
## Not run: 
open_payments_collect_and_convert(
  duckdb_file_path = "/my_data/nppes_historical_downloads/duckdb_file.duckdb",
  specialty_csv_path = "/my_data/open_payments_data/filtered_data.csv",
  crosswalk_rds_path = "/my_data/crosswalk_data/crosswalk_file.rds",
  log_path = "/my_data/logs/processing_log.txt",
  output_csv = "/my_data/output/merged_data.csv"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{open\_payments\_processed\_per\_npi}{Process Payments Per NPI from CSV in Chunks and Write to CSV with Date in Filename}{open.Rul.payments.Rul.processed.Rul.per.Rul.npi}
%
\begin{Description}
This function reads data from a CSV file in chunks, processes it by grouping the data by \code{Covered\_Recipient\_NPI},
and writes the result to a specified CSV file with the current date in the file name. It includes logging,
error handling, and system beeps for progress and completion.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
open_payments_processed_per_npi(
  input_csv_path,
  output_csv_path =
    "/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/end_payments_per_npi.csv",
  chunk_size = 1e+05,
  log_function = message,
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_csv\_path}] A file path to read the input CSV file.

\item[\code{output\_csv\_path}] A file path to write the resulting CSV file (with the current date appended).
The path is checked for existence and writability before writing.

\item[\code{chunk\_size}] Integer, size of chunks to process. Defaults to 100,000 rows per chunk.

\item[\code{log\_function}] A logging function, defaults to \code{message}.

\item[\code{verbose}] Logical, whether to display detailed logging. Defaults to TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the total rows processed and the output CSV file path.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Process payments per NPI from CSV and write output with date
result <- open_payments_processed_per_npi(
  input_csv_path = "path/to/input.csv",
  output_csv_path = "path/to/output.csv"
)

# Example 2: Custom logging function and different output path
result <- open_payments_processed_per_npi(
  input_csv_path = "path/to/input.csv",
  output_csv_path = "custom/output/path.csv",
  log_function = print
)
\end{ExampleCode}
\end{Examples}
\HeaderA{open\_payments\_specialty\_cleaning}{Clean and Filter Open Payments Data by Specialty}{open.Rul.payments.Rul.specialty.Rul.cleaning}
%
\begin{Description}
This function connects to a DuckDB database, processes tables containing Open Payments data, and filters records based on specified medical specialties.
The data is processed in chunks to minimize memory usage, and the filtered results are saved as a CSV file.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
open_payments_specialty_cleaning(
  con = DBI::dbConnect(duckdb::duckdb(),
    "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb"),
  table_names = c("OP_DTL_GNRL_PGYR2020_P01182024", "OP_DTL_GNRL_PGYR2021_P01182024",
    "OP_DTL_GNRL_PGYR2022_P01182024", "OP_DTL_GNRL_PGYR2023_P06282024_06122024"),
  output_csv_path =
    "/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_merged/clean_open_payments_specialty.csv",
  specialties = c("Specialist",
    "Student in an Organized Health Care Education/Training Program",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Critical Care Medicine",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Female Pelvic Medicine and Reconstructive Surgery",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecologic Oncology",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Gynecology", 
    
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Hospice and Palliative Medicine",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Obstetrics",
    "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology"),
  chunk_size = 1e+05
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{con}] Database connection to a DuckDB database. Defaults to a connection using the provided DuckDB file path.

\item[\code{table\_names}] A character vector of table names in the DuckDB to be processed. Each table is processed in chunks to reduce memory load.

\item[\code{output\_csv\_path}] The file path where the cleaned and filtered Open Payments data will be saved. If the file path directory does not exist, it will be created.

\item[\code{specialties}] A character vector of medical specialties to filter the data. The function will return rows where any of the Covered\_Recipient\_Specialty\_X columns match one of the given specialties.

\item[\code{chunk\_size}] The size of each data chunk to process at a time. This allows the function to handle large datasets efficiently by breaking the query into smaller parts. Defaults to 100,000 rows per chunk.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A list containing the names of the processed tables.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Process two Open Payments tables for specific specialties and save to a CSV
open_payments_specialty_cleaning(
  con = DBI::dbConnect(duckdb::duckdb(), "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb"),
  table_names = c("OP_DTL_GNRL_PGYR2020_P01182024", "OP_DTL_GNRL_PGYR2021_P01182024"),
  output_csv_path = "/Volumes/Video Projects Muffly 1/openpayments/clean_open_payments_specialty.csv",
  specialties = c("Allopathic & Osteopathic Physicians|Obstetrics & Gynecology",
                  "Student in an Organized Health Care Education/Training Program"),
  chunk_size = 50000
)

# Example 2: Process multiple years of Open Payments data and filter by multiple OB-GYN specialties
open_payments_specialty_cleaning(
  con = DBI::dbConnect(duckdb::duckdb(), "/Volumes/Video Projects Muffly 1/nppes_historical_downloads/nber/nber_my_duckdb.duckdb"),
  table_names = c("OP_DTL_GNRL_PGYR2022_P01182024", "OP_DTL_GNRL_PGYR2023_P06282024_06122024"),
  output_csv_path = "/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/open_payments_filtered.csv",
  specialties = c("Allopathic & Osteopathic Physicians|Obstetrics & Gynecology",
                  "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Maternal & Fetal Medicine"),
  chunk_size = 100000
)

# Example 3: Process data with default connection and specialty filtering
open_payments_specialty_cleaning(
  table_names = c("OP_DTL_GNRL_PGYR2021_P01182024"),
  output_csv_path = "/Volumes/Video Projects Muffly 1/openpayments/unzipped_files/cleaned_open_payments_specialty.csv",
  specialties = c("Allopathic & Osteopathic Physicians|Gynecologic Oncology",
                  "Allopathic & Osteopathic Physicians|Obstetrics & Gynecology|Reproductive Endocrinology")
)

\end{ExampleCode}
\end{Examples}
\HeaderA{physician\_age}{Calculate and Summarize Physician Age}{physician.Rul.age}
%
\begin{Description}
This function calculates the median age, as well as the 25th and 75th percentiles (Interquartile Range, IQR) of a specified age column in a data frame. It returns a sentence summarizing these statistics.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
physician_age(df, age_column)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the age data.

\item[\code{age\_column}] A character string representing the name of the column in \code{df} that contains the age data.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function calculates the median, 25th percentile (Q1), and 75th percentile (Q3) of the age data, rounding the results to two decimal places for the median and one decimal place for the percentiles. It then constructs a summary sentence describing these statistics.
\end{Details}
%
\begin{Value}
A character string summarizing the median age and IQR of the specified age column in the dataset.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Basic usage with a small dataset
df <- data.frame(age = c(30, 40, 50, 60, 35, 45, 55, 65))
summary_sentence <- physician_age(df, "age")
print(summary_sentence)

# Example 2: Handling missing data
df_with_na <- data.frame(age = c(30, 40, NA, 60, 35, NA, 55, 65))
summary_sentence <- physician_age(df_with_na, "age")
print(summary_sentence)

# Example 3: Different age distribution
df_large <- data.frame(age = c(rep(30, 70), rep(40, 30), rep(50, 20), rep(60, 10)))
summary_sentence <- physician_age(df_large, "age")
print(summary_sentence)

\end{ExampleCode}
\end{Examples}
\HeaderA{plot\_and\_save\_emmeans}{Plot and Save Estimated Marginal Means (EMMs)}{plot.Rul.and.Rul.save.Rul.emmeans}
%
\begin{Description}
This function computes the estimated marginal means (EMMs) from a model object, creates a plot of the EMMs with confidence intervals, and saves the plot to a specified directory. This is particularly useful in studies like mystery caller studies where you need to analyze differences in outcomes such as appointment wait times across different groups or treatments.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
plot_and_save_emmeans(
  model_object,
  specs,
  variable_of_interest,
  color_by,
  output_dir = "Ari/Figures"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{model\_object}] A fitted model object from which EMMs are to be computed. This can be a generalized linear model (GLM), linear model, or other suitable models.

\item[\code{specs}] A character string specifying the predictor variable(s) for which EMMs are to be computed. For example, this could be the treatment groups, scenarios, or demographic variables.

\item[\code{variable\_of\_interest}] A character string specifying the variable to be plotted on the x-axis. Typically, this would be the same as the \code{specs}.

\item[\code{color\_by}] A character string specifying the variable used to color the points and error bars. This could be a categorical variable like gender, insurance type, or academic affiliation.

\item[\code{output\_dir}] A character string specifying the directory where the plot will be saved. Defaults to "Ari/Figures".
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function computes EMMs using the \code{emmeans} package, which provides adjusted means (or other summary statistics) for the levels of factors, corrected for the influence of other predictors in the model. This is particularly useful in the context of mystery caller studies, where you might want to compare outcomes such as appointment wait times between different insurance types, scenarios, or demographic groups, while controlling for potential confounders.

The plot generated by this function displays the EMMs along with their 95\% confidence intervals, making it easy to visualize the differences between groups. The plot is saved to a specified directory with a unique filename that includes a timestamp.
\end{Details}
%
\begin{Value}
A list containing the estimated marginal means data (\code{data}) and the ggplot object (\code{plot}).
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example 1: Comparing Appointment Wait Times Between Scenarios
# Assume 'model' is a fitted GLM object with appointment wait time as the outcome
result <- plot_and_save_emmeans(
  model_object = model,
  specs = "scenario",
  variable_of_interest = "scenario",
  color_by = "insurance",
  output_dir = "Figures/ScenarioComparison"
)

# Example 2: Comparing EMMs Across Different Insurance Types
# Assume 'model' is a fitted GLM object
result <- plot_and_save_emmeans(
  model_object = model,
  specs = "insurance",
  variable_of_interest = "insurance",
  color_by = "academic",
  output_dir = "Figures/InsuranceComparison"
)

# Example 3: Visualizing EMMs for Different Age Groups in a Mystery Caller Study
# Assume 'model' is a fitted GLM object with age groups as a factor
result <- plot_and_save_emmeans(
  model_object = model,
  specs = "age_group",
  variable_of_interest = "age_group",
  color_by = "gender",
  output_dir = "Figures/AgeGroupComparison"
)

# Example 4: Interaction Effects Between Gender and Insurance Type
# Assume 'model' is a fitted GLM object
result <- plot_and_save_emmeans(
  model_object = model,
  specs = c("gender", "insurance"),
  variable_of_interest = "gender",
  color_by = "insurance",
  output_dir = "Figures/InteractionEffects"
)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{poisson\_wait\_time\_stats}{Fit a Poisson Regression Model for Waiting Times by Group}{poisson.Rul.wait.Rul.time.Rul.stats}
%
\begin{Description}
This function reads a dataset, fits a Poisson regression model to predict
waiting times (\code{business\_days\_until\_appointment}) based on a specified grouping variable,
and prints the incidence rate ratios (IRR) along with their confidence intervals and p-values.
The function returns the fitted Poisson model.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
poisson_wait_time_stats(data_dir, file_name, group_var = "insurance")
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_dir}] A character string specifying the directory where the data file is located.

\item[\code{file\_name}] A character string specifying the name of the RDS file to be read.

\item[\code{group\_var}] A character string specifying the grouping variable to use in the model (e.g., "insurance" or "scenario").
\end{ldescription}
\end{Arguments}
%
\begin{Details}
The function fits a Poisson regression model with a log link function.
The model estimates the incidence rate ratios (IRRs) for each level of the specified grouping variable
relative to the reference level (the first level in the dataset). The function then prints out the IRRs,
confidence intervals, and p-values for each comparison. The fitted model is returned for further use.
\end{Details}
%
\begin{Value}
A fitted Poisson regression model object (\code{glm}).
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Example usage:
poisson_model <- poisson_wait_time_stats(data_dir = "your_data_directory",
                                         file_name = "Phase_2.rds",
                                         group_var = "scenario")

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{prepare\_dataset}{Prepare the Dataset by Excluding Certain Columns with Logging}{prepare.Rul.dataset}
%
\begin{Description}
This function prepares the dataset by excluding specified columns from the predictor variables.
It logs the process, including inputs and outputs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
prepare_dataset(df, target_variable, excluded_columns)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the dataset.

\item[\code{target\_variable}] A string representing the name of the target variable.

\item[\code{excluded\_columns}] A vector of strings representing the names of columns to exclude from the predictors.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A vector of strings representing the names of predictor variables.
\end{Value}
\HeaderA{process\_and\_save\_isochrones}{Process and Save Isochrones}{process.Rul.and.Rul.save.Rul.isochrones}
%
\begin{Description}
This function takes an input file of locations, retrieves isochrones for each location,
and saves them as shapefiles. It processes the data in chunks of 25 rows at a time to
prevent data loss in case of errors.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
process_and_save_isochrones(input_file, chunk_size = 25)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_file}] A data frame containing location data with columns "lat" and "long."
The input file should represent geographic coordinates for which
isochrones will be calculated.

\item[\code{chunk\_size}] The number of rows to process in each chunk. Default is 25.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This function uses the \code{hereR} package to calculate isochrones based on the
provided geographic coordinates. It retrieves isochrones for each location in
the input file, processes the data in chunks to minimize the risk of data loss
in case of errors, and saves the isochrones as shapefiles for further analysis.
\end{Details}
%
\begin{Value}
An sf (simple features) data frame containing isochrone polygons.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Load the input file (e.g., from a CSV)
input_file <- read_csv("data/locations.csv")

# Process and save isochrones for the input file (chunk size set to 25)
isochrones_data <- process_and_save_isochrones(input_file, chunk_size = 25)

# Optionally, write the combined isochrones to a shapefile
sf::st_write(isochrones_data, dsn = "data/isochrones/isochrones_all_combined",
             layer = "isochrones", driver = "ESRI Shapefile", quiet = FALSE)

\end{ExampleCode}
\end{Examples}
\HeaderA{race\_drive\_time\_generate\_summary\_sentence}{Generate Summary Sentence for Race and Drive Time}{race.Rul.drive.Rul.time.Rul.generate.Rul.summary.Rul.sentence}
%
\begin{Description}
This function generates a summary sentence indicating the level of access to gynecologic oncologists for a specified race and drive time.
It can run for individual races or all races in the dataset.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
race_drive_time_generate_summary_sentence(
  tabulated_data,
  driving_time_minutes = 180,
  race = "American Indian/Alaska Native"
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{tabulated\_data}] A data frame containing the data to analyze. Must include columns \AsIs{\texttt{Driving Time (minutes)}}, \code{Year}, \code{total\_female\_026}, and columns for race proportions like \code{White\_prop}, \code{Black\_prop}, etc.

\item[\code{driving\_time\_minutes}] A numeric value specifying the driving time in minutes to filter the data. Default is 180.

\item[\code{race}] A character string specifying the race for which to generate the summary sentence. Supported values are "White", "Black", "American Indian/Alaska Native", "Asian", "Native Hawaiian or Pacific Islander", or "all" to generate sentences for all supported races. Default is "American Indian/Alaska Native".
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character string containing the summary sentence, or a list of summary sentences if \code{race = "all"}.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage
summary_sentence <- race_drive_time_generate_summary_sentence(
  tabulated_data = tabulated_all_years_numeric,
  driving_time_minutes = 180,
  race = "American Indian/Alaska Native"
)
print(summary_sentence)
\end{ExampleCode}
\end{Examples}
\HeaderA{remove\_constant\_vars}{Remove Constant Variables from a Data Frame}{remove.Rul.constant.Rul.vars}
%
\begin{Description}
This function takes a data frame and returns a new data frame with constant variables removed.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
remove_constant_vars(data_frame)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_frame}] A data frame from which constant variables should be removed.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with constant variables removed.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
new_data <- remove_constant_vars(data_frame)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{remove\_near\_zero\_var}{Remove Near-Zero Variance Variables from a Data Frame}{remove.Rul.near.Rul.zero.Rul.var}
%
\begin{Description}
This function takes a data frame and returns a new data frame with near-zero variance variables removed.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
remove_near_zero_var(data_frame, freqCut = 19, uniqueCut = 10)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_frame}] A data frame from which near-zero variance variables should be removed.

\item[\code{freqCut}] The ratio of the most common value to the second most common value. Defaults to 19.

\item[\code{uniqueCut}] The percentage of distinct values out of the number of total samples. Defaults to 10.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with near-zero variance variables removed.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
new_data <- remove_near_zero_var(data_frame)

## End(Not run)

\end{ExampleCode}
\end{Examples}
\HeaderA{rename\_columns\_by\_substring}{Rename columns based on substring matches}{rename.Rul.columns.Rul.by.Rul.substring}
%
\begin{Description}
This function searches through column names in a data frame for specified substrings and renames the first matching column to a new name provided by the user. It provides detailed logs for each operation, including the columns found and any renaming actions taken. If multiple columns match a substring, only the first is renamed, and a warning is issued.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
rename_columns_by_substring(data, target_strings, new_names)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame whose columns need renaming.

\item[\code{target\_strings}] A vector of substrings to search for within column names.

\item[\code{new\_names}] A vector of new names corresponding to the target strings.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with renamed columns.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
df <- data.frame(
  doctor_info = 1:5,
  patient_contact_data = 6:10
)
# Renaming 'doctor_info' to 'physician_info'
df <- rename_columns_by_substring(df,
                                  target_strings = c("doctor"),
                                  new_names = c("physician_info"))
print(df)

# More complex example with multiple renamings
df <- data.frame(
  doc_information = 1:5,
  patient_contact = 6:10,
  doctor_notes = 11:15
)
# Renaming 'doc_information' to 'doctor_info' and 'doctor_notes' to 'notes'
df <- rename_columns_by_substring(df,
                                  target_strings = c("doc_information", "doctor_notes"),
                                  new_names = c("doctor_info", "notes"))
print(df)
\end{ExampleCode}
\end{Examples}
\HeaderA{retrieve\_clinician\_data}{Retrieve Clinician Data}{retrieve.Rul.clinician.Rul.data}
%
\begin{Description}
This function retrieves clinician data for each valid NPI in the input dataframe.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
retrieve_clinician_data(input_data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_data}] Either a dataframe containing NPI numbers or a path to a CSV file.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A tibble with clinician data for the provided NPIs.
\end{Value}
\HeaderA{save\_quality\_check\_table}{Save Quality Check Table}{save.Rul.quality.Rul.check.Rul.table}
%
\begin{Description}
This function takes a data frame containing 'npi' and 'name' columns and creates a quality check table.
The table includes the count of observations for each 'npi' and 'name' combination where the count is greater than 2.
The resulting table is saved as a CSV file.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
save_quality_check_table(df, filepath)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame containing the columns 'npi' and 'name'.

\item[\code{filepath}] The path where the CSV file should be saved.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
Prints a message to the console indicating that the CSV file has been saved successfully.
\end{Value}
\HeaderA{scrape\_physicians\_data\_with\_tor}{Scrape Physicians' Data with Tor}{scrape.Rul.physicians.Rul.data.Rul.with.Rul.tor}
%
\begin{Description}
This function scrapes data for physicians within a specified ID range, excluding wrong IDs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
scrape_physicians_data_with_tor(startID, endID, torPort)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{startID}] The starting ID for scraping.

\item[\code{endID}] The ending ID for scraping.

\item[\code{torPort}] The port number for Tor SOCKS proxy.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing scraped physicians' data.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Call the function
scrape_result <- scrape_physicians_data_with_tor(startID = 9045999, endID = 9046000, torPort = 9150)

\end{ExampleCode}
\end{Examples}
\HeaderA{search\_and\_process\_npi}{Search and Process NPI Numbers}{search.Rul.and.Rul.process.Rul.npi}
%
\begin{Description}
This function takes an input data frame containing first and last names, performs NPI search and processing,
and filters results based on specified taxonomies. It supports customization for enumeration type, search limit,
and filtering credentials.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
search_and_process_npi(
  data,
  enumeration_type = "ind",
  limit = 5L,
  country_code = "US",
  filter_credentials = c("MD", "DO"),
  save_chunk_size = 10,
  dest_dir = NULL
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data}] A data frame with columns 'first' and 'last' containing the names to search.

\item[\code{enumeration\_type}] The enumeration type for NPI search (e.g., "ind", "org", "all"). Default is "ind".

\item[\code{limit}] The maximum number of search results to request for each name pair. Default is 5.

\item[\code{country\_code}] Filter for only the "US".

\item[\code{filter\_credentials}] A character vector containing the credentials to filter the NPI results. Default is c("MD", "DO").

\item[\code{save\_chunk\_size}] The number of results to save per chunk. Default is 10.

\item[\code{dest\_dir}] Destination directory to save chunked results. Default is NULL (current working directory).
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame containing the processed NPI search results.
\end{Value}
\HeaderA{search\_by\_taxonomy}{Search NPI Database by Taxonomy}{search.Rul.by.Rul.taxonomy}
%
\begin{Description}
This function searches the NPI Database for healthcare providers based on a taxonomy description.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
search_by_taxonomy(taxonomy_to_search)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{taxonomy\_to\_search}] A character vector containing the taxonomy description(s) to search for.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A data frame with filtered NPI data based on the specified taxonomy description.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage with multiple taxonomy descriptions:
go_data <- search_by_taxonomy("Gynecologic Oncology")
fpmrs_data <- search_by_taxonomy("Female Pelvic Medicine and Reconstructive Surgery")
rei_data <- search_by_taxonomy("Reproductive Endocrinology")
mfm_data <- search_by_taxonomy("Maternal & Fetal Medicine")

\end{ExampleCode}
\end{Examples}
\HeaderA{search\_npi}{Search NPI Numbers for Given Names}{search.Rul.npi}
%
\begin{Description}
This function searches for NPI (National Provider Identifier) numbers based on
given first and last names. It can accept input data in the form of a dataframe,
a CSV file, or an RDS file with columns named 'first' and 'last' representing
first names and last names, respectively.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
search_npi(input_data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_data}] A dataframe, a file path to a CSV, or an RDS file containing
first and last names.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing NPI numbers for the provided names that match
the specified taxonomies.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Input as a dataframe
input_df <- data.frame(
  first = c("John", "Jane", "Alice"),
  last = c("Doe", "Smith", "Johnson")
)
npi_results <- search_npi(input_df)

# Input as a CSV file
input_csv <- "path/to/input.csv"
npi_results <- search_npi(input_csv)

\end{ExampleCode}
\end{Examples}
\HeaderA{split\_and\_save}{Split data into multiple parts and save each part as separate Excel files}{split.Rul.and.Rul.save}
%
\begin{Description}
This function splits the data based on provided lab assistant names and saves each part as a separate Excel file.
It allows the arrangement of calls by insurance type to prioritize Medicaid in the first two days and Blue Cross/Blue Shield in the last two days.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
split_and_save(
  data_or_path,
  output_directory,
  lab_assistant_names,
  seed = 1978,
  complete_file_prefix = "complete_non_split_version_",
  split_file_prefix = "",
  recursive_create = TRUE,
  insurance_order = c("Medicaid", "Blue Cross/Blue Shield")
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{data\_or\_path}] Either a dataframe containing the input data or a path to the input data file (RDS, CSV, or XLS/XLSX).

\item[\code{output\_directory}] Directory where output Excel files will be saved.

\item[\code{lab\_assistant\_names}] Vector of lab assistant names to name the output files.

\item[\code{seed}] Seed value for randomization (default is 1978).

\item[\code{complete\_file\_prefix}] Prefix for the complete output file name (default is "complete\_non\_split\_version\_").

\item[\code{split\_file\_prefix}] Prefix for each split output file name (default is empty).

\item[\code{recursive\_create}] Logical indicating if directories should be created recursively (default is TRUE).

\item[\code{insurance\_order}] Vector of insurance types ordered by priority for call scheduling (default is c("Medicaid", "Blue Cross/Blue Shield")).
\end{ldescription}
\end{Arguments}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
library(tyler)
input_data <- readr::read_csv("/path/to/your/input/file.csv")
output_directory <- "/path/to/your/output/directory"
lab_assistant_names <- c("Label1", "Label2", "Label3")
insurance_order <- c("Medicaid", "Blue Cross/Blue Shield")
split_and_save(data_or_path = input_data, output_directory, lab_assistant_names, insurance_order = insurance_order)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{states\_where\_physicians\_were\_NOT\_contacted}{Summarize States Where Physicians Were NOT Contacted}{states.Rul.where.Rul.physicians.Rul.were.Rul.NOT.Rul.contacted}
%
\begin{Description}
This function summarizes the demographic details by identifying the states where physicians
were not successfully contacted and those that were included.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
states_where_physicians_were_NOT_contacted(filtered_data, all_states = NULL)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{filtered\_data}] A data frame containing filtered data of contacted physicians.

\item[\code{all\_states}] A character vector of all possible states including Washington, DC.
If not provided, a default set of states will be used.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A character string summarizing the inclusion and exclusion of states.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example with provided all_states
filtered_data <- data.frame(state = c("California", "New York", "Texas"))
all_states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado",
                 "Connecticut", "Delaware", "Florida", "Georgia", "Hawaii", "Idaho",
                 "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana",
                 "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota",
                 "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
                 "New Hampshire", "New Jersey", "New Mexico", "New York",
                 "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon",
                 "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota",
                 "Tennessee", "Texas", "Utah", "Vermont", "Virginia",
                 "Washington", "West Virginia", "Wisconsin", "Wyoming",
                 "District of Columbia")
states_where_physicians_were_NOT_contacted(filtered_data, all_states)

# Example with default all_states
filtered_data <- data.frame(state = c("California", "New York", "Texas", "Nevada"))
states_where_physicians_were_NOT_contacted(filtered_data)

\end{ExampleCode}
\end{Examples}
\HeaderA{taxonomy}{Taxonomy Codes for Obstetricians and Gynecologists}{taxonomy}
\keyword{datasets}{taxonomy}
%
\begin{Description}
This dataset contains taxonomy codes for Obstetricians and Gynecologists among other healthcare providers.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
taxonomy
\end{verbatim}
\end{Usage}
%
\begin{Format}
A data frame with two columns:
\begin{description}

\item[NUCC Code] NUCC (National Uniform Claim Committee) code for healthcare providers.
\item[Provider Type] The type of healthcare provider corresponding to the NUCC code.

\end{description}

\end{Format}
%
\begin{Source}
\url{https://www.nucc.org/images/stories/PDF/taxonomy_23_0.pdf}
\end{Source}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Load the taxonomy dataset
data(taxonomy)

# Explore the dataset
head(taxonomy)

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{test\_and\_process\_isochrones}{Test and Process Isochrones}{test.Rul.and.Rul.process.Rul.isochrones}
%
\begin{Description}
This function tests and processes isochrones for each location in the input file. It
identifies and reports any errors encountered during the isochrone retrieval process.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
test_and_process_isochrones(input_file)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_file}] A data frame containing location data with columns "lat" and "long."
The input file should represent geographic coordinates for which
isochrones will be calculated.
\end{ldescription}
\end{Arguments}
%
\begin{Details}
This function uses the \code{hereR} package to calculate isochrones based on the
provided geographic coordinates. It retrieves isochrones for each location in
the input file, identifies any errors during the retrieval process, and reports
these errors. The function is designed to be used with input data that meets
specific requirements, including valid latitude and longitude values.
\end{Details}
%
\begin{Value}
Prints messages indicating errors, if any, during isochrone retrieval.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Validate the file of geocoded data.
input_file <- readr::read_csv("data/isochrones/inner_join_postmastr_clinician_data.csv") %>%
  dplyr::mutate(id = dplyr::row_number()) %>%
  dplyr::filter(postmastr.name.x != "Hye In Park, MD")

test_and_process_isochrones(input_file = input_file)

# Filter out the rows that are going to error out after using the test_and_process_isochrones function.
# error_rows <- c(265, 431, 816, 922, 1605, 2049, 2212, 2284, 2308, 2409, 2482, 2735, 2875, 2880, 3150, 3552, 3718)
# input_file_no_error_rows <- input_file %>%
#   dplyr::filter(!id %in% error_rows)

\end{ExampleCode}
\end{Examples}
\HeaderA{tm\_write2pdf}{Generate overall table}{tm.Rul.write2pdf}
%
\begin{Description}
Generate an overall table summarizing the demographics of the Table 1.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
tm_write2pdf(object, filename)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_file\_path}] The path to the data file (in RDS, CSV, or XLS format).

\item[\code{output\_directory}] The directory where the output table file will be saved.

\item[\code{title}] The title for the overall table summary (default is "Overall Table Summary").

\item[\code{selected\_columns}] Optional vector of selected columns to include in the table.

\item[\code{label\_translations}] Optional named list for label translations.
\end{ldescription}
\end{Arguments}
%
\begin{Examples}
\begin{ExampleCode}
## Not run: 
# Generate the overall table
generate_overall_table("data/Table1.rds", "output_tables")

## End(Not run)
\end{ExampleCode}
\end{Examples}
\HeaderA{validate\_and\_remove\_invalid\_npi}{Validate and Remove Invalid NPI Numbers}{validate.Rul.and.Rul.remove.Rul.invalid.Rul.npi}
%
\begin{Description}
This function reads a CSV file containing NPI numbers, validates their
format using the npi package, and removes rows with missing or invalid NPIs.

This function reads a CSV file containing NPI numbers, validates their
format using the npi package, and removes rows with missing or invalid NPIs.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
validate_and_remove_invalid_npi(input_data)

validate_and_remove_invalid_npi(input_data)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{input\_data}] Either a dataframe containing NPI numbers or a path to a CSV file.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
A dataframe containing valid NPI numbers.

A dataframe containing valid NPI numbers.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example usage:
# input_data <- "~/path/to/your/NPI/file.csv"
# valid_df <- validate_and_remove_invalid_npi(input_data)

\end{ExampleCode}
\end{Examples}
\HeaderA{write\_output\_csv}{Write a Data Frame to CSV with Robust Logging and Error Handling}{write.Rul.output.Rul.csv}
%
\begin{Description}
This function writes a data frame to a CSV file in the specified output directory.
It includes detailed logging to inform the user of the function's progress and any potential issues.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
write_output_csv(
  df,
  filename,
  output_dir = "ortho_sports_med/Figures",
  verbose = TRUE
)
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{df}] A data frame to be written to a CSV file.

\item[\code{filename}] A string specifying the name of the output file (with .csv extension).

\item[\code{output\_dir}] A string specifying the directory where the CSV file will be saved. Default is "ortho\_sports\_med/Figures".

\item[\code{verbose}] A boolean indicating whether to print detailed logs. Default is TRUE.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
NULL. The function saves the CSV file to the specified location.
\end{Value}
%
\begin{Examples}
\begin{ExampleCode}
# Example 1: Save a data frame to the default directory with detailed logging
write_output_csv(df, "output.csv")

# Example 2: Save a data frame to a custom directory without logging
write_output_csv(df, "output.csv", output_dir = "custom/directory", verbose = FALSE)

\end{ExampleCode}
\end{Examples}
\HeaderA{\Rpercent{}>\Rpercent{}}{Pipe operator}{.Rpcent.>.Rpcent.}
\keyword{internal}{\Rpercent{}>\Rpercent{}}
%
\begin{Description}
See \code{magrittr::\LinkA{\Rpercent{}>\Rpercent{}}{.Rpcent.>.Rpcent.}} for details.
\end{Description}
%
\begin{Usage}
\begin{verbatim}
lhs %>% rhs
\end{verbatim}
\end{Usage}
%
\begin{Arguments}
\begin{ldescription}
\item[\code{lhs}] A value or the magrittr placeholder.

\item[\code{rhs}] A function call using the magrittr semantics.
\end{ldescription}
\end{Arguments}
%
\begin{Value}
The result of calling \code{rhs(lhs)}.
\end{Value}
\printindex{}
\end{document}
